{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":8887035,"sourceType":"datasetVersion","datasetId":5329119},{"sourceId":8941212,"sourceType":"datasetVersion","datasetId":5355934},{"sourceId":8945052,"sourceType":"datasetVersion","datasetId":5322809},{"sourceId":9012537,"sourceType":"datasetVersion","datasetId":5374587},{"sourceId":9041806,"sourceType":"datasetVersion","datasetId":5346563},{"sourceId":186587463,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"CRIA_BASE_TRAIN= True\nFOLDS = 5","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:07.295988Z","iopub.execute_input":"2024-07-26T19:26:07.298555Z","iopub.status.idle":"2024-07-26T19:26:07.306399Z","shell.execute_reply.started":"2024-07-26T19:26:07.298485Z","shell.execute_reply":"2024-07-26T19:26:07.304985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### imports","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --find-links=/kaggle/input/downloads-isic/ /kaggle/input/downloads-isic/efficientnet-1.1.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:07.309010Z","iopub.execute_input":"2024-07-26T19:26:07.309409Z","iopub.status.idle":"2024-07-26T19:26:22.892002Z","shell.execute_reply.started":"2024-07-26T19:26:07.309378Z","shell.execute_reply":"2024-07-26T19:26:22.890457Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport random\nimport pickle\nimport warnings\nimport io\nimport typing as tp\nfrom itertools import chain\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nfrom tqdm import tqdm\nimport pywt\nimport librosa\nimport cv2\nimport h5py\nfrom PIL import Image\nfrom io import BytesIO\n\nimport tensorflow as tf\nfrom tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\nimport efficientnet.tfkeras as efn\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n\nimport lightgbm as lgb\nfrom lightgbm import log_evaluation, early_stopping\n\n# Configurações\npd.set_option('display.max_rows', 10)\nwarnings.simplefilter(\"ignore\", pd.errors.PerformanceWarning)  # Suprimir todos os PerformanceWarning globalmente\n\n# Limpeza de memória\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:22.894140Z","iopub.execute_input":"2024-07-26T19:26:22.894629Z","iopub.status.idle":"2024-07-26T19:26:23.349621Z","shell.execute_reply.started":"2024-07-26T19:26:22.894590Z","shell.execute_reply":"2024-07-26T19:26:23.348122Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### config gpu","metadata":{}},{"cell_type":"code","source":"# Definindo quais GPUs utilizar\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\n# Verifica as GPUs disponíveis e configura para crescimento de memória\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Configura a estratégia de distribuição\nif len(gpus) <= 1:\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f'Using {len(gpus)} GPU')\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} GPUs')\n\n# Habilita mixed precision se necessário\nMIX = True\nif MIX:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n    print('Mixed precision enabled')\nelse:\n    print('Using full precision')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.351348Z","iopub.execute_input":"2024-07-26T19:26:23.351825Z","iopub.status.idle":"2024-07-26T19:26:23.367955Z","shell.execute_reply.started":"2024-07-26T19:26:23.351785Z","shell.execute_reply":"2024-07-26T19:26:23.366747Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# funçoes","metadata":{}},{"cell_type":"markdown","source":"set_random_seed","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.random.set_seed(seed)\n    if deterministic:\n        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    else:\n        os.environ.pop('TF_DETERMINISTIC_OPS', None)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.371466Z","iopub.execute_input":"2024-07-26T19:26:23.371867Z","iopub.status.idle":"2024-07-26T19:26:23.381985Z","shell.execute_reply.started":"2024-07-26T19:26:23.371835Z","shell.execute_reply":"2024-07-26T19:26:23.380721Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DISTANCIAS EMBEDS\n* df_vetores_controle\n* calcular_distancias_treino_teste\n* calcular_distancias_treino_teste_idx_2_especial","metadata":{}},{"cell_type":"code","source":"def df_vetores_controle(df_, embeds_data, col_saudavel, q_inf=0.1, col_group='patient_id', name_='b0'):\n   \n    embeds_data_df = pd.DataFrame(embeds_data)\n    \n    # Passo 1: Encontrar o quartil 0.1 de cada paciente\n    quartil_01 = df_.groupby(col_group)[col_saudavel].quantile(q_inf).reset_index()\n    quartil_01.columns = [col_group, 'col_saudavel_quartil_01']\n\n    # Merge para adicionar a coluna de quartil_01 ao DataFrame original\n    df_merged = pd.merge(df_, quartil_01, on=col_group, how='left')\n    \n    # Filtrar as linhas que são menores ou iguais ao quartil 0.1\n    df_filtered = df_merged[df_merged[col_saudavel] <= df_merged['col_saudavel_quartil_01']]\n\n    # Passo 2: Criar embeds_data_sadios para cada paciente\n    embeds_data_sadios = {}\n    for paciente_id in df_filtered[col_group].unique():\n        indices = df_filtered[df_filtered[col_group] == paciente_id].index\n        embeds_data_sadios[paciente_id] = embeds_data_df.loc[indices].mean(axis=0)\n    \n    # Passo 3: Calcular a diferença de cada embed do paciente contra o embeds_data_sadios\n    def calcular_diferenca(row):\n        if row.name % 10_000==0:   \n            print(f'--{row.name}', end=' ')\n        paciente_id = row[col_group]\n        embed_data_sadio = embeds_data_sadios[paciente_id]\n        embed_data_atual = embeds_data_df.loc[row.name]\n        distancia_cos = distance.cosine(embed_data_sadio, embed_data_atual) \n        distancia_euclidiana = np.linalg.norm(embed_data_sadio - embed_data_atual)\n        return distancia_cos, distancia_euclidiana\n    \n    print('calculando df_vetores_controle ------------------')\n    df_[f'distancia_sauldavel_cos_{name_}'], df_[f'distancia_sauldavel_eucli_{name_}'] = zip(*df_.apply(calcular_diferenca, axis=1))\n    \n    # Passo 5: Retornar a coluna com os dados\n    return df_, [f'distancia_sauldavel_cos_{name_}',f'distancia_sauldavel_eucli_{name_}']\n\ndef calcular_distancias_treino_teste(df_train_, df_test_, embeds_data_train, embeds_data_test, is_train=True, name_='b0'):\n    # Transformar os dados de embeddings em DataFrames\n    embeds_data_df_train = pd.DataFrame(embeds_data_train)\n\n    df_positivos = df_train_[df_train_['target'] == 1]\n    media_geral = embeds_data_df_train.loc[df_positivos.index].mean(axis=0)\n    \n    def calcular_media_sem_o_atual(index, n):\n        # Calcula a nova média excluindo o vetor de embedding atual do cálculo da média\n        soma_total = media_geral * n\n        vetor_atual = embeds_data_df_train.loc[index]\n        nova_media = (soma_total - vetor_atual) / (n - 1)\n        return nova_media\n    \n    def calcular_diferenca(row, embeds_totais, n, is_train=True):\n        if row.name % 10_000==0:   \n            print(f'--{row.name}', end=' ')\n        if is_train and row['target'] == 1:\n            embed_data_doente = calcular_media_sem_o_atual(row.name, len(df_positivos))\n        else:\n            embed_data_doente = media_geral\n        embed_data_atual = embeds_totais.loc[row.name]\n        distancia_cos = distance.cosine(embed_data_doente, embed_data_atual)\n        distancia_euclidiana = np.linalg.norm(embed_data_doente - embed_data_atual)\n        return distancia_cos, distancia_euclidiana\n    print('calculando distâncias ------------------')\n    if is_train:\n        resultado = df_train_.apply(lambda row: calcular_diferenca(row, embeds_data_df_train, len(df_positivos), is_train=True), axis=1)\n        df_train_[f'dist_pos_cos_{name_}'], df_train_[f'dist_pos_eucli_{name_}'] = zip(*resultado)\n    else:\n        embeds_data_df_test = pd.DataFrame(embeds_data_test)\n        resultado = df_test_.apply(lambda row: calcular_diferenca(row, embeds_data_df_test, len(df_positivos), is_train=False), axis=1)\n        df_test_[f'dist_pos_cos_{name_}'], df_test_[f'dist_pos_eucli_{name_}'] = zip(*resultado)\n\n    return df_train_, df_test_, [f'dist_pos_cos_{name_}',f'dist_pos_eucli_{name_}']\n\ndef calcular_distancias_treino_teste_idx_2_especial(df_train_, df_test_, embeds_data_train, embeds_data_test, is_train=True, name_='b0'):\n    # Transformar os dados de embeddings em DataFrames\n    embeds_data_df_train = pd.DataFrame(embeds_data_train)\n    embeds_data_df_test = pd.DataFrame(embeds_data_test)\n\n    # Filtra os dados de acordo com as condições especificadas\n    condicoes = {\n        'Malignant epidermal proliferations': None,\n        'Malignant adnexal epithelial proliferations - Follicular': None,\n        'Malignant melanocytic proliferations (Melanoma)': None,\n        'Indeterminate epidermal proliferations':None,\n        'Indeterminate melanocytic proliferations':None,\n        'Benign melanocytic proliferations':None,\n        'Benign epidermal proliferations':None,\n        'Benign soft tissue proliferations - Fibro-histiocytic':None,\n        'Flat melanotic pigmentations - not melanocytic nevus':None,\n        'Inflammatory or infectious diseases':None,\n    }\n    # Calcula a média dos embeddings para cada condição\n    for condicao in condicoes.keys():\n        df_positivos = df_train_[(df_train_['target'] == 1) & (df_train_['iddx_2'] == condicao)]\n        print(df_positivos.shape)\n        if not df_positivos.empty:\n            media_embeddings = embeds_data_df_train.loc[df_positivos.index].mean(axis=0)\n            condicoes[condicao] = media_embeddings\n        else:\n            condicoes[condicao] = np.zeros(embeds_data_df_train.shape[1])\n\n    def calcular_media_sem_o_atual(media_geral_, index, n):\n        soma_total = media_geral_ * n\n        vetor_atual = embeds_data_df_train.loc[index]\n        nova_media = (soma_total - vetor_atual) / (n - 1)\n        return nova_media\n\n    def calcular_diferenca(row, embeds_totais, is_train=True):\n        distancias = {}\n        for condicao, indices in condicoes.items():\n            if is_train and row['target'] == 1 and row['iddx_2'] == condicao:\n                embed_data_condicao = calcular_media_sem_o_atual(indices, row.name, len(indices))\n            else:\n                embed_data_condicao = indices\n            \n            embed_data_atual = embeds_totais.loc[row.name]\n            distancias[f'dist_idx2_cos_{name_}_{condicao}'] = distance.cosine(embed_data_condicao, embed_data_atual)\n            distancias[f'dist_idx2_eucli_{name_}_{condicao}'] = np.linalg.norm(embed_data_condicao - embed_data_atual)\n        return pd.Series(distancias)\n    if is_train:\n        tqdm.pandas(desc=\"Calculando distâncias para dados de treino\")\n        resultado_train = df_train_.progress_apply(lambda row: calcular_diferenca(row, embeds_data_df_train, is_train=True), axis=1)\n        df_train_ = pd.concat([df_train_, resultado_train], axis=1)\n    else:\n        tqdm.pandas(desc=\"Calculando distâncias para dados de teste\")\n        resultado_test = df_test_.progress_apply(lambda row: calcular_diferenca(row, embeds_data_df_test, is_train=False), axis=1)\n        df_test_ = pd.concat([df_test_, resultado_test], axis=1)\n    \n    new_cols=[]\n    for condicao, indices in condicoes.items():\n        new_cols.append(f'dist_idx2_cos_{name_}_{condicao}')\n        new_cols.append(f'dist_idx2_eucli_{name_}_{condicao}')\n    \n    return df_train_, df_test_, new_cols","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.383861Z","iopub.execute_input":"2024-07-26T19:26:23.384249Z","iopub.status.idle":"2024-07-26T19:26:23.422792Z","shell.execute_reply.started":"2024-07-26T19:26:23.384205Z","shell.execute_reply":"2024-07-26T19:26:23.421479Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CALCULO MEDIA DE FEATURES\n* calculate_ratio_to_patient_mean\n* calculate_ratio_to_patient_mean_capped\n* calculate_ratio_to_patient_mean_quartil\n","metadata":{}},{"cell_type":"code","source":"def calculate_ratio_to_patient_mean(df, groupby_key, feature_list):\n    new_columns = []\n    for feature in tqdm(feature_list, desc=\"Processing features\"):\n        # Calcular a média de cada feature por paciente\n        patient_mean = df.groupby(groupby_key)[feature].transform('mean')\n        \n        # Criar uma nova coluna para a razão da feature à média do paciente\n        new_col_name = f\"{feature}_ratio_to_patient_mean\"\n        df[new_col_name] = df[feature] / patient_mean\n        \n        # Adicionar o nome da nova coluna à lista\n        new_columns.append(new_col_name)\n        \n    return df, new_columns\n\n\ndef calculate_ratio_to_patient_mean_capped(df, groupby_key, feature_list, q_inf=0.1, q_sup=0.9):\n    new_columns = []\n    percentiles = {}\n    \n    # Pré-calcular os percentis 0.10 e 0.90 para cada grupo e cada feature\n    for feature in feature_list:\n        percentiles[feature] = df.groupby(groupby_key)[feature].quantile([q_inf, q_sup]).unstack(level=-1)\n    \n    # DataFrame para armazenar as novas colunas\n    new_df = pd.DataFrame(index=df.index)\n    \n    # Iterar sobre cada feature para calcular as razões com uma barra de progresso\n    for feature in tqdm(feature_list, desc=\"Processing features\"):\n        lower_percentile = df[groupby_key].map(percentiles[feature][q_inf])\n        upper_percentile = df[groupby_key].map(percentiles[feature][q_sup])\n        \n        # Capar os valores pelo percentil 0.10 e 0.90 e calcular a média capada por paciente\n        capped_values = df[feature].clip(lower=lower_percentile, upper=upper_percentile)\n        patient_mean_capped = df.groupby(groupby_key)[capped_values.name].transform('mean')\n        \n        # Criar uma nova coluna para a razão da feature à média capada do paciente\n        new_col_name = f\"{feature}_ratio_to_capped_patient_mean\"\n        new_df[new_col_name] = df[feature] / patient_mean_capped\n        \n        # Adicionar o nome da nova coluna à lista\n        new_columns.append(new_col_name)\n        \n    # Concatenar as novas colunas ao DataFrame original\n    df = pd.concat([df, new_df], axis=1)\n    \n    return df, new_columns\n\ndef calculate_ratio_to_patient_mean_quartil(df, col_group, feature_list, q_inf=0.1):\n    new_columns = []\n    \n    # Passo 1: Encontrar o quartil 0.1 de cada paciente e filtrar\n    df['quartil_01'] = df.groupby(col_group)['media_pred_oof'].transform(lambda x: x.quantile(q_inf))\n    df_filtered = df[df['media_pred_oof'] <= df['quartil_01']]\n\n    # Passo 2: Calcular a média dos valores filtrados por paciente para cada feature\n    means = df_filtered.groupby(col_group)[feature_list].mean()\n\n    # Passo 3: Calcular as razões das features à média do quartil do paciente\n    for feature in tqdm(feature_list, desc=\"Processing features\"):\n        # Criar nova coluna de razão\n        new_col_name = f\"{feature}_ratio_to_quartil_patient_mean\"\n        # Calcula a razão de maneira vetorizada usando o .loc e .div\n        df[new_col_name] = df[feature].div(df[col_group].map(means[feature]))\n        new_columns.append(new_col_name)\n    \n    return df, new_columns\n\ndef calculate_ratio_to_patient_mean_capped2(df, groupby_key, feature_list, q_inf=0.1, q_sup=0.9):\n    new_columns = []\n    percentiles = {}\n    \n    # Pré-calcular os percentis 0.10 e 0.90 para cada grupo e cada feature\n    for feature in feature_list:\n        percentiles[feature] = df.groupby(groupby_key)[feature].quantile([q_inf, q_sup]).unstack(level=-1)\n    \n    new_df = pd.DataFrame(index=df.index)\n    \n    for feature in tqdm(feature_list, desc=\"Processing features\"):\n        lower_percentile = df[groupby_key].map(percentiles[feature][q_inf])\n        upper_percentile = df[groupby_key].map(percentiles[feature][q_sup])\n        capped_values = df[feature].clip(lower=lower_percentile, upper=upper_percentile)\n        patient_mean_capped = df.groupby(groupby_key)[capped_values.name].transform('mean')\n        skewness_capped = df.groupby(groupby_key)[capped_values.name].transform('skew')\n#         kurtosis_capped = df.groupby(groupby_key)[capped_values.name].transform('kurt')\n        \n        new_df[f\"{feature}_z_score_capped\"] = (df[feature] - patient_mean_capped) / df.groupby(groupby_key)[capped_values.name].transform('std')\n        new_df[f\"{feature}_min_max_scaled_capped\"] = (capped_values - lower_percentile) / (upper_percentile - lower_percentile)\n        new_df[f\"{feature}_skewness_capped\"] = skewness_capped\n#         new_df[f\"{feature}_kurtosis_capped\"] = kurtosis_capped\n        \n        new_columns.extend([\n            f\"{feature}_z_score_capped\",\n            f\"{feature}_min_max_scaled_capped\",\n            f\"{feature}_skewness_capped\",\n#             f\"{feature}_kurtosis_capped\"\n        ])\n    \n    df = pd.concat([df, new_df], axis=1)\n    return df, new_columns\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# def calculate_ratio_to_patient_mean_capped2(df, groupby_key, feature_list, q_inf=0.1, q_sup=0.9):\n#     new_columns = []\n#     percentiles = {}\n\n#     # Pré-calcular os percentis e curtose\n#     for feature in feature_list:\n#         percentiles[feature] = df.groupby(groupby_key)[feature].quantile([q_inf, q_sup]).unstack(level=-1)\n#         df[f\"{feature}_capped\"] = df[feature].clip(lower=df[groupby_key].map(percentiles[feature][q_inf]),\n#                                                    upper=df[groupby_key].map(percentiles[feature][q_sup]))\n    \n#     # Cálculo de curtose uma vez para cada feature capada\n#     kurtosis = df.groupby(groupby_key).apply(lambda x: x[[f\"{f}_capped\" for f in feature_list]].kurt())\n\n#     new_df = pd.DataFrame(index=df.index)\n    \n#     for feature in tqdm(feature_list, desc=\"Processing features\"):\n#         capped_values = df[f\"{feature}_capped\"]\n#         patient_mean_capped = df.groupby(groupby_key)[capped_values.name].transform('mean')\n#         skewness_capped = df.groupby(groupby_key)[capped_values.name].transform('skew')\n        \n#         new_df[f\"{feature}_z_score_capped\"] = (df[feature] - patient_mean_capped) / df.groupby(groupby_key)[capped_values.name].transform('std')\n#         new_df[f\"{feature}_min_max_scaled_capped\"] = (capped_values - df[groupby_key].map(percentiles[feature][q_inf])) / (df[groupby_key].map(percentiles[feature][q_sup]) - df[groupby_key].map(percentiles[feature][q_inf]))\n#         new_df[f\"{feature}_skewness_capped\"] = skewness_capped\n#         new_df[f\"{feature}_kurtosis_capped\"] = df[groupby_key].map(kurtosis[f\"{feature}_capped\"])\n        \n#         new_columns.extend([\n#             f\"{feature}_z_score_capped\",\n#             f\"{feature}_min_max_scaled_capped\",\n#             f\"{feature}_skewness_capped\",\n#             f\"{feature}_kurtosis_capped\"\n#         ])\n    \n#     df = pd.concat([df, new_df], axis=1)\n#     return df, new_columns\n# def calculate_ratio_to_patient_mean_capped2(df, groupby_key, feature_list, q_inf=0.1, q_sup=0.9):\n#     new_columns = []\n#     percentiles = {}\n    \n#     # Pré-calcular os percentis 0.10 e 0.90 para cada grupo e cada feature\n#     for feature in feature_list:\n#         percentiles[feature] = df.groupby(groupby_key)[feature].quantile([q_inf, q_sup]).unstack(level=-1)\n    \n#     new_df = pd.DataFrame(index=df.index)\n    \n#     for feature in tqdm(feature_list, desc=\"Processing features\"):\n#         lower_percentile = df[groupby_key].map(percentiles[feature][q_inf])\n#         upper_percentile = df[groupby_key].map(percentiles[feature][q_sup])\n#         capped_values = df[feature].clip(lower=lower_percentile, upper=upper_percentile)\n#         patient_mean_capped = df.groupby(groupby_key)[capped_values.name].transform('mean')\n#         skewness_capped = df.groupby(groupby_key)[capped_values.name].transform('skew')\n        \n#         # Usar apply para calcular curtose\n#         kurtosis_capped = df.groupby(groupby_key)[capped_values.name].apply(lambda x: x.kurt())\n        \n#         new_df[f\"{feature}_z_score_capped\"] = (df[feature] - patient_mean_capped) / df.groupby(groupby_key)[capped_values.name].transform('std')\n#         new_df[f\"{feature}_min_max_scaled_capped\"] = (capped_values - lower_percentile) / (upper_percentile - lower_percentile)\n#         new_df[f\"{feature}_skewness_capped\"] = skewness_capped\n#         new_df[f\"{feature}_kurtosis_capped\"] = df[groupby_key].map(kurtosis_capped)\n        \n#         new_columns.extend([\n#             f\"{feature}_z_score_capped\",\n#             f\"{feature}_min_max_scaled_capped\",\n#             f\"{feature}_skewness_capped\",\n#             f\"{feature}_kurtosis_capped\"\n#         ])\n    \n#     df = pd.concat([df, new_df], axis=1)\n#     return df, new_columns","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.424931Z","iopub.execute_input":"2024-07-26T19:26:23.425449Z","iopub.status.idle":"2024-07-26T19:26:23.457538Z","shell.execute_reply.started":"2024-07-26T19:26:23.425414Z","shell.execute_reply":"2024-07-26T19:26:23.455990Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SCORE\n* comp_score\n* comp_scorel","metadata":{}},{"cell_type":"code","source":"def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n    v_gt = abs(np.asarray(solution.values)-1)\n    v_pred = np.array([1.0 - x for x in submission.values])\n    max_fpr = abs(1-min_tpr)\n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    return partial_auc\n\ndef comp_scorel(y_true, y_pred):\n    v_gt = abs(y_true - 1)\n    v_pred = 1.0 - y_pred\n    min_tpr = 0.80\n    max_fpr = abs(1 - min_tpr)\n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    return 'custom_auc', partial_auc, True\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.459105Z","iopub.execute_input":"2024-07-26T19:26:23.459544Z","iopub.status.idle":"2024-07-26T19:26:23.474817Z","shell.execute_reply.started":"2024-07-26T19:26:23.459510Z","shell.execute_reply":"2024-07-26T19:26:23.473572Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"FEA FUNC\n* carregar_oof_embeddings_np\n* carregar_e_preparar_dados_de_treino\n* fit_encode_categorical_columns\n* transform_categorical_columns_test","metadata":{}},{"cell_type":"code","source":"def carregar_oof_embeddings_np(path_embs='/kaggle/input/modelos-down/oof0_embedding.npy', \n                               n_comp=32, verbose=True, model_n=3):\n    # Carregamento dos dados\n    embed_data = np.load(path_embs)\n    \n    # Aplicação do PCA para reduzir a dimensionalidade\n    pca = PCA(n_components=n_comp)\n    principal_components = pca.fit_transform(embed_data)\n    \n    # Criação do DataFrame com os componentes principais\n    columns = [f'pred_embed_{i}_b{model_n}' for i in range(n_comp)]\n    pca_df = pd.DataFrame(data=principal_components, columns=columns)\n    \n    # Exibição opcional do resultado\n    if verbose:\n        print(pca_df.shape)\n        display(pca_df.head(2))\n    \n    return embed_data, pca, pca_df, columns","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.476595Z","iopub.execute_input":"2024-07-26T19:26:23.477044Z","iopub.status.idle":"2024-07-26T19:26:23.493760Z","shell.execute_reply.started":"2024-07-26T19:26:23.477002Z","shell.execute_reply":"2024-07-26T19:26:23.492509Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def carregar_e_preparar_dados_de_treino_old(df_meta, oof_paths, pca_df=None):\n    # Carregar metadados\n    new_cols = []\n    \n    # Processar cada arquivo OOF\n    for key, value in oof_paths.items():\n        df_temp = pd.read_csv(value)\n        # Verificar a ordem dos 'isic_id'\n        if not df_meta['isic_id'].equals(df_temp['isic_id']):\n            raise ValueError(f'Discrepância na ordem dos isic_id entre df_meta e {value}')\n        df_temp = df_temp.rename(columns={\"pred_oof\": key})  # Renomeia a coluna de predições\n        if 'isic_id' in df_temp.columns:\n            if key == 'pred_oof0':\n                df_meta[[key, 'fold']] = df_temp[[key, 'fold']]\n            else:\n                df_meta[key] = df_temp[key]\n                if (df_meta['fold'] != df_temp['fold']).all():\n                    raise ValueError(f'Discrepância nos folds entre df_meta e {value}')\n            new_cols.append(key)\n    \n    # Adicionar componentes PCA se fornecido\n    if pca_df is not None:\n        for pca__ in pca_df:\n            df_meta = pd.concat([df_meta, pca__], axis=1)\n            new_cols = new_cols + pca__.columns.to_list()\n\n    gc.collect()\n    return df_meta, new_cols","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.495785Z","iopub.execute_input":"2024-07-26T19:26:23.496171Z","iopub.status.idle":"2024-07-26T19:26:23.507895Z","shell.execute_reply.started":"2024-07-26T19:26:23.496139Z","shell.execute_reply":"2024-07-26T19:26:23.506750Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fit_encode_categorical_columns(df, cat_cols):\n    encoder = OrdinalEncoder(\n        categories='auto',\n        dtype=int,\n        handle_unknown='use_encoded_value',\n        unknown_value=-2,  # Valor para categorias desconhecidas\n        encoded_missing_value=-1  # Valor para dados faltantes\n    )\n\n    # Ajusta o codificador e transforma as colunas categóricas\n    X_cat = encoder.fit_transform(df[cat_cols])\n\n    # Substitui as colunas originais pelas codificadas\n    for c, cat_col in enumerate(cat_cols):\n        df[cat_col] = X_cat[:, c]\n\n    return df, encoder\n\ndef transform_categorical_columns_test(df, cat_cols, encoder):\n    X_cat = encoder.transform(df[cat_cols])\n\n    # Substitui as colunas originais pelas codificadas\n    for c, cat_col in enumerate(cat_cols):\n        df[cat_col] = X_cat[:, c]\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.509475Z","iopub.execute_input":"2024-07-26T19:26:23.509854Z","iopub.status.idle":"2024-07-26T19:26:23.526850Z","shell.execute_reply.started":"2024-07-26T19:26:23.509824Z","shell.execute_reply":"2024-07-26T19:26:23.525426Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"FEATURE ENGINEERING","metadata":{}},{"cell_type":"code","source":"num_cols = [\n    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', \n]\ncat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"]\n\n#mudado linha que muda explicitamente dftrain por df \ndef feature_engineering(df):\n    # New features to try...\n    if \"num_rows_per_patient\" not in df.columns:\n        print('Primeira parte das features')\n        df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n        df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n        df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n        df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n        df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n        df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n        df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n        df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n        df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n        df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n        df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n        df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n        df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n\n        print('Segunda parte das features')\n        df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n        df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n        df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n        df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n        df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n        df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n        df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n        df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n        df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n        df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n#         df[\"3d_lesion_orientation\"] = np.arctan2(df_train[\"tbp_lv_y\"], df_train[\"tbp_lv_x\"])\n        df[\"3d_lesion_orientation\"] = np.arctan2(df[\"tbp_lv_y\"], df[\"tbp_lv_x\"])\n        df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n        df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n        df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n\n        print('Terceira parte das features')\n        df['num_rows_per_patient'] = df.groupby('patient_id')['patient_id'].transform('count')\n    \n    new_num_cols = [\n        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n\n        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\", \n        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\", \"num_rows_per_patient\"\n    ]\n    \n    new_cat_cols = [\"combined_anatomical_site\"]\n    return df, new_num_cols, new_cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.528546Z","iopub.execute_input":"2024-07-26T19:26:23.528912Z","iopub.status.idle":"2024-07-26T19:26:23.554486Z","shell.execute_reply.started":"2024-07-26T19:26:23.528882Z","shell.execute_reply":"2024-07-26T19:26:23.553161Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cria_features_agregadas(df, color_columns):\n    # Cria estatísticas para cada coluna especificada e para cada paciente\n    features = df.groupby('patient_id')[color_columns].agg(\n        [\n            np.mean,    # Média\n            np.median,  # Mediana\n            np.std,     # Desvio padrão\n            np.min,     # Mínimo\n            np.max      # Máximo\n        ]\n    )\n    \n    # Achatando o MultiIndex nas colunas após o groupby e agg\n    features.columns = ['_'.join(col).strip() for col in features.columns.values]\n\n    # Resetando o índice para mesclar de volta ao DataFrame original\n    features.reset_index(inplace=True)\n\n    # Mescla as features agregadas de volta ao DataFrame original\n    df_with_features = pd.merge(df, features, on='patient_id', how='left')\n\n    # Lista dos nomes das novas colunas\n    new_columns = features.columns.tolist()\n    new_columns.remove('patient_id')  # Removendo o 'patient_id' pois não é uma nova coluna\n\n    return df_with_features, new_columns","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.555947Z","iopub.execute_input":"2024-07-26T19:26:23.556362Z","iopub.status.idle":"2024-07-26T19:26:23.572281Z","shell.execute_reply.started":"2024-07-26T19:26:23.556322Z","shell.execute_reply":"2024-07-26T19:26:23.570981Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TEST FUNCTIONS\n","metadata":{}},{"cell_type":"code","source":"class DataGenerator_old(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, batch_size=8, shuffle=False, fp_hdf_=None, mode='train', size_image=300):\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.mode = mode\n        self.fp_hdf = fp_hdf_\n        self.size_image = size_image\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.data) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n        X, y = self.__data_generation(indexes)\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.data))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples'\n        X = np.zeros((len(indexes), self.size_image, self.size_image, 3), dtype='float32')  # Pre-allocate memory for efficiency\n        y = np.zeros((len(indexes), 1), dtype='float32')\n        \n        with h5py.File(self.fp_hdf, 'r') as hdf:\n            for j, i in enumerate(indexes):\n                row = self.data.iloc[i]\n                isic_id = row['isic_id']\n                img_data = hdf[isic_id][()]\n                img_array = np.array(Image.open(BytesIO(img_data)))\n                img_resized = cv2.resize(img_array, (self.size_image, self.size_image))  # Usar cv2 para redimensionar a imagem\n                img_resized = img_resized / 255.0  # Normalizar os valores dos pixels para o intervalo [0, 1]\n                X[j] = img_resized\n                if self.mode != 'test':\n                    y[j] = row[\"target\"]\n\n        return X, y\n\ndef lrfn(epoch):\n        return [1e-3,1e-3,1e-3,1e-4,1e-4,1e-4,1e-5,1e-5,1e-5][epoch]\n    \nclass pAUC2(tf.keras.metrics.Metric):\n    def __init__(self, name='pAUC', **kwargs):\n        super(pAUC2, self).__init__(name=name, **kwargs)\n        self.predictions = []\n        self.labels = []\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.labels.append(y_true)\n        self.predictions.append(y_pred)\n\n    def result(self):\n        labels = tf.concat(self.labels, axis=0)\n        predictions = tf.concat(self.predictions, axis=0)\n\n        # Use tf.py_function to wrap roc_curve\n        return tf.py_function(self.calculate_pauc, [labels, predictions], Tout=tf.float32)\n\n    def calculate_pauc(self, labels, predictions):\n        labels = labels.numpy()\n        predictions = predictions.numpy()\n        fpr, tpr, thresholds = roc_curve(labels, predictions)\n        idxs = tpr > 0.8\n        if np.any(idxs):\n            return np.float32(np.trapz(tpr[idxs] - 0.8, fpr[idxs]))\n        else:\n            return np.float32(0.0)\n\n    def reset_states(self):\n        self.labels = []\n        self.predictions = []\n        \nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n# Criar uma instância da métrica AUC\naucMETRIC = tf.keras.metrics.AUC(name='auc')\n        \ndef build_EfficientNet(input_shape=(300, 300, 3), num_classes=1, model_weights=None, weight_decay=0.001, dropout_rate=0.2, model_n = 3):\n    inp = tf.keras.Input(shape=input_shape)\n\n    # Base model, sem top layer\n    if model_n==0:\n        base_model = efn.EfficientNetB0(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    elif model_n==3:\n        base_model = efn.EfficientNetB3(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    elif model_n==5:\n        base_model = efn.EfficientNetB5(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    \n\n    # Adicionar regularização L2\n    for layer in base_model.layers:\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            layer.kernel_regularizer = tf.keras.regularizers.l2(weight_decay)\n\n    # OUTPUT\n    x = base_model(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.Dense(num_classes,activation='sigmoid', dtype='float32')(x)\n\n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n    loss = BinaryCrossentropy()\n\n    model.compile(loss=loss, optimizer = opt, metrics=[aucMETRIC])\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.580214Z","iopub.execute_input":"2024-07-26T19:26:23.580644Z","iopub.status.idle":"2024-07-26T19:26:23.627662Z","shell.execute_reply.started":"2024-07-26T19:26:23.580613Z","shell.execute_reply":"2024-07-26T19:26:23.626262Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_predict_old(models_paths, data_generator, image_size, model_n, oof_n):\n    # DataFrame para armazenar as previsões finais\n    df_predictions = pd.DataFrame()\n    new_cols=[]\n    # Seleciona a GPU\n    \n    for i in range(len(models_paths[0])):\n        pred = 0\n        for fold in range(len(models_paths)):\n            with strategy.scope():\n                model_path = models_paths[fold][i]\n                print(f\"Loading model from: {model_path}\")\n                model = build_EfficientNet(input_shape=(image_size, image_size, 3), num_classes=1, model_weights=None, weight_decay=0.001, dropout_rate=0.2, model_n = model_n)\n                model.load_weights(model_path)\n                pred += model.predict(data_generator, verbose=1)\n                print(f\"Current prediction sum: {pred}\")\n\n            # Limpeza de memória\n            tf.keras.backend.clear_session()\n            gc.collect()\n\n        # Média das previsões\n        final_pred = pred / len(models_paths)\n        print(f\"Final prediction for model {i}: {final_pred}\")\n        final_pred = np.squeeze(final_pred)\n        df_predictions[f'pred_oof{oof_n}'] = final_pred\n        new_cols.append(f'pred_oof{oof_n}')\n    return df_predictions,new_cols","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.628972Z","iopub.execute_input":"2024-07-26T19:26:23.629362Z","iopub.status.idle":"2024-07-26T19:26:23.640822Z","shell.execute_reply.started":"2024-07-26T19:26:23.629322Z","shell.execute_reply":"2024-07-26T19:26:23.639583Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pred_embeds_old(models_path, df_test, valid_gen, pca_, n_comp=32, image_size=300, model_n=3):\n    def find_layer_by_prefix(model, prefix):\n        for layer in model.layers:\n            if layer.name.startswith(prefix):\n                return layer\n        return None\n\n    pred = 0\n    \n    for path in models_path:\n        with strategy.scope():\n            print(f\"Loading model from: {path}\")\n            model = model = build_EfficientNet(input_shape=(image_size, image_size, 3), num_classes=1, model_weights=None, weight_decay=0.001, dropout_rate=0.2, model_n = model_n)\n            model.load_weights(path)\n            layer = find_layer_by_prefix(model, 'global_average_pooling2d')\n            model_embedding = tf.keras.Model(inputs=model.input, \n                                             outputs=model.get_layer(layer.name).output)\n            pred = model_embedding.predict(valid_gen, verbose=1)\n\n        # Liberar memória\n        del model\n        tf.keras.backend.clear_session()\n        gc.collect()\n\n    # Média das previsões\n    final_p = pred / len(models_path)\n    print(f\"Final prediction: {final_p.shape}\")\n\n    principal_components_test = pca_.transform(final_p)\n\n    # Criação do DataFrame com os componentes principais\n    columns_to_save = [f'pred_embed_{i}_b{model_n}' for i in range(n_comp)]  # Apenas 10 colunas\n    principal_df_test = pd.DataFrame(data=principal_components_test, columns=columns_to_save)\n    df_test = pd.concat([df_test, principal_df_test], axis=1)\n    print(df_test.shape)\n    return df_test, final_p,columns_to_save","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.642466Z","iopub.execute_input":"2024-07-26T19:26:23.642839Z","iopub.status.idle":"2024-07-26T19:26:23.657389Z","shell.execute_reply.started":"2024-07-26T19:26:23.642808Z","shell.execute_reply":"2024-07-26T19:26:23.655901Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\nfrom tqdm import tqdm\n\n\ndef calculate_ratio_to_patient_mean_capped(df, groupby_key, feature_list, q_inf=0.1, q_sup=0.9):\n    new_columns = []\n    percentiles = {}\n\n    # Pré-calcular os percentis 0.10 e 0.90 para cada grupo e cada feature\n    for feature in feature_list:\n        percentiles[feature] = df.groupby(groupby_key)[feature].quantile([q_inf, q_sup]).unstack(level=-1)\n\n    # DataFrame para armazenar as novas colunas\n    new_df = pd.DataFrame(index=df.index)\n\n    # Iterar sobre cada feature para calcular as razões com uma barra de progresso\n    for feature in tqdm(feature_list, desc=\"Processing features\"):\n        lower_percentile = df[groupby_key].map(percentiles[feature][q_inf])\n        upper_percentile = df[groupby_key].map(percentiles[feature][q_sup])\n\n        # Capar os valores pelo percentil 0.10 e 0.90 e calcular a média capada por paciente\n        capped_values = df[feature].clip(lower=lower_percentile, upper=upper_percentile)\n        patient_mean_capped = df.groupby(groupby_key)[capped_values.name].transform('mean')\n\n        # Criar uma nova coluna para a razão da feature à média capada do paciente\n        new_col_name = f\"{feature}_ratio_to_capped_patient_mean\"\n        new_df[new_col_name] = df[feature] / patient_mean_capped\n\n        # Adicionar o nome da nova coluna à lista\n        new_columns.append(new_col_name)\n\n    # Concatenar as novas colunas ao DataFrame original\n    df = pd.concat([df, new_df], axis=1)\n\n    return df, new_columns\n\nnum_cols = [\n    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean',\n    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n]\ncat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"]\n\ndef feature_engineering(df):\n    # New features to try...\n    if \"num_rows_per_patient\" not in df.columns:\n        print('Primeira parte das features')\n        df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n        df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n        df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n        df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n        df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n        df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n        df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n        df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2)\n        df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n        df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n        df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n        df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n        df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n\n        print('Segunda parte das features')\n        df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n        df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n        df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n        df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n        df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n        df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n        df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n        df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n        df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n        df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n        df[\"3d_lesion_orientation\"] = np.arctan2(df[\"tbp_lv_y\"], df[\"tbp_lv_x\"])\n        df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n        df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n        df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n\n        print('Terceira parte das features')\n        df['num_rows_per_patient'] = df.groupby('patient_id')['patient_id'].transform('count')\n        # Calculando a razão do sinal versus cor da pele\n        df['skin_color_signal_ratio'] = df['tbp_lv_H'] / df['tbp_lv_deltaLBnorm']\n\n    new_num_cols = [\n        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n\n        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\",\n        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\", \"num_rows_per_patient\", 'skin_color_signal_ratio'\n    ]\n\n    new_cat_cols = [\"combined_anatomical_site\"]\n    return df, new_num_cols, new_cat_cols\n\n\ndef carregar_e_preparar_dados_de_treino(path_metadata):\n    df_meta = pd.read_csv(path_metadata)\n    return df_meta\ndef handle_infinities_and_nans(df, columns):\n    # Substitui infinitos por NaN\n    df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)\n    # Preenche NaNs com a média ou mediana da coluna, ajuste conforme necessário\n    for column in columns:\n        df[column].fillna(df[column].mean(), inplace=True)\n    return df\n\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nimport pandas as pd\n\n# Função para carregar e preparar dados de treino e teste\ndef carregar_e_preparar_dados(path_metadata, train=True):\n    df = pd.read_csv(path_metadata)\n    # Adicione aqui o código específico de preparação de dados que você usa para treino e teste\n    return df\n\n# Função principal de feature engineering e transformação\ndef processar_dados(df, encoder=None, scaler=None, train=True, onehot_columns=None, onehot_column_names=None):\n    df, new_num_cols, new_cat_cols = feature_engineering(df)\n    num_cols_ = num_cols + new_num_cols\n    cat_cols_ = cat_cols + new_cat_cols\n    df, new_feature_names = calculate_ratio_to_patient_mean_capped(df, 'patient_id', num_cols_, q_inf=0.01, q_sup=0.99)\n    num_cols_ = num_cols_ + new_feature_names\n\n    # Codificação ordinal para variáveis categóricas\n    if train:\n        encoder = OrdinalEncoder(\n            categories='auto',\n            dtype=int,\n            handle_unknown='use_encoded_value',\n            unknown_value=-2,  # Valor para categorias desconhecidas\n            encoded_missing_value=-1  # Valor para dados faltantes\n        )\n        df[cat_cols_] = encoder.fit_transform(df[cat_cols_])\n    else:\n        df[cat_cols_] = encoder.transform(df[cat_cols_])\n\n        \n    # Codificação one-hot\n    if train:\n        # Cria um novo DataFrame com colunas one-hot\n        dummies = pd.get_dummies(df[onehot_columns], columns=onehot_columns)\n        onehot_column_names = dummies.columns.tolist()\n        # Concatena o novo DataFrame de dummies com o original, excluindo as colunas que foram transformadas\n        df = pd.concat([df.drop(columns=onehot_columns), dummies], axis=1)\n    else:\n        # Assegura que o DataFrame de teste inclui as mesmas colunas dummies do treino, preenchendo com zeros onde não houver dados\n        dummies = pd.get_dummies(df[onehot_columns], columns=onehot_columns)\n        dummies = dummies.reindex(columns=onehot_column_names, fill_value=0)\n        df = pd.concat([df.drop(columns=onehot_columns), dummies], axis=1)\n        \n        \n    # Normalização de todas as features usando MinMaxScaler\n    all_features = num_cols_ + cat_cols_+onehot_column_names\n    df = handle_infinities_and_nans(df, all_features)\n    \n    if train:\n        scaler = MinMaxScaler()\n        df[all_features] = scaler.fit_transform(df[all_features])\n    else:\n        df[all_features] = scaler.transform(df[all_features])\n        \n\n    \n    meta_features = all_features\n    n_meta_features = len(meta_features)\n\n    return df, meta_features, n_meta_features, encoder, scaler, onehot_column_names\n\n# Função para obter meta dados da base de treino e teste\ndef get_meta_data(train_path, test_path):\n    # Processando dados de treino\n    df_train = carregar_e_preparar_dados(train_path)\n    df_train, meta_features, n_meta_features, encoder, scaler,onehot_column_names = processar_dados(df_train, train=True, onehot_columns=['anatom_site_general', 'attribution'])\n\n    # Processando dados de teste\n    df_test = carregar_e_preparar_dados(test_path, train=False)\n    df_test, _, _, _, _,_ = processar_dados(df_test, encoder=encoder, scaler=scaler, train=False, onehot_columns=['anatom_site_general', 'attribution'], onehot_column_names=onehot_column_names)\n\n    return df_train, df_test, meta_features, n_meta_features, encoder, scaler,onehot_column_names\n\n# Exemplo de uso\ntrain_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntest_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\ndf_train_meta, df_test_meta, meta_features, n_meta_features, encoder, scaler, onehot_column_names = get_meta_data(train_path, test_path)\n\n\n#EXP1\n# meta_features = [\n#     \"age_approx\",\n#     \"sex\",\n#     # \"anatom_site_general\",\n#     \"tbp_lv_location\",\n#     \"tbp_tile_type\",\n#     \"tbp_lv_location_simple\",\n#     # 'attribution'\n# ]\n# # +onehot_column_names\n# n_meta_features=len(meta_features)\n\n# #EXP2\n# meta_features = [\n#     \"age_approx\",\n#     \"sex\",\n#     # \"anatom_site_general\",\n#     # \"tbp_lv_location\",\n#     # \"tbp_tile_type\",\n#     # \"tbp_lv_location_simple\",\n#     # 'attribution'\n# ]+onehot_column_names\n# n_meta_features=len(meta_features)\n\n#EXP3\nmeta_features = [\n    \"age_approx\",\n    \"sex\",\n    # \"anatom_site_general\",\n    \"tbp_lv_location\",\n    \"tbp_tile_type\",\n    \"tbp_lv_location_simple\",\n    # 'attribution'\n]+onehot_column_names\nn_meta_features=len(meta_features)\n\n# Visualizar as features da base de treino\ndisplay(df_train_meta[meta_features])\n# Visualizar as features da base de teste\ndisplay(df_test_meta[meta_features])\n\nprint(\"Colunas de treino:\", df_train_meta[meta_features].shape)\nprint(\"Colunas de teste:\", df_test_meta[meta_features].shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:23.659592Z","iopub.execute_input":"2024-07-26T19:26:23.660257Z","iopub.status.idle":"2024-07-26T19:26:57.866124Z","shell.execute_reply.started":"2024-07-26T19:26:23.660205Z","shell.execute_reply":"2024-07-26T19:26:57.864844Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport albumentations\n# Função que retorna as transformações para o treinamento e validação\ndef get_transforms(image_size):\n    transforms_train = albumentations.Compose([\n        albumentations.Transpose(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75),\n        #albumentations.RandomContrast(limit=0.2, p=0.75),\n        albumentations.OneOf([\n            albumentations.MotionBlur(blur_limit=5),\n            albumentations.MedianBlur(blur_limit=5),\n            albumentations.GaussianBlur(blur_limit=5),\n            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n        ], p=0.7),\n        albumentations.OneOf([\n            albumentations.OpticalDistortion(distort_limit=1.0),\n            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n            albumentations.ElasticTransform(alpha=3),\n        ], p=0.7),\n        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n        albumentations.Resize(image_size, image_size),\n        #albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n        albumentations.Normalize()\n    ])\n\n    transforms_val = albumentations.Compose([\n        albumentations.Resize(image_size, image_size),\n        albumentations.Normalize()\n    ])\n\n    return transforms_train, transforms_val\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, df_, meta_features, n_meta_features,batch_size=8, shuffle=False, fp_hdf_=None, mode='train', image_size=385):\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.mode = mode\n        self.fp_hdf_ = fp_hdf_\n        self.image_size = image_size\n        self.transforms_train, self.transforms_val = get_transforms(self.image_size)\n        self.df_, self.meta_features, self.n_meta_features = df_, meta_features, n_meta_features\n        # Precompute the mapping from isic_id to meta features\n        self.isic_to_meta = {isic_id: meta_features for isic_id, meta_features in zip(self.df_['isic_id'], self.df_[self.meta_features].values)}\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.data) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n        X, y, meta = self.__data_generation(indexes)\n        # Retornar as entradas como uma lista que contém X e meta\n        return (X, meta), y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.data))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples'\n        X = np.zeros((len(indexes), self.image_size, self.image_size, 3), dtype='float32')  # Pre-allocate memory for efficiency\n        y = np.zeros((len(indexes), 1), dtype='float32')\n        meta = np.zeros((len(indexes), self.n_meta_features), dtype='float32')  # Pre-allocate memory for meta features\n\n        with h5py.File(self.fp_hdf_, 'r') as hdf:\n            for j, i in enumerate(indexes):\n                row = self.data.iloc[i]\n                isic_id = row['isic_id']\n                img_data = hdf[isic_id][()]\n                img_array = np.array(Image.open(BytesIO(img_data)))\n                img_array = cv2.resize(img_array, (self.image_size, self.image_size))\n                if self.mode == 'train':\n                    img_array = self.transforms_train(image=img_array)['image']\n                else:\n                    img_array = self.transforms_val(image=img_array)['image']\n                img_array = img_array / 255.0  # Normalizar os valores dos pixels para o intervalo [0, 1]\n                X[j] = img_array\n                if self.mode != 'test':\n                    y[j] = row[\"target\"]\n                else:\n                    y[j] = 0\n\n                # Adicionar dados de meta-features usando o dicionário precomputado\n                meta[j] = self.isic_to_meta[isic_id].flatten()\n\n        return X, y, meta\n\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport h5py\nfrom PIL import Image\nfrom io import BytesIO\n\ndef lrfn(epoch):\n        return [1e-3,1e-3,1e-3,1e-4,1e-4,1e-4,1e-5,1e-5,1e-5][epoch]\n\n        \nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n# Criar uma instância da métrica AUC\naucMETRIC = tf.keras.metrics.AUC(name='auc')\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import roc_curve\nimport warnings\n\ndef pAUC(truth, preds):\n    if np.sum(truth) == 0:\n        return 0.0  # ou qualquer valor padrão que você considere apropriado\n    fpr, tpr, threshold = roc_curve(truth, preds)\n    idxs = tpr > 0.8\n    return np.trapz(tpr[idxs] - 0.8, fpr[idxs])\n\ndef pAUC_metric(y_true, y_pred):\n    # Converting tensors to numpy arrays\n    y_true = tf.keras.backend.flatten(y_true)\n    y_pred = tf.keras.backend.flatten(y_pred)\n\n    pAUC_value = tf.py_function(func=pAUC, inp=[y_true, y_pred], Tout=tf.float32)\n\n    return pAUC_value\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nimport efficientnet.tfkeras as efn\n\ndef build_EfficientNetB0(input_shape=(385, 385, 3), num_classes=1, model_weights=None, weight_decay=0.001, dropout_rate=0.2, model_n = 3):\n    n_meta_dim=[512, 128]\n    # Input layers for image and metadata\n    inp = layers.Input(shape=input_shape)\n    inp_meta = layers.Input(shape=(n_meta_features,))\n\n        # Base model, sem top layer\n    if model_n==0:\n        base_model = efn.EfficientNetB0(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    elif model_n==3:\n        base_model = efn.EfficientNetB3(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    elif model_n==5:\n        base_model = efn.EfficientNetB5(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    \n\n    # Adding L2 regularization to each convolutional layer\n    for layer in base_model.layers:\n        if isinstance(layer, layers.Conv2D):\n            layer.kernel_regularizer = regularizers.l2(weight_decay)\n\n    # Processing image features\n    x = base_model(inp)\n    x = layers.GlobalAveragePooling2D()(x)\n\n    # Processing metadata features, if provided\n    if n_meta_features > 0:\n        x_meta = layers.Dense(n_meta_dim[0], activation='swish')(inp_meta)\n        x_meta = layers.BatchNormalization()(x_meta)\n        x_meta = layers.Dropout(dropout_rate)(x_meta)\n        x_meta = layers.Dense(n_meta_dim[1], activation='swish')(x_meta)\n        x_meta = layers.BatchNormalization()(x_meta)\n\n        # Concatenate image features and metadata features\n        x = layers.concatenate([x, x_meta])\n\n    # Final classification layer\n    x = layers.Dropout(dropout_rate)(x)\n    output = layers.Dense(num_classes, activation='sigmoid', dtype='float32')(x)\n\n    # Model definition\n    model = models.Model(inputs=[inp, inp_meta], outputs=output)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    loss = tf.keras.losses.BinaryCrossentropy()\n\n    model.compile(loss=loss, optimizer = optimizer, metrics=[pAUC_metric])\n\n    return model\n\ndef pred_embeds_new(models_path, df_test, valid_gen, pca_, n_comp=32, image_size=300, model_n=3):\n    def find_layer_by_prefix(model, prefix):\n        for layer in model.layers:\n            if layer.name.startswith(prefix):\n                return layer\n        return None\n\n    pred = 0\n    \n    for path in models_path:\n        with strategy.scope():\n            print(f\"Loading model from: {path}\")\n            model = model = build_EfficientNetB0(input_shape=(image_size, image_size, 3), num_classes=1, model_weights=None, weight_decay=0.001, dropout_rate=0.2, model_n = model_n)\n            model.load_weights(path)\n            layer = find_layer_by_prefix(model, 'global_average_pooling2d')\n            model_embedding = tf.keras.Model(inputs=model.input, \n                                             outputs=model.get_layer(layer.name).output)\n            pred = model_embedding.predict(valid_gen, verbose=1)\n\n        # Liberar memória\n        del model\n        tf.keras.backend.clear_session()\n        gc.collect()\n\n    # Média das previsões\n    final_p = pred / len(models_path)\n    print(f\"Final prediction: {final_p.shape}\")\n\n    principal_components_test = pca_.transform(final_p)\n\n    # Criação do DataFrame com os componentes principais\n    columns_to_save = [f'pred_embed_{i}_b{model_n}' for i in range(n_comp)]  # Apenas 10 colunas\n    principal_df_test = pd.DataFrame(data=principal_components_test, columns=columns_to_save)\n    df_test = pd.concat([df_test, principal_df_test], axis=1)\n    print(df_test.shape)\n    return df_test, final_p,columns_to_save","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:57.868058Z","iopub.execute_input":"2024-07-26T19:26:57.868561Z","iopub.status.idle":"2024-07-26T19:26:57.926974Z","shell.execute_reply.started":"2024-07-26T19:26:57.868516Z","shell.execute_reply":"2024-07-26T19:26:57.925432Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# path_pre = '/kaggle/input/prepros-isic/preprocessamtnto isic/'\n# df_train = pd.read_parquet(f'{path_pre}df_train_new_features.parquet')\n# def find_layer_by_prefix(model, prefix):\n#     for layer in model.layers:\n#         if layer.name.startswith(prefix):\n#             return layer\n#     return None\n\n# for i in range(3):\n#     df_valid_index = df_train[\"fold\"] == i\n#     df_valid= df_train[df_valid_index]\n#     TEST_HDF = f'/kaggle/input/isic-2024-challenge/train-image.hdf5'\n\n#     set_random_seed(42, deterministic=True)\n#     valid_gen = DataGenerator(df_valid, \n#                           shuffle=False, \n#                           batch_size=256, \n#                           mode='test', \n#                           fp_hdf_=TEST_HDF,\n#                               df_=df_train_meta, \n#                               meta_features=meta_features, \n#                               n_meta_features=n_meta_features)\n\n#     pred = 0\n#     with strategy.scope():\n#         print(f\"Loading model from: {f'/kaggle/input/modelos-down/modelossalvos_auc_down9_novo_b5_aug_meta/MLP_fold_{i}_model_0.h5'}\")\n#         model = build_EfficientNetB0()\n#         model.load_weights(f'/kaggle/input/modelos-down/modelossalvos_auc_down9_novo_b5_aug_meta/MLP_fold_{i}_model_0.h5')\n#         layer = find_layer_by_prefix(model, 'global_average_pooling2d')\n#         model_embedding = tf.keras.Model(inputs=model.input, \n#                                          outputs=model.get_layer(layer.name).output)\n#         pred = model_embedding.predict(valid_gen, verbose=1)\n#         df_train = df_train.copy()\n#         for j in range(pred.shape[1]):\n#             df_train.loc[df_valid_index, f'pred_embed_{j}'] = pred[:, j]\n#         print(f\"Current prediction sum: {pred}\")\n        \n\n#     # Liberar memória\n#     del model\n#     tf.keras.backend.clear_session()\n#     gc.collect()\n\n#     print(pred.shape)\n# columns_to_save = [f'pred_embed_{i}' for i in range(pred.shape[1])]\n# print(f\"Salvando en formato para quê para otimizar.{df_train[columns_to_save].shape}\")\n# data_array = df_train[columns_to_save].to_numpy()\n# np.save('oof0_embedding_B5_M0_DOWN9_aug_meta.npy', data_array)\n# prunt()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:57.929279Z","iopub.execute_input":"2024-07-26T19:26:57.929744Z","iopub.status.idle":"2024-07-26T19:26:57.938813Z","shell.execute_reply.started":"2024-07-26T19:26:57.929701Z","shell.execute_reply":"2024-07-26T19:26:57.937641Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# agrupamentos","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\n# def preprocess_data(embeds):\n#     # Substitui infinitos por NaN em arrays NumPy\n#     embeds = np.where(np.isinf(embeds), np.nan, embeds)\n    \n#     # Imputação de média\n#     imputer = SimpleImputer(strategy='mean')\n#     embeds_filled = imputer.fit_transform(embeds)\n    \n#     # Escalamento dos dados\n#     scaler = StandardScaler()\n#     embeds_scaled = scaler.fit_transform(embeds_filled)\n    \n#     return embeds_scaled\n\n# from sklearn.ensemble import IsolationForest\n\n# def apply_isolation_forest(group, embeds):\n#     # Inicializa scores de anomalia\n#     group['scores'] =0.5 # Predefine todos os scores como 0.5\n\n#     if len(group) >= 20:\n#         # Processa com Isolation Forest usando embeddings\n#         emb = embeds[group.index]\n#         iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n#         iso_forest.fit(emb)\n#         group['scores'] = iso_forest.decision_function(emb)  # Atribui scores do modelo\n    \n# #     print(group)\n# #     print(group.shape)\n#     return group['scores']\n\n# def process_patients(df, embeds):\n#     df_copy = deepcopy(df)\n#     embeds_copy = deepcopy(embeds)\n#     # Preprocessamento de embeddings\n#     embeds_copy = preprocess_data(embeds_copy)\n    \n#     # Aplicação do Isolation Forest com barra de progresso\n#     tqdm.pandas(desc=\"Calculando distâncias para dados de treino\")\n#     results = df_copy.groupby('patient_id').progress_apply(lambda group: apply_isolation_forest(group, embeds_copy))\n# #     print(results)\n#     print(results.shape)\n#     df_copy['score'] = results.values\n#     return df_copy['score'] \n\n# # from sklearn.cluster import KMeans\n# # from sklearn.preprocessing import StandardScaler\n# # import pandas as pd\n# # from tqdm import tqdm\n\n# # def apply_kmeans(group, embeds, n_clusters=5):\n# #     if len(group) < 30:\n# #         group['cluster_labels'] = 0  # Considera não-anômalo\n# #     else:\n# #         # Aplicação do K-Means\n# #         kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n# #         group['cluster_labels'] = kmeans.fit_predict(emb_scaled)\n# #     return group\n\n# # def process_patients_kmeans(df, embeds, n_clusters=10):\n# #     embeds = preprocess_data(embeds)\n# #     # Processando os dados de cada paciente individualmente usando uma barra de progresso\n# #     tqdm.pandas(desc=\"Aplicando K-Means nos dados de cada paciente\")\n# #     results = df.groupby('patient_id').progress_apply(lambda group: apply_kmeans(group, embeds, n_clusters))\n# #     results.reset_index(drop=True, inplace=True)\n# #     return results\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:57.940061Z","iopub.execute_input":"2024-07-26T19:26:57.940459Z","iopub.status.idle":"2024-07-26T19:26:57.959221Z","shell.execute_reply.started":"2024-07-26T19:26:57.940429Z","shell.execute_reply":"2024-07-26T19:26:57.958109Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('future.no_silent_downcasting', True)\ndef paciente_ultima_tentativa_iso_forest(df, cols):\n    df = df.copy()\n    df['scores_iso'] = 0.5\n    for i,pac in enumerate(df.groupby('patient_id')):\n        if i % 20==0:   \n            print(f'--{i}', end=' ')\n        \n        paciente_id = pac[0]\n        df_pac = pac[1]\n        \n        if len(df_pac) >= 20:\n            df_pac1=df_pac.replace([np.inf, -np.inf], np.nan)\n            iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n            iso_forest.fit(df_pac1[cols].fillna(0).values)\n            df.loc[df_pac.index, 'scores_iso'] = iso_forest.decision_function(df_pac1[cols].fillna(0).values)  # A\n#             display(df.loc[df_pac.index, 'scores'])\n#         if i >3:\n#             break\n    return df, ['scores_iso']\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import OneClassSVM\n\npd.set_option('future.no_silent_downcasting', True)\n\ndef paciente_ultima_tentativa_svm(df, cols):\n    print('----- calculando OneClassSVM')\n    df = df.copy()\n    df['scores_svm'] = 0.5  # Inicializar scores\n\n    for i, (paciente_id, df_pac) in enumerate(df.groupby('patient_id')):\n        if i % 20 == 0:\n            print(f'--{i}', end=' ')\n        \n        if len(df_pac) >= 20:\n            df_pac = df_pac.replace([np.inf, -np.inf], np.nan)\n            # Configurar o One-Class SVM\n            oc_svm = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n            oc_svm.fit(df_pac[cols].fillna(0).values)\n            \n            # Decision function retorna valores, quanto menor, mais anômalo é considerado o ponto\n            scores = oc_svm.decision_function(df_pac[cols].fillna(0).values)\n            df.loc[df_pac.index, 'scores_svm'] = scores\n\n    return df, ['scores_svm']\n\n# df_train2 = paciente_ultima_tentativa_svm(df_train_meta, num_cols)\n# df_train2","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:57.960893Z","iopub.execute_input":"2024-07-26T19:26:57.961295Z","iopub.status.idle":"2024-07-26T19:26:57.979782Z","shell.execute_reply.started":"2024-07-26T19:26:57.961257Z","shell.execute_reply":"2024-07-26T19:26:57.978546Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PRE PROCESSAMENTO","metadata":{}},{"cell_type":"markdown","source":"BASE DE TREINO","metadata":{}},{"cell_type":"code","source":"def oof_preds_treino(df_, path_, oof_name='pred_oof0', model_n=3, n_comp=32, verbose=True, mode='save'):\n    \n    if mode == 'save':\n        path_embed_=path_+'embedding_array.npy'\n        path_oof_=path_+'oof_fold_0.csv'\n        embed_data_b0, pca_b0, pca_df_b0, pca_df_columns_b0 = carregar_oof_embeddings_np(path_embs=path_embed_, n_comp=n_comp, verbose=verbose, model_n=model_n)\n        oof_paths = {oof_name: path_oof_}\n        df_, new_cols_embeds_oof = carregar_e_preparar_dados_de_treino_old(df_, oof_paths, [pca_df_b0])\n        # CALCULAR DISTANCIAS DE VETORES \n        df_, new_cols_vetores_controles_b0 = df_vetores_controle(df_, embed_data_b0, oof_name, q_inf=0.1, col_group='patient_id', name_=f'b{model_n}')\n        df_, _, new_cols_dis_pos_b0 = calcular_distancias_treino_teste(df_, None, embed_data_b0, None, is_train=True, name_=f'b{model_n}')\n        \n        #df_=process_patients(df_, pca_df_b0.values)\n        \n        novas_colunas = new_cols_embeds_oof+new_cols_vetores_controles_b0+new_cols_dis_pos_b0#+['scores']\n        if verbose:\n            print('Base de dados de treino OOF PRED')\n            display(df_.head(2))\n            display(df_.shape)\n\n        with open('df_.pkl', 'wb') as f:\n            pickle.dump(df_, f)\n        with open('novas_colunas.pkl', 'wb') as f:\n            pickle.dump(novas_colunas, f)\n    else:\n        with open(f'/kaggle/input/prepros-isic/df_.pkl', 'rb') as f:\n            df_ = pickle.load(f)\n        with open('/kaggle/input/prepros-isic/novas_colunas.pkl',  'rb') as f:\n            novas_colunas = pickle.load(f)\n                  \n    return df_, novas_colunas","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:57.981370Z","iopub.execute_input":"2024-07-26T19:26:57.982201Z","iopub.status.idle":"2024-07-26T19:26:57.996820Z","shell.execute_reply.started":"2024-07-26T19:26:57.982147Z","shell.execute_reply":"2024-07-26T19:26:57.995577Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\npd.set_option('display.max_columns', None)\nset_random_seed(42)\n\n\n\n# TREINO ---------------------------------------------------------\nif CRIA_BASE_TRAIN:\n    df_train = pd.read_csv('/kaggle/input/isic-2024-challenge/train-metadata.csv')\n    df_train, novas_colunas= oof_preds_treino(df_train, \n            path_='/kaggle/input/modelos-down/modelossalvos_auc_down_b3_aug_sem_meta_EXP5_5FOLDS/', \n            oof_name='pred_oof0', \n            model_n=3, n_comp=32, verbose=True, mode='save')\n#     df_train, novas_colunas2= oof_preds_treino(df_train, \n#             path_='/kaggle/input/modelos-down/modelossalvos_auc_down_b3_aug_sem_meta/', \n#             oof_name='pred_oof1', \n#             model_n=33, n_comp=32, verbose=True)\n#     df_train, novas_colunas3= oof_preds_treino(df_train, \n#             path_='/kaggle/input/modelos-down/modelossalvos_auc_down_b5_aug_sem_meta/', \n#             oof_name='pred_oof2', \n#             model_n=5, n_comp=32, verbose=True)\n#     novas_colunas = novas_colunas1+novas_colunas2+novas_colunas3\n    # FEATURE ENGINERING\n    df_train, new_num_cols, new_cat_cols = feature_engineering(df_train.copy())\n    num_cols_ = num_cols+new_num_cols+novas_colunas\n    cat_cols_ = cat_cols+new_cat_cols\n\n    # RATIO POR PACIENTE DAS PRINCIPAIS COLUNAS NUMERICAS\n    df_train, new_feature_names1= calculate_ratio_to_patient_mean_capped2(df_train, 'patient_id', num_cols_, q_inf=0.01, q_sup=0.99)\n#     df_train, new_feature_names2 = cria_features_agregadas(df_train, num_cols_)\n    num_cols_ = num_cols_+new_feature_names1\n\n    # TRANSFORMAÇÃO DE CATEGORIAS EM NUMEROS\n    df_train, category_encoder = fit_encode_categorical_columns(df_train, cat_cols_)\n\n    # FEATURE SELECTION INICIAL\n    train_cols = num_cols_ + cat_cols_\n\n#     df_train, new_col_scores_svm=paciente_ultima_tentativa_svm(df_train, train_cols)\n#     df_train, new_col_scores_iso=paciente_ultima_tentativa_iso_forest(df_train, train_cols)\n#     cluster_cols = train_cols\n#     train_cols = train_cols+new_col_scores_svm+new_col_scores_iso\n    \n    print('Base de dados de treino COM FEATURE SELECTION INICIAL')\n    display(df_train[train_cols].head(2))\n    display(df_train[train_cols].shape)\n    \n    # SALVA O NECESSARIO PARA CSV\n    # Salvando o DataFrame como Parquet com compressão 'snappy'\n    df_train.to_parquet('df_train_new_features.parquet', engine='pyarrow', compression='snappy')\n    # Salvando train_cols\n    with open('train_cols.pkl', 'wb') as f:\n        pickle.dump(train_cols, f)\n    # Salvando category_encoder\n    with open('category_encoder.pkl', 'wb') as f:\n        pickle.dump(category_encoder, f)\n    # Salvando train_cols\n    with open('num_cols_.pkl', 'wb') as f:\n        pickle.dump(num_cols_, f)\n    # Salvando category_encoder\n    with open('cat_cols_.pkl', 'wb') as f:\n        pickle.dump(cat_cols_, f)\nelse:\n    path_pre = '/kaggle/input/prepros-isic/results (6)/'\n    df_train = pd.read_parquet(f'{path_pre}df_train_new_features.parquet')\n    with open(f'{path_pre}train_cols.pkl', 'rb') as f:\n        train_cols = pickle.load(f)\n    with open(f'{path_pre}category_encoder.pkl', 'rb') as f:\n        category_encoder = pickle.load(f)\n    with open(f'{path_pre}num_cols_.pkl', 'rb') as f:\n        num_cols_ = pickle.load(f)\n    with open(f'{path_pre}cat_cols_.pkl', 'rb') as f:\n        cat_cols_ = pickle.load(f)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:26:57.998645Z","iopub.execute_input":"2024-07-26T19:26:57.998995Z","iopub.status.idle":"2024-07-26T19:35:15.763689Z","shell.execute_reply.started":"2024-07-26T19:26:57.998966Z","shell.execute_reply":"2024-07-26T19:35:15.761941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"BASE DE TEST","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nimport efficientnet.tfkeras as efn\ndef build_EfficientNetB0(input_shape=(300, 300, 3), num_classes=1, model_weights=None, weight_decay=0.001, dropout_rate=0.2, model_n = 3):\n    n_meta_dim=[512, 128]\n    # Input layers for image and metadata\n    inp = layers.Input(shape=input_shape)\n    inp_meta = layers.Input(shape=(n_meta_features,))\n\n        # Base model, sem top layer\n    if model_n==0:\n        base_model = efn.EfficientNetB0(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    elif model_n==3:\n        base_model = efn.EfficientNetB3(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n    elif model_n==5:\n        base_model = efn.EfficientNetB5(include_top=False, weights=None, input_tensor=None)\n        base_model.load_weights(f'/kaggle/input/modelos-isic/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n\n    # Adding L2 regularization to each convolutional layer\n    for layer in base_model.layers:\n        if isinstance(layer, layers.Conv2D):\n            layer.kernel_regularizer = regularizers.l2(weight_decay)\n\n    # Processing image features\n    x = base_model(inp)\n    x = layers.GlobalAveragePooling2D()(x)\n    embeddings = layers.Lambda(lambda x: x, name='embeddings')(x)\n\n#     # Processing metadata features, if provided\n#     if n_meta_features > 0:\n#         x_meta = layers.BatchNormalization()(inp_meta)\n#         x_meta = layers.Dense(n_meta_dim[0], activation='relu')(x_meta)\n#         x_meta = layers.BatchNormalization()(x_meta)\n#         x_meta = layers.Dropout(dropout_rate)(x_meta)\n#         x_meta = layers.Dense(n_meta_dim[1], activation='relu')(x_meta)\n#         x_meta = layers.BatchNormalization()(x_meta)\n\n#         # Concatenate image features and metadata features\n#         x = layers.concatenate([x, x_meta])\n\n#     # Adding BatchNormalization after concatenation\n#     x = layers.BatchNormalization()(x)\n\n    # Final classification layer\n    x = layers.Dropout(dropout_rate)(x)\n    classification_output = layers.Dense(num_classes, activation='sigmoid', name='c_out', dtype='float32')(x)\n\n    # Model definition\n    model = models.Model(inputs=[inp, inp_meta], outputs=[classification_output, embeddings])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    loss = {'c_out': tf.keras.losses.BinaryCrossentropy(), 'embeddings': None}\n\n    model.compile(loss=loss, optimizer=optimizer, metrics={'c_out': [pAUC_metric]})\n\n    return model\n\ndef pred_embeds_new2(models_path, df_test, valid_gen, pca_, n_comp=32, image_size=300, model_n=3, oof_col_name='pred_oof0',tag_=3):\n    pred_oof = 0   \n    pred_embeds = 0   \n    for path in models_path:\n        with strategy.scope():\n            print(f\"Loading model from: {path}\")\n            model = model = build_EfficientNetB0(input_shape=(image_size, image_size, 3), model_n = model_n)\n            model.load_weights(path)\n            pred_ = model.predict(valid_gen, verbose=1)\n            pred_oof=pred_[0]\n            pred_embeds=pred_[1]\n        # Liberar memória\n        del model\n        tf.keras.backend.clear_session()\n        gc.collect()\n\n    # Média das previsões\n    final_p = pred_embeds / len(models_path)\n    print(f\"Final prediction: {final_p.shape}\")\n    principal_components_test = pca_.transform(final_p)\n    # Criação do DataFrame com os componentes principais\n    columns_to_save = [f'pred_embed_{i}_b{tag_}' for i in range(n_comp)]  # Apenas 10 colunas\n    principal_df_test = pd.DataFrame(data=principal_components_test, columns=columns_to_save)\n    df_test = pd.concat([df_test, principal_df_test], axis=1)\n    print(df_test.shape)\n    \n    # Média das previsões\n    final_pred = pred_oof / len(models_path)\n    final_pred = np.squeeze(final_pred)\n    df_test[oof_col_name] = final_pred\n    \n    \n    return df_test, final_p,columns_to_save,principal_df_test","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:35:15.765509Z","iopub.execute_input":"2024-07-26T19:35:15.765888Z","iopub.status.idle":"2024-07-26T19:35:15.790763Z","shell.execute_reply.started":"2024-07-26T19:35:15.765856Z","shell.execute_reply":"2024-07-26T19:35:15.789045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def oof_preds_test(df_test, df_train, path_, oof_name='pred_oof0', model_n=3, n_comp=32, verbose=True, image_size=300, tag_=3):\n    path_embed_=path_+'embedding_array.npy'\n    embed_data, pca, pca_df, pca_df_columns = carregar_oof_embeddings_np(path_embs=path_embed_, n_comp=n_comp, verbose=verbose, model_n=tag_)    \n    models_path = [\n            f'{path_}MLP_fold_0_model_0.h5',\n            f'{path_}MLP_fold_1_model_0.h5',\n            f'{path_}MLP_fold_2_model_0.h5',\n            f'{path_}MLP_fold_3_model_0.h5',\n            f'{path_}MLP_fold_4_model_0.h5'\n        ]\n    test_hdf = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n    valid_gen = DataGenerator(df_test, shuffle=False, batch_size=256, mode='test', \n                              fp_hdf_=test_hdf, image_size=image_size, \n                              df_=df_test_meta, meta_features=meta_features, n_meta_features=n_meta_features)\n    df_test, embeds_test, new_col_pca,pca_df_test = pred_embeds_new2(models_path, df_test, valid_gen, pca, n_comp=n_comp, image_size=image_size, model_n=model_n, oof_col_name=oof_name, tag_=tag_)\n    \n    nel_col_oof_test=new_col_pca+[oof_name]\n    \n    # CALCULAR DISTANCIAS DE VETORES \n    df_test, new_cols_vetores_controles = df_vetores_controle(df_test, embeds_test, oof_name, q_inf=0.1, col_group='patient_id', name_=f'b{tag_}')\n    _, df_test, new_cols_dis_pos = calcular_distancias_treino_teste(df_train, df_test, embed_data, embeds_test, is_train=False, name_=f'b{tag_}')\n    \n    #df_test=process_patients(df_test, pca_df_test.values)\n    \n    novas_colunas = nel_col_oof_test+new_cols_vetores_controles+new_cols_dis_pos#+['scores']\n    if verbose:\n        print('Base de dados de treino OOF PRED')\n        display(df_test.head(2))\n        display(df_test.shape)\n        \n    return df_test, novas_colunas\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:35:15.792365Z","iopub.execute_input":"2024-07-26T19:35:15.792781Z","iopub.status.idle":"2024-07-26T19:35:15.809091Z","shell.execute_reply.started":"2024-07-26T19:35:15.792744Z","shell.execute_reply":"2024-07-26T19:35:15.807928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n    \n# TEST ------------------------------------------------------------\ndf_test = pd.read_csv(f'/kaggle/input/isic-2024-challenge/test-metadata.csv')\ntest_hdf = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\nset_random_seed(42)\n\ndf_test, novas_colunas= oof_preds_test(df_test, df_train,\n        path_='/kaggle/input/modelos-down/modelossalvos_auc_down_b3_aug_sem_meta_EXP5_5FOLDS/', \n        oof_name='pred_oof0', \n        model_n=3, n_comp=32, verbose=True, image_size=300, tag_=3)\n# df_test, novas_colunas2= oof_preds_test(df_test, df_train,\n#         path_='/kaggle/input/modelos-down/modelossalvos_auc_down_b3_aug_sem_meta/', \n#         oof_name='pred_oof1', \n#         model_n=3, n_comp=32, verbose=True, image_size=300, tag_=33)\n# df_test, novas_colunas3= oof_preds_test(df_test, df_train,\n#         path_='/kaggle/input/modelos-down/modelossalvos_auc_down_b5_aug_sem_meta/', \n#         oof_name='pred_oof2', \n#         model_n=5, n_comp=32, verbose=True, image_size=384, tag_=5)\n# novas_colunas = novas_colunas1+novas_colunas2+novas_colunas3\n# FEATURE ENGINERING\ndf_test, new_num_cols, new_cat_cols = feature_engineering(df_test.copy())\nnum_cols_ = num_cols+new_num_cols+novas_colunas\ncat_cols_ = cat_cols+new_cat_cols\n\n# RATIO POR PACIENTE DAS PRINCIPAIS COLUNAS NUMERICAS\ndf_test, new_feature_names1= calculate_ratio_to_patient_mean_capped2(df_test, 'patient_id', num_cols_, q_inf=0.01, q_sup=0.99)\n# df_test, new_feature_names2 = cria_features_agregadas(df_test, num_cols_)\nnum_cols_ = num_cols_+new_feature_names1\n\n# TRANSFORMAÇÃO DE CATEGORIAS EM NUMEROS\ndf_test, category_encoder = fit_encode_categorical_columns(df_test, cat_cols_)\n\n# df_test, new_col_scores_svm=paciente_ultima_tentativa_svm(df_test, cluster_cols)\n# df_test, new_col_scores_iso=paciente_ultima_tentativa_iso_forest(df_test, cluster_cols)\n\nprint('Base de dados de treino COM FEATURE SELECTION INICIAL')\ndisplay(df_train[train_cols].head(2))\ndisplay(df_train[train_cols].shape)\n\n# FEATURE SELECTION INICIAL\nprint('Base de dados de treino COM FEATURE SELECTION INICIAL')\ndisplay(df_test[train_cols].head(2))\ndisplay(df_train[train_cols].shape, df_test[train_cols].shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:35:15.810944Z","iopub.execute_input":"2024-07-26T19:35:15.811411Z","iopub.status.idle":"2024-07-26T19:36:30.756286Z","shell.execute_reply.started":"2024-07-26T19:35:15.811369Z","shell.execute_reply":"2024-07-26T19:36:30.755226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pré-processamento","metadata":{}},{"cell_type":"markdown","source":"# TREINO","metadata":{}},{"cell_type":"code","source":"# df_train=df_train[df_train['copyright_license']=='CC-BY']\n# df_train","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:36:30.758163Z","iopub.execute_input":"2024-07-26T19:36:30.760154Z","iopub.status.idle":"2024-07-26T19:36:30.764948Z","shell.execute_reply.started":"2024-07-26T19:36:30.760105Z","shell.execute_reply":"2024-07-26T19:36:30.763608Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### lgbm","metadata":{}},{"cell_type":"code","source":"   \nresult={}\n\n# Callbacks para o treinamento do modelo\ncallbacks = [\n    log_evaluation(period=100),\n    early_stopping(stopping_rounds=200, first_metric_only=True, verbose=True)\n]\n    \nfor i in range(42,43,1):\n    set_random_seed(i)\n\n    lgb_params = {\n        'objective': 'binary',\n        'metrics': 'None',\n        \"random_state\": i,\n        \"n_estimators\":1000,\n        'learning_rate':0.003,\n        'num_leaves':20,\n        'min_data_in_leaf':40,\n        'bagging_freq': 1,\n        'pos_bagging_fraction':0.75,\n        'neg_bagging_fraction':0.05,\n        'feature_fraction':0.57,\n        'lambda_l1':0.27,\n        'lambda_l2':1.0,\n        \"verbosity\": -1,\n#         \"class_weight\":'balanced',\n#         \"extra_trees\": True\n    }\n    \n    scores = []\n    models = []\n    for fold in range(FOLDS):\n        _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n        _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n        model = lgb.LGBMRegressor(\n#             **top_10_paramns[i]\n            **lgb_params\n        )\n        model.fit(\n            _df_train[train_cols], _df_train[\"target\"],\n            eval_set=[(_df_valid[train_cols], _df_valid[\"target\"]), (_df_train[train_cols], _df_train[\"target\"])], \n            eval_metric=comp_scorel,\n            callbacks=callbacks\n        )\n        preds = model.predict(_df_valid[train_cols])\n        score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n        print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n        scores.append(score)\n        models.append(copy.deepcopy(model))\n        \n    result[i] ={\n        \"scores_\":np.mean(scores),\n#         \"lgb_params\": top_10_paramns[i],\n        \"lgb_params\": lgb_params,\n        \"models\": copy.deepcopy(models),\n        \"fold1Score\":scores[0],\n        \"fold2Score\":scores[1],\n        \"fold3Score\":scores[2],\n        \"fold4Score\":scores[3],\n        \"fold5Score\":scores[4],\n        \"scores_total\":scores,\n    }\n    print(result[i])\n# Salvando os resultados\nwith open('result_data.pkl', 'wb') as file:\n    pickle.dump(result, file)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:36:30.766392Z","iopub.execute_input":"2024-07-26T19:36:30.766806Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome do arquivo onde os dados foram salvos\nfilename = 'result_data.pkl'\n\n# Carregando os dados do arquivo\nwith open(filename, 'rb') as file:\n    loaded_result = pickle.load(file)\n\n# Ordenando os resultados do melhor para o pior baseado em 'scores_'\nsorted_results = sorted(loaded_result.items(), key=lambda x: x[1]['scores_'], reverse=True)\n\n# Imprimindo os resultados ordenados\nfor i, (key, value) in enumerate(sorted_results):\n    if i < 2:\n        print(50*'*')\n        print(50*'*')\n        print(f\"Rank {i+1}:\")\n        print(f\"ID: {key}\")\n        print(f\"Score: {value['scores_']}\")\n        print(f\"Parameters: {value['lgb_params']}\")\n        print(f\"Models: {value['models']}\")\n        print(f\"fold1score: {value['fold1Score']}\")\n        print(f\"fold2score: {value['fold2Score']}\")\n        print(f\"fold3score: {value['fold3Score']}\")\n        print(50*'*')\n        print(50*'*')\n        print()\n\n\n# Lista para armazenar os três melhores modelos\ntop_3_models = []\nvalor_fold = []\nfor i in range(FOLDS):\n    ii = i+1\n    # Ordenando os resultados do melhor para o pior baseado em 'scores_'\n    sorted_results = sorted(loaded_result.items(), key=lambda x: x[1][f'fold{ii}Score'], reverse=True)\n    # Coletando os três melhores modelos\n    if ii==3:\n        limit = 1\n    elif ii==2:\n        limit = 1\n    else:\n        limit = 1\n#     limit = 1\n    for j, (key, value) in enumerate(sorted_results):\n        if j<limit:\n            valor_fold.append(value[f'fold{ii}Score']) \n            top_3_models.append(value['models'][i]) \n#             print(50*'*')\n#             print(50*'*')\n#             print(f\"Rank {i+1}:\")\n#             print(f\"ID: {key}\")\n#             print(f\"Score: {value['scores_']}\")\n#             print(f\"Parameters: {value['lgb_params']}\")\n#             print(f\"Models: {value['models']}\")\n#             print(f\"fold1score: {value['fold1Score']}\")\n#             print(f\"fold2score: {value['fold2Score']}\")\n#             print(f\"fold3score: {value['fold3Score']}\")\n#             print(50*'*')\n#             print(50*'*')\n            print()\ndisplay(len(top_3_models))\ndisplay(valor_fold)\ndisplay(np.mean(valor_fold))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"importances = np.mean([model.feature_importances_ for model in top_3_models], axis=0)\nfeature_names = df_train[train_cols].columns\ndf_imp = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How does the new feature(s) perform?\n# I would go with the > 20 in the index.\npd.set_option('display.max_rows', 500)\ndf_imp.sort_values(\"importance\", ascending=False)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_lgbm = np.mean([model.predict(df_test[train_cols]) for model in top_3_models], axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### catboost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nimport optuna\nimport catboost as cb\nimport lightgbm as lgb\nimport xgboost as xgb\n\nOPTIMIZE_OPTUNA = False\nSUBSAMPLE = False\nSUBSAMPLE_RATIO = 0.5 # only effective if SUBSAMPLE=True\nDISPLAY_FEATURE_IMPORTANCE = False\n\ndef objective(trial):\n    param = {\n        \"objective\":         trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\":             trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\":     trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\":    trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        # \"task_type\":       \"GPU\",\n        # \"used_ram_limit\":  \"3gb\",\n    }\n    \n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    scores = []\n    \n    for fold in range(N_SPLITS):\n        _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n        _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n        gbm = cb.CatBoostClassifier(**param)\n        gbm.fit(_df_train[train_cols], _df_train[\"target\"], eval_set=[(_df_valid[train_cols], _df_valid[\"target\"])], verbose=0, early_stopping_rounds=100)\n        preds = gbm.predict(_df_valid[train_cols])\n        score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n        scores.append(score)\n        \n    return np.mean(scores)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=21, timeout=500)\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n# print(\"Best trial:\")\n# trial = study.best_trial\n# print(\"  Value: {}\".format(trial.value))\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom catboost import MetricVisualizer\nfrom sklearn.metrics import roc_auc_score\n\nclass CustomAUCMetric:\n    def is_max_optimal(self):\n        \"\"\"Define que uma métrica maior é melhor.\"\"\"\n        return True\n\n    def get_final_error(self, error, weight):\n        \"\"\"Retorna o erro final usado para a métrica.\"\"\"\n        return error\n\n    def evaluate(self, approxes, target, weight):\n        \"\"\"Avalia a métrica durante o treinamento.\n        \n        Args:\n            approxes: Lista de listas das previsões do modelo.\n            target: Lista dos valores verdadeiros (rótulos).\n            weight: Pode ser usado para ponderação, mas ignorado neste exemplo.\n        \n        Returns:\n            Tuple[float, int]: Retorna a métrica calculada e o peso (assumido como 1 aqui).\n        \"\"\"\n        approxes = np.array(approxes[0])\n        target = np.array(target)\n        \n        min_tpr = 0.80  # Taxa mínima de verdadeiros positivos\n        max_fpr = 1 - min_tpr\n        \n        v_gt = abs(target - 1)\n        v_pred = 1.0 - approxes\n\n        # Cálculo do AUC parcial\n        partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n\n        # Reescala o AUC para o intervalo customizado\n        partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n        \n        return partial_auc, 1","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cb_params = {\n    'objective': 'Logloss',\n    \"random_state\": 42,\n    # \"colsample_bylevel\": 0.3, # 0.01, 0.1\n    \"iterations\": 700,\n    \"learning_rate\": 0.05,\n    \"cat_features\": cat_cols,\n    \"max_depth\": 12,\n    \"l2_leaf_reg\": 3,\n    \"task_type\": \"GPU\",\n    # \"scale_pos_weight\": 2,\n    \"verbose\": 50,\n    \"eval_metric\": CustomAUCMetric()\n}\ncb_scores = []\ncb_models = []\nfor fold in range(FOLDS):\n    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n    model = cb.CatBoostClassifier(**cb_params)\n    model.fit(_df_train[train_cols], _df_train[\"target\"],\n          eval_set=(_df_valid[train_cols], _df_valid[\"target\"]),\n          early_stopping_rounds=200,\n          plot=True)\n    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n    print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n    cb_scores.append(score)\n    cb_models.append(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cb_score = np.mean(cb_scores)\nprint(f\"CatBoost Score: {cb_score:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"importances = np.mean([model.feature_importances_ for model in cb_models], axis=0)\nfeature_names = df_train[train_cols].columns\ndf_imp = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\npd.set_option('display.max_rows', 700)\ndf_imp.sort_values(\"importance\", ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('cb_models.pkl', 'wb') as file:\n    pickle.dump(cb_models, file)\ncb_preds  = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in cb_models],  0)\n# cb_preds = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in cb_models[:2]], 0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# fazer previsoes","metadata":{}},{"cell_type":"code","source":"preds = preds_lgbm * 0.5 + cb_preds * 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sub = pd.read_csv(\"/kaggle/input/isic-2024-challenge/sample_submission.csv\")\ndf_sub[\"target\"] = preds\ndf_sub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sub","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}