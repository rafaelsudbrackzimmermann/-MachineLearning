{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":3586,"sourceType":"datasetVersion","datasetId":2134},{"sourceId":4155787,"sourceType":"datasetVersion","datasetId":2453564},{"sourceId":4256323,"sourceType":"datasetVersion","datasetId":2508107},{"sourceId":8141507,"sourceType":"datasetVersion","datasetId":4813598},{"sourceId":8166166,"sourceType":"datasetVersion","datasetId":4832208},{"sourceId":8210732,"sourceType":"datasetVersion","datasetId":4839002},{"sourceId":8304640,"sourceType":"datasetVersion","datasetId":4933292},{"sourceId":8339744,"sourceType":"datasetVersion","datasetId":4791897},{"sourceId":8387330,"sourceType":"datasetVersion","datasetId":4927912},{"sourceId":8403792,"sourceType":"datasetVersion","datasetId":5000565},{"sourceId":8616856,"sourceType":"datasetVersion","datasetId":5143121},{"sourceId":8819783,"sourceType":"datasetVersion","datasetId":5170250},{"sourceId":8819912,"sourceType":"datasetVersion","datasetId":5306037},{"sourceId":8825173,"sourceType":"datasetVersion","datasetId":4994513},{"sourceId":174250445,"sourceType":"kernelVersion"},{"sourceId":175940118,"sourceType":"kernelVersion"},{"sourceId":176861104,"sourceType":"kernelVersion"},{"sourceId":176987758,"sourceType":"kernelVersion"},{"sourceId":176989302,"sourceType":"kernelVersion"},{"sourceId":177473688,"sourceType":"kernelVersion"},{"sourceId":177475777,"sourceType":"kernelVersion"},{"sourceId":177724159,"sourceType":"kernelVersion"},{"sourceId":178258998,"sourceType":"kernelVersion"},{"sourceId":179037205,"sourceType":"kernelVersion"},{"sourceId":179720343,"sourceType":"kernelVersion"},{"sourceId":182101478,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":258.371239,"end_time":"2024-05-13T19:21:02.799575","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-13T19:16:44.428336","version":"2.3.4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fix RAPIDS Installation\n","metadata":{"papermill":{"duration":0.007393,"end_time":"2024-05-13T19:16:47.378261","exception":false,"start_time":"2024-05-13T19:16:47.370868","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd \ntest = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\nprint(\"Test shape\",test.shape)\ndisplay(test.head())\n\nif len(test) <10:\n    prunt()\n\nDEBUG = False\nv_debug = 100","metadata":{"execution":{"iopub.status.busy":"2024-07-02T02:59:00.210398Z","iopub.execute_input":"2024-07-02T02:59:00.210765Z","iopub.status.idle":"2024-07-02T02:59:01.715275Z","shell.execute_reply.started":"2024-07-02T02:59:00.210735Z","shell.execute_reply":"2024-07-02T02:59:01.713743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n!pip install --find-links /kaggle/input/downgrade-pandas /kaggle/input/downgrade-pandas/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# BIBLIOTECAS INSTALAÇÂO\n!pip install /kaggle/input/pre-processamento-treino/en_core_web_md-3.7.1-py3-none-any.whl\n!pip install /kaggle/input/lexical-diversity/lexical_diversity-0.1.1-py3-none-any.whl\n!pip install /kaggle/input/lexical-diversity/syllapy-0.7.2-py3-none-any.whl\n\nimport sys\nsys.path.append('/kaggle/input/aes-fucoes')\nfrom funcoes import *","metadata":{"_kg_hide-output":true,"papermill":{"duration":44.909529,"end_time":"2024-05-13T19:17:32.295355","exception":false,"start_time":"2024-05-13T19:16:47.385826","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:29:08.341106Z","iopub.execute_input":"2024-07-01T19:29:08.341399Z","iopub.status.idle":"2024-07-01T19:32:29.567849Z","shell.execute_reply.started":"2024-07-01T19:29:08.341368Z","shell.execute_reply":"2024-07-01T19:32:29.566779Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Libraries and Data","metadata":{"papermill":{"duration":0.012248,"end_time":"2024-05-13T19:17:32.316881","exception":false,"start_time":"2024-05-13T19:17:32.304633","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nimport numpy as np, gc, re \nimport pandas as pd \n\ntrain = pd.read_csv(\"/kaggle/input/pre-processamento-treino/train_prompt_fold.csv\")\nif DEBUG:\n    train = train[:v_debug]\nprint(\"Train shape\",train.shape)\ndisplay(train.head())\nprint()\n\ntest = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\nprint(\"Test shape\",test.shape)\ndisplay(test.head())\n","metadata":{"papermill":{"duration":1.267719,"end_time":"2024-05-13T19:17:33.592756","exception":false,"start_time":"2024-05-13T19:17:32.325037","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:29.569981Z","iopub.execute_input":"2024-07-01T19:32:29.570506Z","iopub.status.idle":"2024-07-01T19:32:30.348174Z","shell.execute_reply.started":"2024-07-01T19:32:29.570477Z","shell.execute_reply":"2024-07-01T19:32:30.347023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stratified 15 K Fold","metadata":{"papermill":{"duration":0.008383,"end_time":"2024-05-13T19:17:33.610024","exception":false,"start_time":"2024-05-13T19:17:33.601641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# from sklearn.model_selection import StratifiedKFold\n\n# FOLDS = 15\n# train[\"fold\"] = -1\n# skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n# for fold,(train_index, val_index) in enumerate(skf.split(train,train[\"score\"])):\n#     train.loc[val_index,\"fold\"] = fold\n# print('Train samples per fold:')\n# train.fold.value_counts().sort_index()","metadata":{"papermill":{"duration":1.218633,"end_time":"2024-05-13T19:17:34.837061","exception":false,"start_time":"2024-05-13T19:17:33.618428","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:30.353556Z","iopub.execute_input":"2024-07-01T19:32:30.353874Z","iopub.status.idle":"2024-07-01T19:32:30.358848Z","shell.execute_reply.started":"2024-07-01T19:32:30.353847Z","shell.execute_reply":"2024-07-01T19:32:30.357686Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# promtp = pd.read_csv('/kaggle/input/funcoes-featurestext/predicted_prompt.csv')\n# # Add prompt_name to the train DataFrame by merging on essay_id\n# train = pd.merge(train, promtp[['essay_id', 'prompt_name']], on='essay_id', how='left')\n# train\n\n# from sklearn.model_selection import StratifiedGroupKFold\n\n# # Number of folds\n# FOLDS = 3\n\n# # Initialize the 'fold' column with -1\n# train[\"fold\"] = -1\n\n# # Initialize the StratifiedGroupKFold object\n# sgkf = StratifiedGroupKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n# # Assuming 'group' is the column used for grouping\n# for fold, (train_index, val_index) in enumerate(sgkf.split(train, train[\"score\"], groups=train[\"prompt_name\"])):\n#     train.loc[val_index, \"fold\"] = fold\n\n# # Display the number of samples per fold\n# print('Train samples per fold:')\n# print(train.fold.value_counts().sort_index())","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:32:30.360164Z","iopub.execute_input":"2024-07-01T19:32:30.360551Z","iopub.status.idle":"2024-07-01T19:32:30.372468Z","shell.execute_reply.started":"2024-07-01T19:32:30.360507Z","shell.execute_reply":"2024-07-01T19:32:30.371634Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Embeddings","metadata":{"papermill":{"duration":0.008867,"end_time":"2024-05-13T19:17:34.854859","exception":false,"start_time":"2024-05-13T19:17:34.845992","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from transformers import AutoModel,AutoTokenizer\nimport torch, torch.nn.functional as F\nfrom tqdm import tqdm","metadata":{"papermill":{"duration":6.722973,"end_time":"2024-05-13T19:17:41.586735","exception":false,"start_time":"2024-05-13T19:17:34.863762","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:30.373429Z","iopub.execute_input":"2024-07-01T19:32:30.373675Z","iopub.status.idle":"2024-07-01T19:32:32.031684Z","shell.execute_reply.started":"2024-07-01T19:32:30.373654Z","shell.execute_reply":"2024-07-01T19:32:32.030906Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mean_pooling_ultima(model_output, attention_mask):\n    # ULTIMA SAIDA\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\ndef mean_pooling_media_todas(model_output, attention_mask):\n    # MEDIA TODAS CAMADAS\n    token_embeddings = model_output.hidden_states[:]\n    \n    # Calcula a média das três primeiras camadas\n    token_embeddings = torch.stack(token_embeddings).mean(dim=0)\n    \n    # Detach e move para a CPU\n    token_embeddings = token_embeddings.detach().cpu()\n    \n    # Expandir a máscara de atenção para a dimensão do token_embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    \n    # Aplicar a máscara de atenção aos embeddings\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    \n    # Calcular a média levando em conta a máscara de atenção\n    return sum_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# def mean_pooling(model_output, attention_mask):\n#      # CONCATENA AS 3 PRIMEIRAS\n#     token_embeddings = torch.cat(model_output.hidden_states[:3], dim=-1).detach().cpu()\n    \n#     # Expandir a máscara de atenção para a dimensão do token_embeddings\n#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    \n#     # Aplicar a máscara de atenção aos embeddings\n#     sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    \n#     # Calcular a média levando em conta a máscara de atenção\n#     return sum_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\ndef mean_pooling_3_primeiras_media(model_output, attention_mask):\n    # Extrai as três primeiras camadas\n    token_embeddings = model_output.hidden_states[:3]\n    \n    # Calcula a média das três primeiras camadas\n    token_embeddings = torch.stack(token_embeddings).mean(dim=0)\n    \n    # Detach e move para a CPU\n    token_embeddings = token_embeddings.detach().cpu()\n    \n    # Expandir a máscara de atenção para a dimensão do token_embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    \n    # Aplicar a máscara de atenção aos embeddings\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    \n    # Calcular a média levando em conta a máscara de atenção\n    return sum_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# def mean_pooling(model_output, attention_mask):\n#     # Extrai as primeiras 8 camadas e as últimas 4 camadas\n#     first_8_layers = model_output.hidden_states[:8]\n#     last_4_layers = model_output.hidden_states[-4:]\n    \n#     # Calcula a média das 8 primeiras camadas\n#     first_8_mean = torch.stack(first_8_layers).mean(dim=0)\n    \n#     # Calcula a média das 4 últimas camadas\n#     last_4_mean = torch.stack(last_4_layers).mean(dim=0)\n    \n#     # Detach e move para a CPU\n#     first_8_mean = first_8_mean.detach().cpu()\n#     last_4_mean = last_4_mean.detach().cpu()\n    \n#     # Expandir a máscara de atenção para a dimensão dos embeddings\n#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(first_8_mean.size()).float()\n    \n#     # Aplicar a máscara de atenção aos embeddings das 8 primeiras camadas\n#     sum_first_8_embeddings = torch.sum(first_8_mean * input_mask_expanded, 1)\n    \n#     # Aplicar a máscara de atenção aos embeddings das 4 últimas camadas\n#     sum_last_4_embeddings = torch.sum(last_4_mean * input_mask_expanded, 1)\n    \n#     # Calcular a média levando em conta a máscara de atenção\n#     first_8_mean_pooled = sum_first_8_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n#     last_4_mean_pooled = sum_last_4_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n#     # Concatenar as médias das 8 primeiras camadas e das 4 últimas camadas\n#     concatenated_mean = torch.cat((first_8_mean_pooled, last_4_mean_pooled), dim=-1)\n    \n#     return concatenated_mean\n\n# def mean_pooling(model_output, attention_mask):\n#     # Extrai as primeiras 8 camadas e as últimas 4 camadas\n#     first_8_layers = model_output.hidden_states[:4]\n#     last_4_layers = model_output.hidden_states[-8:-4]\n    \n#     # Calcula a média das 8 primeiras camadas\n#     first_8_mean = torch.stack(first_8_layers).mean(dim=0)\n    \n#     # Calcula a média das 4 últimas camadas\n#     last_4_mean = torch.stack(last_4_layers).mean(dim=0)\n    \n#     # Detach e move para a CPU\n#     first_8_mean = first_8_mean.detach().cpu()\n#     last_4_mean = last_4_mean.detach().cpu()\n    \n#     # Expandir a máscara de atenção para a dimensão dos embeddings\n#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(first_8_mean.size()).float()\n    \n#     # Aplicar a máscara de atenção aos embeddings das 8 primeiras camadas\n#     sum_first_8_embeddings = torch.sum(first_8_mean * input_mask_expanded, 1)\n    \n#     # Aplicar a máscara de atenção aos embeddings das 4 últimas camadas\n#     sum_last_4_embeddings = torch.sum(last_4_mean * input_mask_expanded, 1)\n    \n#     # Calcular a média levando em conta a máscara de atenção\n#     first_8_mean_pooled = sum_first_8_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n#     last_4_mean_pooled = sum_last_4_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n#     # Concatenar as médias das 8 primeiras camadas e das 4 últimas camadas\n#     concatenated_mean = torch.cat((first_8_mean_pooled, last_4_mean_pooled), dim=-1)\n    \n#     return concatenated_mean\n\ndef mean_pooling_4_meio(model_output, attention_mask):\n    # SOMENTE AS 4 DO MEIO\n    last_4_layers = model_output.hidden_states[-8:-4]\n\n    # Calcula a média das 4 últimas camadas\n    last_4_mean = torch.stack(last_4_layers).mean(dim=0)\n    last_4_mean = last_4_mean.detach().cpu()\n\n    # Expandir a máscara de atenção para a dimensão dos embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_4_mean.size()).float()\n\n    # Aplicar a máscara de atenção aos embeddings das 4 últimas camadas\n    sum_last_4_embeddings = torch.sum(last_4_mean * input_mask_expanded, 1)\n\n    # Calcular a média levando em conta a máscara de atenção\n    last_4_mean_pooled = sum_last_4_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    return last_4_mean_pooled","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:32:32.033021Z","iopub.execute_input":"2024-07-01T19:32:32.033631Z","iopub.status.idle":"2024-07-01T19:32:32.050223Z","shell.execute_reply.started":"2024-07-01T19:32:32.033591Z","shell.execute_reply":"2024-07-01T19:32:32.049326Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self,df,tokenizer,max_length):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max = max_length\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        text = self.df.loc[idx,\"full_text\"]\n        tokens = self.tokenizer(\n                text,\n                None,\n                add_special_tokens=True,\n                padding='max_length',\n                truncation=True,\n                max_length=self.max,\n                return_tensors=\"pt\")\n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens","metadata":{"papermill":{"duration":0.017943,"end_time":"2024-05-13T19:17:41.6398","exception":false,"start_time":"2024-05-13T19:17:41.621857","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:32.051532Z","iopub.execute_input":"2024-07-01T19:32:32.051797Z","iopub.status.idle":"2024-07-01T19:32:32.066083Z","shell.execute_reply.started":"2024-07-01T19:32:32.051775Z","shell.execute_reply":"2024-07-01T19:32:32.065198Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract Embeddings","metadata":{"papermill":{"duration":0.008495,"end_time":"2024-05-13T19:17:41.656901","exception":false,"start_time":"2024-05-13T19:17:41.648406","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_embeddings(disk_name='', max_length=1024, batch_size=32, mean_pooling=None, compute_train=True, compute_test=True, tokenizerpath=None):\n\n    global train, test\n\n    DEVICE = \"cuda:1\" # EXTRACT EMBEDDINGS WITH GPU #2\n#     path = \"/kaggle/input/download-huggingface-models/\"\n#     path = \"/kaggle/input/modeloslearningfinal/deberta-v3-large-2-stagios-asap-Fold0/\"\n#     disk_name = path + model_name\n#     disk_name = path + model_name.replace(\"/\",\"_\")\n#     model = AutoModel.from_pretrained( disk_name , trust_remote_code=True)\n    model = AutoModel.from_pretrained(disk_name, output_hidden_states=True, trust_remote_code=True)\n    if tokenizerpath is None:\n        tokenizer = AutoTokenizer.from_pretrained( disk_name , trust_remote_code=True)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained( '/kaggle/input/download-huggingface-models/microsoft_deberta-v3-large/' , trust_remote_code=True)\n\n    ds_tr = EmbedDataset(train, tokenizer, max_length)\n    embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\n                            batch_size=batch_size,\n                            shuffle=False)\n    ds_te = EmbedDataset(test, tokenizer, max_length)\n    embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\n                            batch_size=batch_size,\n                            shuffle=False)\n    \n    model = model.to(DEVICE)\n    model.eval()\n\n    # COMPUTE TRAIN EMBEDDINGS\n    all_train_text_feats = []\n    if compute_train:\n        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            with torch.no_grad():\n                with torch.cuda.amp.autocast(enabled=True):\n                    model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n            # Normalize the embeddings\n            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n            all_train_text_feats.extend(sentence_embeddings)\n    all_train_text_feats = np.array(all_train_text_feats)\n\n    # COMPUTE TEST EMBEDDINGS\n    all_test_text_feats = []\n    if compute_test:\n        for batch in embed_dataloader_te:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            with torch.no_grad():\n                with torch.cuda.amp.autocast(enabled=True):\n                    model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n            # Normalize the embeddings\n            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n            all_test_text_feats.extend(sentence_embeddings)\n        all_test_text_feats = np.array(all_test_text_feats)\n    all_test_text_feats = np.array(all_test_text_feats)\n\n    # CLEAR MEMORY\n    del ds_tr, ds_te\n    del embed_dataloader_tr, embed_dataloader_te\n    del model, tokenizer\n    del model_output, sentence_embeddings, input_ids, attention_mask\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # RETURN EMBEDDINGS\n    return all_train_text_feats, all_test_text_feats","metadata":{"papermill":{"duration":0.026311,"end_time":"2024-05-13T19:17:41.691963","exception":false,"start_time":"2024-05-13T19:17:41.665652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:32.067678Z","iopub.execute_input":"2024-07-01T19:32:32.068041Z","iopub.status.idle":"2024-07-01T19:32:32.085246Z","shell.execute_reply.started":"2024-07-01T19:32:32.068009Z","shell.execute_reply":"2024-07-01T19:32:32.084388Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EMBEDDINGS TO LOAD/COMPUTE\n# PARAMETERS = (MODEL_NAME, MAX_LENGTH, BATCH_SIZE)\n# CHOOSE LARGEST BATCH SIZE WITHOUT MEMORY ERROR\n\nmodels = [\n#     ('microsoft/deberta-base', 1024, 32),\n#     ('microsoft/deberta-large', 1024, 8),\n#     ('microsoft/deberta-v3-large', 1024, 8),\n#     ('allenai/longformer-base-4096', 1024, 32),\n#     ('google/bigbird-roberta-base', 1024, 32),\n#     ('google/bigbird-roberta-large', 1024, 8),\n    \n#     ('/kaggle/input/modeloslearningfinal/deberta-v3-large-2-stagios-asap-Fold0/checkpoint-2400/', 1024, 8, mean_pooling_4_meio, 'asap_4_meio'),\n#     ('/kaggle/input/modeloslearningfinal/deberta-v3-large-2-stagios-asap-Fold0/checkpoint-2400/', 1024, 8, mean_pooling_3_primeiras_media, 'asap_3_primeiras_media', 1),\n    ('/kaggle/input/download-huggingface-models/microsoft_deberta-v3-large/', 1024, 8, mean_pooling_media_todas, 'deberta_media_todas', None),\n#     ('/kaggle/input/download-huggingface-models/microsoft_deberta-v3-large/', 1024, 8, mean_pooling_ultima, 'deberta_ultima', None),\n    ('/kaggle/input/download-huggingface-models/google_bigbird-roberta-large/', 1024, 8, mean_pooling_4_meio, 'bigbird_roberta_4_meio', None),\n#     ('/kaggle/input/download-huggingface-models/allenai_longformer-base-4096/', 1024, 32, mean_pooling_ultima, 'allenai_longformer_4_ultima', None),\n]","metadata":{"papermill":{"duration":0.016991,"end_time":"2024-05-13T19:17:41.717967","exception":false,"start_time":"2024-05-13T19:17:41.700976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:32.090547Z","iopub.execute_input":"2024-07-01T19:32:32.090960Z","iopub.status.idle":"2024-07-01T19:32:32.098651Z","shell.execute_reply.started":"2024-07-01T19:32:32.090930Z","shell.execute_reply":"2024-07-01T19:32:32.097553Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\npath = \"/kaggle/input/funcoes-featurestext/\"\nall_train_embeds = []\nall_test_embeds = []\n\nfor (model, max_length, batch_size, mean_func_, name_, tk_) in models:\n    name = path +name_+ '.npy'\n    if os.path.exists(name):\n        _, test_embed = get_embeddings(disk_name=model, max_length=max_length, batch_size=batch_size, mean_pooling=mean_func_, compute_train=False, tokenizerpath=tk_)\n        train_embed = np.load(name)\n        print(f\"Loading train embeddings for {name} {train_embed.shape}\")\n        if DEBUG:\n            train_embed = train_embed[:v_debug]\n            print(f\"DEBUG Loading train embeddings for {name} {train_embed.shape}\")\n    else:\n        print(f\"Computing train embeddings for {name}\")\n        train_embed, test_embed = get_embeddings(disk_name=model, max_length=max_length, batch_size=batch_size, mean_pooling=mean_func_, compute_train=True, tokenizerpath=tk_)\n        np.save(name_+ '.npy', train_embed)\n    all_train_embeds.append(train_embed)\n    all_test_embeds.append(test_embed)\n\ndel train_embed, test_embed\n\n","metadata":{"papermill":{"duration":51.388457,"end_time":"2024-05-13T19:18:33.115494","exception":false,"start_time":"2024-05-13T19:17:41.727037","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:32:32.099827Z","iopub.execute_input":"2024-07-01T19:32:32.100193Z","iopub.status.idle":"2024-07-01T19:33:01.528899Z","shell.execute_reply.started":"2024-07-01T19:32:32.100162Z","shell.execute_reply":"2024-07-01T19:33:01.527858Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deberta inf","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nfrom glob import glob\nimport gc\nimport torch\nfrom scipy.special import softmax\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import Whitespace\n\n\ndef executa_modelo_transform(df_, path_model, max_lenght, eval_batch, path_tokenizer=None):\n    \n    models = glob(path_model)\n    print(models)\n    \n    if path_tokenizer is None:\n        path_tokenizer = models[0]\n        \n    tokenizer = AutoTokenizer.from_pretrained(path_tokenizer)\n\n    def tokenize(sample):\n        return tokenizer(sample['full_text'], max_length=max_lenght, truncation=True)\n\n    \n    ds = Dataset.from_pandas(df_).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\n    args = TrainingArguments(\n        \".\", \n        per_device_eval_batch_size=eval_batch, \n        report_to=\"none\"\n    )\n\n\n    predictions = []\n\n    for model in models:\n        model = AutoModelForSequenceClassification.from_pretrained(model)\n        trainer = Trainer(\n            model=model, \n            args=args, \n            data_collator=DataCollatorWithPadding(tokenizer), \n            tokenizer=tokenizer\n        )\n\n        preds = trainer.predict(ds).predictions\n        predictions.append(softmax(preds, axis=-1))\n#         predictions.append(preds)\n        del model, trainer\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    predicted_score = 0.\n\n    for p in predictions:\n        predicted_score += p\n\n    predicted_score /= len(predictions)\n    \n    return predicted_score\n\ndef executa_modelo_transform2(df_, path_model, max_lenght, eval_batch, path_tokenizer=None):\n    \n    models = [os.path.join(path_model, d) for d in os.listdir(path_model) if os.path.isdir(os.path.join(path_model, d))]\n    print(models)\n    \n    if path_tokenizer is None:\n        path_tokenizer = models[0]\n        \n    tokenizer = AutoTokenizer.from_pretrained(path_tokenizer)\n\n    def tokenize(sample):\n        return tokenizer(sample['full_text'], max_length=max_lenght, truncation=True)\n\n    \n    ds = Dataset.from_pandas(df_).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\n    args = TrainingArguments(\n        \".\", \n        per_device_eval_batch_size=eval_batch, \n        report_to=\"none\"\n    )\n\n\n    predictions = []\n    predictionsarg = []\n\n    for model in models:\n        model = AutoModelForSequenceClassification.from_pretrained(model)\n        trainer = Trainer(\n            model=model, \n            args=args, \n            data_collator=DataCollatorWithPadding(tokenizer), \n            tokenizer=tokenizer\n        )\n\n        preds = trainer.predict(ds)\n        predictionsarg.append(preds.predictions.argmax(-1))\n        predictions.append(preds.predictions)\n        del model, trainer\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    predicted_score = 0.\n\n    for p in predictionsarg:\n        predicted_score += p\n\n    predicted_score /= len(predictionsarg)\n    \n    predictions = np.array(predictions)\n    logits = predictions.mean(axis=0)\n    \n    return predicted_score, logits\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:33:01.530403Z","iopub.execute_input":"2024-07-01T19:33:01.530954Z","iopub.status.idle":"2024-07-01T19:33:11.694077Z","shell.execute_reply.started":"2024-07-01T19:33:01.530926Z","shell.execute_reply":"2024-07-01T19:33:11.693037Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_array(logits_str):\n    try:\n        # Remove brackets and split the string into individual numbers\n        logits_list = logits_str.strip('[]').split()\n        # Convert the list of strings to a NumPy array of floats\n        return np.array(logits_list, dtype=float)\n    except (ValueError, SyntaxError):\n        return np.array([])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:33:11.695489Z","iopub.execute_input":"2024-07-01T19:33:11.696134Z","iopub.status.idle":"2024-07-01T19:33:11.702512Z","shell.execute_reply.started":"2024-07-01T19:33:11.696107Z","shell.execute_reply":"2024-07-01T19:33:11.701528Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predicted_train = executa_modelo_transform(train, \n#                                             '/kaggle/input/modeloslearningfinal/deberta-v3-large-2-stagios-asap-Fold0/checkpoint-2400/', \n#                                             1024, \n#                                             1,\n#                                             path_tokenizer = '/kaggle/input/deberta-large-fold0/deberta-large-fold0 (1)/deberta-large-fold0/checkpoint-1200',\n#                                            )\n# np.save('predicted_train_deb_asap.npy', predicted_train)\npredicted_train = pd.read_csv('/kaggle/input/funcoes-featurestext/deberta-v3-large-groupfold/oof_predictions.csv')\nsorted_predicted_train = pd.merge(train[['essay_id']], predicted_train, on='essay_id', how='left')\npredicted_test = executa_modelo_transform2(test, \n                                            '/kaggle/input/funcoes-featurestext/deberta-v3-large-groupfold/', \n                                            1024, \n                                            1,\n                                            path_tokenizer = '/kaggle/input/deberta-large-fold0/deberta-large-fold0 (1)/deberta-large-fold0/checkpoint-1200',\n                                           )\n\n# if DEBUG:\n#     predicted_train = predicted_train[:v_debug]\n\n# display(predicted_train)\n\npredicted_train = sorted_predicted_train['prediction']+1\npredicted_test = predicted_test[0] +1\n\ndisplay(predicted_train)\ndisplay(predicted_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:33:11.703839Z","iopub.execute_input":"2024-07-01T19:33:11.704222Z","iopub.status.idle":"2024-07-01T19:33:56.407639Z","shell.execute_reply.started":"2024-07-01T19:33:11.704187Z","shell.execute_reply":"2024-07-01T19:33:56.406672Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_train.shape, predicted_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:33:56.409172Z","iopub.execute_input":"2024-07-01T19:33:56.409632Z","iopub.status.idle":"2024-07-01T19:33:56.416238Z","shell.execute_reply.started":"2024-07-01T19:33:56.409601Z","shell.execute_reply":"2024-07-01T19:33:56.415321Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predicted_train_2d = predicted_train[:, np.newaxis]\n# predicted_test_2d = predicted_test[:, np.newaxis]\n\n# display(predicted_train_2d)\n# predicted_train_2d.shape, predicted_test_2d.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:33:56.417329Z","iopub.execute_input":"2024-07-01T19:33:56.417641Z","iopub.status.idle":"2024-07-01T19:33:56.426937Z","shell.execute_reply.started":"2024-07-01T19:33:56.417616Z","shell.execute_reply":"2024-07-01T19:33:56.425905Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combine Feature Embeddings","metadata":{"papermill":{"duration":0.008934,"end_time":"2024-05-13T19:18:33.13384","exception":false,"start_time":"2024-05-13T19:18:33.124906","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n# CRIAÇÂO DAS FEATURES --------------------------------\nCRIA_FEATURES_TRAIN = False\nCRIA_ORDER_DICT = False\nif CRIA_FEATURES_TRAIN:\n    if CRIA_ORDER_DICT:\n        docs_dict = doc_dict(train)\n        save_docs_dict(docs_dict, \"documentos_dict.spacy\")\n        print(len(docs_dict))\n    else:\n        # Carregar o dicionário de documentos\n        %time\n        docs_dict = load_docs_dict(\"/kaggle/input/pre-processamento-treino/documentos_dict.spacy\",\n                                   nlp)\n\n    ordered_docs = [docs_dict[str(essay_id)] for essay_id in train['essay_id']]\n    ordered_docs = pd.Series(ordered_docs)\n\n    docs_features = []\n    for doc in tqdm(ordered_docs[:], desc=\"Processando documentos\"):\n        docs_features.append(process_document(doc))\n\n    # Convertendo a lista de features em um DataFrame\n    df = pd.DataFrame(docs_features)\n#     pd.set_option('display.max_columns', None)\n    display(df)\n    display(df.shape)\n    df.to_csv(\"new_features.csv\",index=None)\nelse:\n    \n    df = pd.read_csv('/kaggle/input/pre-processamento-treino/train_features.csv')\n#     pd.set_option('display.max_columns', None)\n    if DEBUG:\n        df = df[:v_debug]\n    display(df)\n    display(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:33:56.428411Z","iopub.execute_input":"2024-07-01T19:33:56.428703Z","iopub.status.idle":"2024-07-01T19:34:05.948484Z","shell.execute_reply.started":"2024-07-01T19:33:56.428679Z","shell.execute_reply":"2024-07-01T19:34:05.947542Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CRIAÇÂO FEATURES TESTE --------------------------------\ndocs_dicttest = doc_dict(test)\nordered_docstest = [docs_dicttest[str(essay_id)] for essay_id in test['essay_id']]\nordered_docstest = pd.Series(ordered_docstest)\nordered_docstest\ndocs_featurestest = []\nfor doc in tqdm(ordered_docstest, desc=\"Processando documentos\"):\n    docs_featurestest.append(process_document(doc))\n    \n# Convertendo a lista de features em um DataFrame\ndftest = pd.DataFrame(docs_featurestest)\ndftest = dftest\ndftest","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:34:05.949851Z","iopub.execute_input":"2024-07-01T19:34:05.950142Z","iopub.status.idle":"2024-07-01T19:34:08.094790Z","shell.execute_reply.started":"2024-07-01T19:34:05.950117Z","shell.execute_reply":"2024-07-01T19:34:08.093893Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_overfitting_features(df_):\n    print(df_.shape)\n    remove_overfiting_features = ['inf_word_taassc',\n#                                   \"overlap_coehsion_\", \"syntactic_density\", \"sim_total\", \"tamanho_texto\", \"fea_letras_\", \"lex_rep_\", \n#                                   \"clause_score_\", \"Rare_Percentage_\", \"long_word_\", \"spelling_errors_\", \"Readability_\", '_per_sentence'\n                                 ]\n\n#     filtered_columns = [col for col in df_.columns if not any(col.startswith(prefix) for prefix in remove_overfiting_features)]\n    filtered_columns = [col for col in df_.columns if not any(prefix in col for prefix in remove_overfiting_features)]\n    return df[filtered_columns]\ndf = filter_overfitting_features(df)\n# pd.set_option('display.max_columns', None)\nprint(df.shape)\nfeature_names = df.columns.to_list()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:34:08.096397Z","iopub.execute_input":"2024-07-01T19:34:08.096783Z","iopub.status.idle":"2024-07-01T19:34:08.167695Z","shell.execute_reply.started":"2024-07-01T19:34:08.096748Z","shell.execute_reply":"2024-07-01T19:34:08.166553Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"fill na","metadata":{}},{"cell_type":"code","source":"# all_train_filled = df[feature_names].fillna(0).to_numpy()\n# all_test_filled = dftest[feature_names].fillna(0).to_numpy()\n\nmean_values = df[feature_names].median()\nall_train_filled = df[feature_names].fillna(mean_values).to_numpy()\nall_test_filled = dftest[feature_names].fillna(mean_values).to_numpy()\n\nall_train_ = df[feature_names].to_numpy()\nall_test_ = dftest[feature_names].to_numpy()\n\nall_train_.shape, all_test_.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:34:08.168949Z","iopub.execute_input":"2024-07-01T19:34:08.169289Z","iopub.status.idle":"2024-07-01T19:35:32.417512Z","shell.execute_reply.started":"2024-07-01T19:34:08.169240Z","shell.execute_reply":"2024-07-01T19:35:32.416639Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# try:\n#     all_train_embeds = np.array(all_train_embeds).squeeze()\n#     all_test_embeds = np.array(all_test_embeds).squeeze()\n# except:\n#     print(\"concatena\")\nall_train_embeds = np.concatenate(all_train_embeds, axis=1)\nall_test_embeds = np.concatenate(all_test_embeds, axis=1)\n\nif DEBUG:\n    all_train_embeds = all_train_embeds[:v_debug]\n\nall_train_embeds.shape, all_test_embeds.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:32.419127Z","iopub.execute_input":"2024-07-01T19:35:32.419443Z","iopub.status.idle":"2024-07-01T19:35:32.457576Z","shell.execute_reply.started":"2024-07-01T19:35:32.419418Z","shell.execute_reply":"2024-07-01T19:35:32.456677Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenação ao longo do eixo 1 (colunas)\nall_train_embeds_filled = np.concatenate([all_train_embeds, all_train_filled\n                                  ], axis=1)\nall_test_embeds_filled = np.concatenate([all_test_embeds, all_test_filled, \n                                 ], axis=1)\nall_train_embeds = np.concatenate([all_train_embeds, all_train_\n                                  ], axis=1)\nall_test_embeds = np.concatenate([all_test_embeds, all_test_, \n                                 ], axis=1)\n\nnp.save('all_train_embeds_filled.npy', all_train_embeds_filled)\nnp.save('all_train_embeds.npy', all_train_embeds)\n\ngc.collect()\nprint('Our concatenated train embeddings have shape', all_train_embeds.shape )\ndisplay(all_train_embeds[:10])","metadata":{"papermill":{"duration":0.257046,"end_time":"2024-05-13T19:18:33.40012","exception":false,"start_time":"2024-05-13T19:18:33.143074","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:35:32.458842Z","iopub.execute_input":"2024-07-01T19:35:32.459137Z","iopub.status.idle":"2024-07-01T19:35:34.761292Z","shell.execute_reply.started":"2024-07-01T19:35:32.459112Z","shell.execute_reply":"2024-07-01T19:35:34.760223Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"normalização","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Criar o objeto StandardScaler\nscaler = StandardScaler()\n\n# Fit nos dados de treino e transformá-lo\nall_train_embeds_scaled = scaler.fit_transform(all_train_embeds_filled)\n\n# Aplicar a mesma transformação nos dados de teste\nall_test_embeds_scaled = scaler.transform(all_test_embeds_filled)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:34.762910Z","iopub.execute_input":"2024-07-01T19:35:34.763649Z","iopub.status.idle":"2024-07-01T19:35:35.708335Z","shell.execute_reply.started":"2024-07-01T19:35:34.763610Z","shell.execute_reply":"2024-07-01T19:35:35.707500Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train RAPIDS cuML SVR\nDocumentation for RAPIDS SVR is [here][1]. Using RAPIDS support vector regression (SVR) is a great model when we have 1000s of features because it is quick and it naturally will perform feature selection and remove features to prevent overfitting.\n\nNote that SVR model likes the inputs to be standardized so that each feature is rougly mean 0 and std 1. We approximate this by performing L2 norm on the extracted embeddings.\n\n[1]: https://docs.rapids.ai/api/cuml/stable/api.html#support-vector-machines","metadata":{"papermill":{"duration":0.010034,"end_time":"2024-05-13T19:18:33.419647","exception":false,"start_time":"2024-05-13T19:18:33.409613","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from cuml.svm import SVR\nimport cuml\nprint('RAPIDS version',cuml.__version__)","metadata":{"papermill":{"duration":5.860218,"end_time":"2024-05-13T19:18:39.289139","exception":false,"start_time":"2024-05-13T19:18:33.428921","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:35:35.709513Z","iopub.execute_input":"2024-07-01T19:35:35.709819Z","iopub.status.idle":"2024-07-01T19:35:39.638765Z","shell.execute_reply.started":"2024-07-01T19:35:35.709793Z","shell.execute_reply":"2024-07-01T19:35:39.637674Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import log_evaluation, early_stopping\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nimport matplotlib.pyplot as plt\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = (y_true + a).clip(1, 6).round()\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 0\nb = 0\n\n# Função callback para ajustar o learning rate de acordo com intervalos específicos\ndef adjust_learning_rate(current_round):\n    if current_round < 500:\n        return 0.1\n    elif current_round < 1000:\n        return 0.05\n    elif current_round < 1500:\n        return 0.025\n    else:\n        return 0.01\n\n# Callbacks para o treinamento do modelo\ncallbacks = [\n    log_evaluation(period=50),\n    early_stopping(stopping_rounds=200, first_metric_only=True, verbose=-1),\n#     lgb.reset_parameter(learning_rate=lambda iter: adjust_learning_rate(iter)),\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:39.640416Z","iopub.execute_input":"2024-07-01T19:35:39.640843Z","iopub.status.idle":"2024-07-01T19:35:39.984467Z","shell.execute_reply.started":"2024-07-01T19:35:39.640808Z","shell.execute_reply":"2024-07-01T19:35:39.983688Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def selecionar_features_importantes(lista_modelos, dados_treino, maior_que_x):\n    # Se a lista de modelos estiver vazia, considerar todas as features como importantes\n    if not lista_modelos and maior_que_x==0:\n        print(\"Lista de modelos vazia, retornando todas as features.\")\n        return np.ones(dados_treino.shape[1], dtype=bool)\n    elif not lista_modelos and maior_que_x>0:\n        print(\"Lendo modelo LGBM salvo\")\n        data = pd.read_pickle('/kaggle/input/pre-processamento-treino/RESULT_TRAIN.pkl')\n        lista_modelos = data[0]['models']\n    \n    # Inicializar um array para armazenar a importância das features para cada modelo\n    importancias_features = []\n    \n    # Calcular a importância das features para cada modelo\n    for modelo in lista_modelos:\n        importancia_atual = modelo.feature_importances_\n        print(\"Importância das features para um modelo:\", importancia_atual[:50])\n        importancias_features.append(importancia_atual)\n    \n    # Converter a lista de importâncias das features em um array numpy\n    importancias_features = np.array(importancias_features)\n    print(\"Array de importâncias das features:\", importancias_features[:50])\n\n    # Calcular a média das importâncias das features\n    media_importancias = np.mean(importancias_features, axis=0)\n    print(\"Média das importâncias das features:\", media_importancias[:50])\n    \n    # Verificar quais features têm média de importância maior que um limiar, por exemplo 0.01\n    features_selecionadas = media_importancias > maior_que_x\n    print(\"Features selecionadas (índice booleano):\", features_selecionadas[:50])\n    \n    # Selecionar as features do conjunto de dados de treinamento\n    dados_treino_selecionados = dados_treino[:, features_selecionadas]\n    print(\"Dados de treino selecionados com base nas features importantes:\", dados_treino_selecionados.shape)\n    \n    return features_selecionadas\n\nfrom sklearn.feature_selection import RFE\n\ndef selecionar_features_importantes_linear(lista_modelos, dados_treino, dados_alvo, n_features_to_select=2000):\n    # Suponha que todos os modelos em lista_modelos são SVR e vamos usar LinearSVR para RFE\n    importancias_features = []\n    \n    # Usar RFE para encontrar as features mais importantes\n    for modelo in lista_modelos:\n        # Usar LinearSVR como um aproximador para o SVR original\n        aproximador = SVR(c=10)\n        selector = RFE(aproximador, n_features_to_select=n_features_to_select, step=1)\n        selector = selector.fit(dados_treino, dados_alvo)\n        importancias_features.append(selector.support_)\n        print(\"Features selecionadas para um modelo:\", selector.support_)\n    \n    # Média das seleções de features (booleana, portanto usa moda)\n    features_selecionadas = np.mean(importancias_features, axis=0) > 5  # Considera uma feature importante se foi selecionada pela maioria dos modelos\n    print(\"Features finais selecionadas:\", features_selecionadas)\n    \n    # Selecionar as features do conjunto de dados de treinamento\n    dados_treino_selecionados = dados_treino[:, features_selecionadas]\n    print(\"Dados de treino selecionados com base nas features importantes:\", dados_treino_selecionados.shape)\n    \n    return features_selecionadas\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:39.985598Z","iopub.execute_input":"2024-07-01T19:35:39.985869Z","iopub.status.idle":"2024-07-01T19:35:40.010330Z","shell.execute_reply.started":"2024-07-01T19:35:39.985845Z","shell.execute_reply":"2024-07-01T19:35:40.009478Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport copy\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\nclass DeepNN(nn.Module):\n    def __init__(self, input_size):\n        super(DeepNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, 16)\n        self.fc5 = nn.Linear(16, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        x = torch.relu(self.fc4(x))\n        x = self.fc5(x)\n        return x\nclass WideNN(nn.Module):\n    def __init__(self, input_size):\n        super(WideNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nclass DeepNN2(nn.Module):\n    def __init__(self, input_size, layer_sizes=[128, 64, 32, 16], dropout_rate=0.2, use_batch_norm=True):\n        super(DeepNN2, self).__init__()\n        self.layers = nn.ModuleList()\n        self.use_batch_norm = use_batch_norm\n\n        # Iterar sobre as camadas e adicionar linear, batch norm (opcional) e dropout\n        previous_size = input_size\n        for size in layer_sizes:\n            self.layers.append(nn.Linear(previous_size, size))\n            if use_batch_norm:\n                self.layers.append(nn.BatchNorm1d(size))\n            \n            self.layers.append(nn.Dropout(dropout_rate))\n            previous_size = size\n\n        # Última camada, sem batch norm ou dropout\n        self.output_layer = nn.Linear(layer_sizes[-1], 1)\n\n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, nn.Linear):\n                x = torch.relu(layer(x))\n            else:\n                x = layer(x)\n        x = self.output_layer(x)\n        return x\n    \nclass DeepNN3(nn.Module):\n    def __init__(self, input_size, layer_sizes=[128, 64, 32, 16], dropout_rate=0.2, use_batch_norm=True, activation_func=torch.relu):\n        super(DeepNN3, self).__init__()\n        self.layers = nn.ModuleList()\n        self.use_batch_norm = use_batch_norm\n        self.activation_func = activation_func\n\n        # Iterar sobre as camadas e adicionar linear, batch norm (opcional) e dropout\n        previous_size = input_size\n        for size in layer_sizes:\n            self.layers.append(nn.Linear(previous_size, size))\n            if use_batch_norm:\n                self.layers.append(nn.BatchNorm1d(size))\n            self.layers.append(nn.Dropout(dropout_rate))\n            previous_size = size\n\n        # Última camada, sem batch norm ou dropout\n        self.output_layer = nn.Linear(layer_sizes[-1], 1)\n\n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, nn.Linear):\n                x = self.activation_func(layer(x))\n            else:\n                x = layer(x)\n        x = self.output_layer(x)\n        return x\nclass FlexibleBatchNN(nn.Module):\n    def __init__(self, input_size, layer_sizes=[128, 64, 32, 16], dropout_rate=0.2, use_batch_norm=True):\n        super(FlexibleBatchNN, self).__init__()\n        self.use_batch_norm = use_batch_norm\n        self.layers = nn.ModuleList()\n        \n        previous_size = input_size\n        for size in layer_sizes:\n            self.layers.append(nn.Linear(previous_size, size))\n            if use_batch_norm:\n                self.layers.append(nn.BatchNorm1d(size))\n            self.layers.append(nn.Dropout(dropout_rate))\n            previous_size = size\n        \n        self.output_layer = nn.Linear(layer_sizes[-1], 1)\n\n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, nn.BatchNorm1d) and x.shape[0] == 1:\n                x = layer(x)  # aplicando BN mesmo com batch size de 1\n            elif not isinstance(layer, nn.BatchNorm1d):\n                x = torch.relu(layer(x))\n        x = self.output_layer(x)\n        return x\nfrom transformers import BertModel, BertTokenizer\n\n\n    \ndef train_model_with_early_stopping(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, num_epochs=10, batch_size=32, n_epochs_stop=5, lr_decay_step=2, lr_decay_gamma=0.5):\n    best_loss = float('inf')\n    epochs_no_improve = 0\n    best_model = None\n    \n    # Definir o scheduler para decair o learning rate\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=lr_decay_gamma)\n    \n    # Verificar se a GPU está disponível e mover o modelo para a GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for i in range(0, len(X_train), batch_size):\n            if len(X_train) - i < 2 and len(X_train) - i > 0:  # Check if the remaining samples are less than 2\n                continue  # Skip this batch if less than 2 samples remaining\n            inputs = torch.tensor(X_train[i:i+batch_size]).float().to(device)\n            targets = torch.tensor(y_train[i:i+batch_size]).float().to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        # Aplicar o scheduler após cada época e registrar quando o learning rate muda\n        last_lr = scheduler.get_last_lr()[0]  # Guarda o último learning rate antes de atualizar\n        scheduler.step()\n        current_lr = scheduler.get_last_lr()[0]  # Pega o novo learning rate após atualizar\n\n        # Verifica se o learning rate mudou nesta época\n        if current_lr != last_lr:\n            print(f\"Learning rate updated from {last_lr} to {current_lr} at epoch {epoch + 1}\")\n        \n        # Validação no final de cada época\n        model.eval()\n        with torch.no_grad():\n            valid_loss = 0\n            for j in range(0, len(X_valid), batch_size):\n                if len(X_valid) - j < 2 and len(X_valid) - j > 0:  # Same check for validation data\n                    continue  # Skip this batch\n                inputs = torch.tensor(X_valid[j:j+batch_size]).float().to(device)\n                targets = torch.tensor(y_valid[j:j+batch_size]).float().to(device)\n\n                outputs = model(inputs)\n                v_loss = criterion(outputs, targets)\n                valid_loss += v_loss.item()\n\n            valid_loss /= (len(X_valid) / batch_size)\n\n        # Mostrar os resultados de treinamento e validação\n        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {valid_loss:.4f}')\n\n        # Atualizar o melhor modelo se a validação melhorou\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            epochs_no_improve = 0\n            best_model = copy.deepcopy(model.state_dict())\n            print(f'New best model saved at epoch {epoch+1} with validation loss {valid_loss:.4f}')\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve == n_epochs_stop:\n                print('Early stopping!')\n                break\n\n    return best_model\nimport torch\nimport numpy as np\nimport random\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:40.011917Z","iopub.execute_input":"2024-07-01T19:35:40.012322Z","iopub.status.idle":"2024-07-01T19:35:40.122493Z","shell.execute_reply.started":"2024-07-01T19:35:40.012287Z","shell.execute_reply":"2024-07-01T19:35:40.121698Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import cohen_kappa_score\n# from sklearn.linear_model import Ridge\n# import pickle\n# FOLDS = 3\n# def comp_score(y_true,y_pred):\n#     p = y_pred.clip(1,6).round(0)\n#     m = cohen_kappa_score(y_true, p, weights='quadratic')\n#     return m\n# RESULT_TRAIN = {}\n# optimizers = {\n#     'Adam': optim.Adam(model.parameters(), lr=0.001),\n# #     'SGD': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n# #     'RMSprop': optim.RMSprop(model.parameters(), lr=0.01),\n# #     'AdamW': optim.AdamW(model.parameters(), lr=0.001),\n# #     'Adagrad': optim.Adagrad(model.parameters(), lr=0.01),\n# #     'Adadelta': optim.Adadelta(model.parameters()),\n# #     'Adamax': optim.Adamax(model.parameters(), lr=0.002),\n# #     'ASGD': optim.ASGD(model.parameters(), lr=0.01),\n# #     'LBFGS': optim.LBFGS(model.parameters(), lr=0.1)\n# }\n\n\n# for n_ in range(3,4,1):\n#     for m_ in np.arange(2, 3, 1):\n#         for d_ in np.arange(0.2, 0.3, 0.1):\n#             for b_ in [32]:\n#                 for lr_ in [0.001]:\n#                     for lr_step in [3]:\n#                         for lr_gamma in [0.5]:\n#                             for ac_ in [torch.relu]:\n#                                 for opt in optimizers:\n#                                     features_selecionadas_lgbm = selecionar_features_importantes([], all_train_embeds,n_)\n#                                     all_train_embeds_scaled_new = all_train_embeds_scaled[:, features_selecionadas_lgbm]\n#                                     all_test_embeds_scaled_new = all_test_embeds_scaled[:, features_selecionadas_lgbm]\n#                                     print('Feature importance retornou esse all_train_embeds', all_train_embeds_scaled_new.shape )\n#                                     models_nn=[]\n#                                     oof_nn = np.zeros(len(train), dtype='float32')\n#                                     test_preds_nn = np.zeros((len(test),FOLDS), dtype='float32')\n#                                     for fold in range(FOLDS):\n#                                         print('#'*25)\n#                                         print('### Fold',fold+1)\n#                                         print('#'*25)\n\n#                                         train_index = train[\"fold\"] != fold\n#                                         valid_index = train[\"fold\"] == fold\n\n#                                         # Verificar se a GPU está disponível e mover o modelo para a GPU\n#                                         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#                                         print(all_train_embeds_scaled_new.shape)\n#                                         X_train = torch.tensor(all_train_embeds_scaled_new[train_index,], dtype=torch.float32).to(device)\n#                                         y_train = torch.tensor(train.loc[train_index,'score'].values, dtype=torch.float32).view(-1, 1).to(device)\n#                                         X_valid = torch.tensor(all_train_embeds_scaled_new[valid_index,], dtype=torch.float32).to(device)\n#                                         y_valid = torch.tensor(train.loc[valid_index,'score'].values, dtype=torch.float32).view(-1, 1).to(device)\n#                                         X_test = torch.tensor(all_test_embeds_scaled_new, dtype=torch.float32).to(device)\n\n\n#                                         # Instantiate the model\n#                                         input_size = X_train.shape[1]\n#                                         #model = SimpleNN(input_size)\n#                                         modelll= DeepNN3\n#                                         seed_everything(42)\n#                                         model = modelll(input_size, \n#                                                         layer_sizes=[round(1024*m_), round(512*m_), round(256*m_)], \n#                                                         dropout_rate=d_, \n#                                                         use_batch_norm=True,\n#                                                         activation_func=ac_\n#                                                        )\n\n#                                         model = model.to(device)\n\n#                                         # Define loss function and optimizer\n#                                         criterion = nn.MSELoss()\n#                                         optimizers = {\n#                                             'Adam': optim.Adam(model.parameters(), lr=0.001),\n# #                                             'SGD': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n# #                                             'RMSprop': optim.RMSprop(model.parameters(), lr=0.01),\n# #                                             'AdamW': optim.AdamW(model.parameters(), lr=0.001),\n# #                                             'Adagrad': optim.Adagrad(model.parameters(), lr=0.01),\n# #                                             'Adadelta': optim.Adadelta(model.parameters()),\n# #                                             'Adamax': optim.Adamax(model.parameters(), lr=0.002),\n# #                                             'ASGD': optim.ASGD(model.parameters(), lr=0.01),\n# #                                             'LBFGS': optim.LBFGS(model.parameters(), lr=0.1)\n#                                         }\n#                                         optimizer = optimizers[opt]\n\n#                                         # Training loop\n#                                         best_model_dict = train_model_with_early_stopping(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, \n#                                                                                           num_epochs=30, batch_size=32, n_epochs_stop=2,\n#                                                                                           lr_decay_step=lr_step, lr_decay_gamma=lr_gamma)\n#                                         seed_everything(42)\n#                                         model.load_state_dict(best_model_dict) \n\n#                                         models_nn.append(model)\n\n#                                         # Evaluate the model\n#                                         with torch.no_grad():\n#                                             model.eval()\n#                                             model = model.to(device)\n#                                             outputs = model(X_valid)\n#                                             test_loss = criterion(outputs, y_valid)\n#                                             print(f'Test Loss: {test_loss.item():.4f}')\n\n#                                         oof_nn[valid_index] = outputs.cpu().numpy().flatten()\n\n#                                         model.eval()\n#                                         with torch.no_grad():\n#                                             predictions = model(X_test)\n#                                         predicted_scores = predictions.cpu().numpy().flatten()\n#                                         test_preds_nn[:,fold] = predicted_scores\n\n#                                         score = comp_score(y_valid.cpu().numpy(), outputs.cpu().numpy().flatten())    \n#                                         print(f\"=> QWK score nn: {score}\")\n#                                         print()\n\n\n#                                     print('#'*25)\n#                                     score = comp_score(train.score.values, oof_nn)\n#                                     print(f'Overall CV QWK score _NN=',score) \n#                                     key = f'{opt}'\n\n#                                     RESULT_TRAIN[key] = {\n#                                         'model_name':'NN',\n#                                         'test_preds':test_preds_nn,\n#                                         'oof':oof_nn,\n#                                         'models':models_nn,\n#                                         'score':score,\n#                                         'features_selecionadas': features_selecionadas_lgbm,\n\n#                                     }\n\n        \n# # Salvar RESULT_TRAIN\n# with open('RESULT_TRAIN.pkl', 'wb') as f:\n#     pickle.dump(RESULT_TRAIN, f)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:40.128959Z","iopub.execute_input":"2024-07-01T19:35:40.129207Z","iopub.status.idle":"2024-07-01T19:35:40.140350Z","shell.execute_reply.started":"2024-07-01T19:35:40.129187Z","shell.execute_reply":"2024-07-01T19:35:40.139400Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for opt in optimizers:\n#     key = f'{opt}'\n#     print(f\"{key} {RESULT_TRAIN[key]['score']} {sum(RESULT_TRAIN[key]['features_selecionadas'])}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:35:40.141591Z","iopub.execute_input":"2024-07-01T19:35:40.141949Z","iopub.status.idle":"2024-07-01T19:35:40.153707Z","shell.execute_reply.started":"2024-07-01T19:35:40.141915Z","shell.execute_reply":"2024-07-01T19:35:40.152949Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\nfrom sklearn.linear_model import Ridge\nimport pickle\nFOLDS = 3\ndef comp_score(y_true,y_pred):\n    p = y_pred.clip(1,6).round(0)\n    m = cohen_kappa_score(y_true, p, weights='quadratic')\n    return m\n\nRESULT_TRAIN = {}\n\nMODELS_RANGE = (0,5,1)\n\nfor m_ in range(*MODELS_RANGE):\n    print(f'{m_}: ----------------------------------------------------')\n    # Treina LGBM\n    RIDGE_ = False\n    LGBM_ = False\n    SVR_ = False\n    NN_ = False\n    if m_==0:\n        LGBM_ = True\n        models_lgbm=[]\n        features_selecionadas_lgbm = selecionar_features_importantes(models_lgbm, all_train_embeds,0)\n        all_train_embeds_new = all_train_embeds[:, features_selecionadas_lgbm]\n        all_test_embeds_new = all_test_embeds[:, features_selecionadas_lgbm]\n        print('Feature importance retornou esse all_train_embeds', all_train_embeds_new.shape )\n        models_lgbm=[]\n        oof_lgbm = np.zeros(len(train), dtype='float32')\n        test_preds_lgbm = np.zeros((len(test),FOLDS), dtype='float32')\n    elif m_==1:\n        SVR_ = True\n        features_selecionadas_lgbm = selecionar_features_importantes([], all_train_embeds,4)\n        all_train_embeds_scaled_new = all_train_embeds_scaled[:, features_selecionadas_lgbm]\n        all_test_embeds_scaled_new = all_test_embeds_scaled[:, features_selecionadas_lgbm]\n        print('Feature importance retornou esse all_train_embeds_scaled', all_train_embeds_scaled_new.shape )\n        models_svr=[]\n        oof_svr = np.zeros(len(train), dtype='float32')\n        test_preds_svr = np.zeros((len(test),FOLDS), dtype='float32')\n    #Treina LGBM Feature importance \n    elif m_==2:\n        LGBM_ = True\n        features_selecionadas_lgbm = selecionar_features_importantes([], all_train_embeds,10)\n        all_train_embeds_new = all_train_embeds[:, features_selecionadas_lgbm]\n        all_test_embeds_new = all_test_embeds[:, features_selecionadas_lgbm]\n        print('Feature importance retornou esse all_train_embeds', all_train_embeds_new.shape )\n        models_lgbm=[]\n        oof_lgbm = np.zeros(len(train), dtype='float32')\n        test_preds_lgbm = np.zeros((len(test),FOLDS), dtype='float32')\n    elif m_==3:\n        RIDGE_ = True\n        features_selecionadas_lgbm = selecionar_features_importantes([], all_train_embeds,10)\n        all_train_embeds_scaled_new = all_train_embeds_scaled[:, features_selecionadas_lgbm]\n        all_test_embeds_scaled_new = all_test_embeds_scaled[:, features_selecionadas_lgbm]\n        print('Feature importance retornou esse all_train_embeds', all_train_embeds_scaled_new.shape )\n        models_ridge=[]\n        oof_ridge = np.zeros(len(train), dtype='float32')\n        test_preds_ridge = np.zeros((len(test),FOLDS), dtype='float32')\n    elif m_==4:\n        NN_ = True\n        features_selecionadas_lgbm = selecionar_features_importantes([], all_train_embeds,3)\n        all_train_embeds_scaled_new = all_train_embeds_scaled[:, features_selecionadas_lgbm]\n        all_test_embeds_scaled_new = all_test_embeds_scaled[:, features_selecionadas_lgbm]\n        print('Feature importance retornou esse all_train_embeds', all_train_embeds_scaled_new.shape )\n        models_nn=[]\n        oof_nn = np.zeros(len(train), dtype='float32')\n        test_preds_nn = np.zeros((len(test),FOLDS), dtype='float32')\n   \n    for fold in range(FOLDS):\n        print('#'*25)\n        print('### Fold',fold+1)\n        print('#'*25)\n\n        train_index = train[\"fold\"] != fold\n        valid_index = train[\"fold\"] == fold\n        \n        # Modelo LGBM -----------------------------------------------------\n        if LGBM_:\n            print(all_train_embeds_new.shape)\n            X_train = all_train_embeds_new[train_index,]\n            y_train = train.loc[train_index,'score'].values\n            X_valid = all_train_embeds_new[valid_index,]\n            y_valid = train.loc[valid_index,'score'].values\n            X_test = all_test_embeds_new\n\n\n            model = lgb.LGBMRegressor(\n                objective=qwk_obj,\n                metrics='None',\n                learning_rate=0.05,\n                max_depth=5,\n                num_leaves=10,\n                colsample_bytree=0.5,\n                reg_alpha=0.1,\n                reg_lambda=0.8,\n                n_estimators=1500,\n                random_state=42,\n                extra_trees=True,\n                class_weight='balanced',\n                verbosity=-1,\n                device_type='gpu'\n            )\n\n            model.fit(\n                X_train, y_train,\n                eval_names=['train', 'valid'],\n                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                eval_metric=quadratic_weighted_kappa,\n                callbacks=callbacks\n            )\n            models_lgbm.append(model)\n            \n            preds_lgbm = model.predict(X_valid)\n            test_preds_lgbm[:,fold] = model.predict(X_test)\n            oof_lgbm[valid_index] = preds_lgbm\n\n            score = comp_score(y_valid, preds_lgbm)    \n            print(f\"=> QWK score lgbm m_:{m_}: {score}\")\n            print()\n        # Modelo SVR -----------------------------------------\n        elif SVR_:\n            print(all_train_embeds_scaled_new.shape)\n            X_train = all_train_embeds_scaled_new[train_index,]\n            y_train = train.loc[train_index,'score'].values\n            X_valid = all_train_embeds_scaled_new[valid_index,]\n            y_valid = train.loc[valid_index,'score'].values\n            X_test = all_test_embeds_scaled_new\n            \n            model = SVR(C=10) #C=10\n            model.fit(X_train, y_train)\n            models_svr.append(model)\n            \n            preds_svr = model.predict(X_valid)\n            test_preds_svr[:,fold] = model.predict(X_test)\n            oof_svr[valid_index] = preds_svr\n\n            score = comp_score(y_valid, preds_svr)    \n            print(f\"=> QWK score svr: {score}\")\n            print()\n        elif RIDGE_:\n            print(all_train_embeds_scaled_new.shape)\n            X_train = all_train_embeds_scaled_new[train_index,]\n            y_train = train.loc[train_index,'score'].values\n            X_valid = all_train_embeds_scaled_new[valid_index,]\n            y_valid = train.loc[valid_index,'score'].values\n            X_test = all_test_embeds_scaled_new\n            \n            model = Ridge(alpha=1.0)\n            model.fit(X_train, y_train)\n            models_ridge.append(model)\n            \n            preds_ridge = model.predict(X_valid)\n            test_preds_ridge[:,fold] = model.predict(X_test)\n            oof_ridge[valid_index] = preds_ridge\n\n            score = comp_score(y_valid, preds_ridge)    \n            print(f\"=> QWK score svr: {score}\")\n            print()\n        elif NN_:\n            # Verificar se a GPU está disponível e mover o modelo para a GPU\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            print(all_train_embeds_scaled_new.shape)\n            X_train = torch.tensor(all_train_embeds_scaled_new[train_index,], dtype=torch.float32).to(device)\n            y_train = torch.tensor(train.loc[train_index,'score'].values, dtype=torch.float32).view(-1, 1).to(device)\n            X_valid = torch.tensor(all_train_embeds_scaled_new[valid_index,], dtype=torch.float32).to(device)\n            y_valid = torch.tensor(train.loc[valid_index,'score'].values, dtype=torch.float32).view(-1, 1).to(device)\n            X_test = torch.tensor(all_test_embeds_scaled_new, dtype=torch.float32).to(device)\n            \n            \n            # Instantiate the model\n            input_size = X_train.shape[1]\n            #model = SimpleNN(input_size)\n            modelll= DeepNN2\n            seed_everything(42)\n            model = modelll(\n                input_size, \n                            layer_sizes=[2048, 1024, 512], \n                            dropout_rate=0.2, \n                            use_batch_norm=True\n            )\n            \n            model = model.to(device)\n\n            # Define loss function and optimizer\n            criterion = nn.MSELoss()\n            optimizer = optim.Adam(model.parameters(), lr=0.001)\n            \n            # Training loop\n            num_epochs = 10\n            batch_size = 32\n            best_model_dict = train_model_with_early_stopping(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, \n                                                              num_epochs=30, batch_size=32, n_epochs_stop=2,\n                                                              lr_decay_step=3, lr_decay_gamma=0.5)\n            seed_everything(42)\n            model.load_state_dict(best_model_dict) \n            \n            models_nn.append(model)\n            \n            # Evaluate the model\n            with torch.no_grad():\n                model.eval()\n                model = model.to(device)\n                outputs = model(X_valid)\n                test_loss = criterion(outputs, y_valid)\n                print(f'Test Loss: {test_loss.item():.4f}')\n            \n            oof_nn[valid_index] = outputs.cpu().numpy().flatten()\n            \n            model.eval()\n            with torch.no_grad():\n                predictions = model(X_test)\n            predicted_scores = predictions.cpu().numpy().flatten()\n            test_preds_nn[:,fold] = predicted_scores\n\n            score = comp_score(y_valid.cpu().numpy(), outputs.cpu().numpy().flatten())    \n            print(f\"=> QWK score nn: {score}\")\n            print()\n    \n    # Termina fold ---------------------------------------\n    if LGBM_:\n        RESULT_TRAIN[m_] = {\n            'model_name':'LGBM',\n            'test_preds':test_preds_lgbm,\n            'oof':oof_lgbm,\n            'models':models_lgbm,\n            'score':score,\n            'features_selecionadas': features_selecionadas_lgbm,\n            \n        }\n        # Score LGBM -----------------------------------------------------\n        print('#'*25)\n        score = comp_score(train.score.values, oof_lgbm)\n        print(f'Overall CV QWK score _lgbm=',score)\n    if SVR_:\n        RESULT_TRAIN[m_] = {\n            'model_name':'SVR',\n            'test_preds':test_preds_svr,\n            'oof':oof_svr,\n            'models':models_svr,\n            'score':score,\n            'features_selecionadas': features_selecionadas_lgbm,\n            \n        }\n        # Score LGBM -----------------------------------------------------\n        print('#'*25)\n        score = comp_score(train.score.values, oof_svr)\n        print(f'Overall CV QWK score _svr=',score    )\n    if RIDGE_:\n        RESULT_TRAIN[m_] = {\n            'model_name':'RIDGE',\n            'test_preds':test_preds_ridge,\n            'oof':oof_ridge,\n            'models':models_ridge,\n            'score':score,\n            'features_selecionadas': features_selecionadas_lgbm,\n            \n        }\n        # Score LGBM -----------------------------------------------------\n        print('#'*25)\n        score = comp_score(train.score.values, oof_ridge)\n        print(f'Overall CV QWK score _RIDGE=',score)  \n    if NN_:\n        RESULT_TRAIN[m_] = {\n            'model_name':'NN',\n            'test_preds':test_preds_nn,\n            'oof':oof_nn,\n            'models':models_nn,\n            'score':score,\n            'features_selecionadas': features_selecionadas_lgbm,\n            \n        }\n        # Score LGBM -----------------------------------------------------\n        print('#'*25)\n        score = comp_score(train.score.values, oof_nn)\n        print(f'Overall CV QWK score _NN=',score)  \n\n        \n# Salvar RESULT_TRAIN\nwith open('RESULT_TRAIN.pkl', 'wb') as f:\n    pickle.dump(RESULT_TRAIN, f)","metadata":{"papermill":{"duration":28.475824,"end_time":"2024-05-13T19:19:07.77502","exception":false,"start_time":"2024-05-13T19:18:39.299196","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T19:35:40.154910Z","iopub.execute_input":"2024-07-01T19:35:40.155346Z","iopub.status.idle":"2024-07-01T19:42:00.406726Z","shell.execute_reply.started":"2024-07-01T19:35:40.155321Z","shell.execute_reply":"2024-07-01T19:42:00.405571Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Modelo Final Ensamble\n# oof = np.zeros(len(train), dtype='float32')\n# test_preds = np.zeros(len(test), dtype='float32') \n\n# # Adiciona os 'oof' dos modelos especificados\n# num_models = len(range(*MODELS_RANGE)) +1\n# for m_ in range(*MODELS_RANGE):\n#     oof += RESULT_TRAIN[m_]['oof']\n#     test_preds += np.mean(RESULT_TRAIN[m_]['test_preds'], axis=1)\n\n# oof = (oof \n#        + predicted_train\n#       ) / num_models\n# test_preds = (test_preds \n#               + predicted_test\n#              ) / num_models\n# test_preds","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:42:00.407998Z","iopub.execute_input":"2024-07-01T19:42:00.408363Z","iopub.status.idle":"2024-07-01T19:42:00.413252Z","shell.execute_reply.started":"2024-07-01T19:42:00.408334Z","shell.execute_reply":"2024-07-01T19:42:00.412282Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# treina ensamble","metadata":{}},{"cell_type":"code","source":"# Selecionar as features importantes\nfeatures_selecionadas_lgbm = selecionar_features_importantes([], all_train_embeds, 0)\n\n# Filtrar os embeddings com as features selecionadas\nall_train_embeds_new = all_train_embeds[:, features_selecionadas_lgbm]\nall_test_embeds_new = all_test_embeds[:, features_selecionadas_lgbm]\n\nprint('Feature importance retornou esse all_train_embeds', all_train_embeds_new.shape)\n\n\n\n# Adiciona os 'oof' dos modelos especificados\nfor m_ in range(*MODELS_RANGE):\n    oof_col = RESULT_TRAIN[m_]['oof'].reshape(-1, 1)\n    test_preds_col = np.mean(RESULT_TRAIN[m_]['test_preds'], axis=1).reshape(-1, 1)\n    \n    # Adicionando colunas aos arrays numpy\n    all_train_embeds_new = np.concatenate((all_train_embeds_new, oof_col), axis=1)\n    all_test_embeds_new = np.concatenate((all_test_embeds_new, test_preds_col), axis=1)\n\n    \n# adiciona deberta\noof_col = predicted_train.to_numpy().reshape(-1, 1)\ntest_preds_col = predicted_test.reshape(-1, 1)\nall_train_embeds_new = np.concatenate((all_train_embeds_new, oof_col), axis=1)\nall_test_embeds_new = np.concatenate((all_test_embeds_new, test_preds_col), axis=1)\n    \nprint('Tamanho do all_train_embeds_new:', all_train_embeds_new.shape)\nprint('Tamanho do all_test_embeds_new:', all_test_embeds_new.shape)\n\n# Mostrar as últimas 7 colunas para visualização\nprint('Últimas 7 colunas de all_train_embeds_new:')\nprint(all_train_embeds_new[:, -7:])\n\nprint('Últimas 7 colunas de all_test_embeds_new:')\nprint(all_test_embeds_new[:, -7:])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:42:00.414599Z","iopub.execute_input":"2024-07-01T19:42:00.414977Z","iopub.status.idle":"2024-07-01T19:42:02.027931Z","shell.execute_reply.started":"2024-07-01T19:42:00.414946Z","shell.execute_reply":"2024-07-01T19:42:02.026897Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(3,6),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n\n# Fit e transform no conjunto de treinamento\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\ndense_train_matrix = train_tfid.toarray()\n\n# Transform no conjunto de teste\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_test_matrix = test_tfid.toarray()\n\n# Concatenar as matrizes TF-IDF com os embeddings fornecidos\nall_train_embeds_new = np.concatenate((all_train_embeds_new, dense_train_matrix), axis=1)\nall_test_embeds_new = np.concatenate((all_test_embeds_new, dense_test_matrix), axis=1)\n\n# Mostrar as últimas 7 colunas para visualização\nprint('Últimas 7 colunas de all_train_embeds_new:')\nprint(all_train_embeds_new.shape)\n\nprint('Últimas 7 colunas de all_test_embeds_new:')\nprint(all_test_embeds_new.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:42:02.029278Z","iopub.execute_input":"2024-07-01T19:42:02.029614Z","iopub.status.idle":"2024-07-01T19:45:25.816075Z","shell.execute_reply.started":"2024-07-01T19:42:02.029590Z","shell.execute_reply":"2024-07-01T19:45:25.814839Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Definir o vetor CountVectorizer conforme sua especificação\nvectorizer_cnt = CountVectorizer(\n    tokenizer=lambda x: x,\n    preprocessor=lambda x: x,\n    token_pattern=None,\n    strip_accents='unicode',\n    analyzer='word',\n    ngram_range=(2, 3),\n    min_df=0.10,\n    max_df=0.85,\n)\n\n# Fit e transform no conjunto de treinamento\ntrain_cnt = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_train_matrix_cnt = train_cnt.toarray()\n\n# Transform no conjunto de teste\ntest_cnt = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_test_matrix_cnt = test_cnt.toarray()\n\n# Concatenar as matrizes TF-IDF com os embeddings fornecidos\nall_train_embeds_new = np.concatenate((all_train_embeds_new, dense_train_matrix_cnt), axis=1)\nall_test_embeds_new = np.concatenate((all_test_embeds_new, dense_test_matrix_cnt), axis=1)\n\n# Mostrar as últimas 7 colunas para visualização\nprint('Últimas 7 colunas de all_train_embeds_new:')\nprint(all_train_embeds_new.shape)\n\nprint('Últimas 7 colunas de all_test_embeds_new:')\nprint(all_test_embeds_new.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:45:25.817357Z","iopub.execute_input":"2024-07-01T19:45:25.817672Z","iopub.status.idle":"2024-07-01T19:46:47.890641Z","shell.execute_reply.started":"2024-07-01T19:45:25.817645Z","shell.execute_reply":"2024-07-01T19:46:47.889607Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models_lgbm=[]\noof_lgbm = np.zeros(len(train), dtype='float32')\ntest_preds_lgbm = np.zeros((len(test),FOLDS), dtype='float32')\n\nfor fold in range(FOLDS):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('#'*25)\n\n    train_index = train[\"fold\"] != fold\n    valid_index = train[\"fold\"] == fold\n    \n    print(all_train_embeds_new.shape)\n    X_train = all_train_embeds_new[train_index,]\n    y_train = train.loc[train_index,'score'].values\n    X_valid = all_train_embeds_new[valid_index,]\n    y_valid = train.loc[valid_index,'score'].values\n    X_test = all_test_embeds_new\n\n\n    model = lgb.LGBMRegressor(\n        objective=qwk_obj,\n        metrics='None',\n        learning_rate=0.05,\n        max_depth=5,\n        num_leaves=10,\n        colsample_bytree=0.5,\n        reg_alpha=0.1,\n        reg_lambda=0.8,\n        n_estimators=1500,\n        random_state=42,\n        extra_trees=True,\n        class_weight='balanced',\n        verbosity=-1,\n        device_type='gpu'\n    )\n\n    model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models_lgbm.append(model)\n\n    preds_lgbm = model.predict(X_valid)\n    test_preds_lgbm[:,fold] = model.predict(X_test)\n    oof_lgbm[valid_index] = preds_lgbm\n\n    score = comp_score(y_valid, preds_lgbm)    \n    print(f\"=> QWK score lgbm m_:{m_}: {score}\")\n    print()   \n    \nRESULT_TRAIN[10] = {\n    'model_name':'LGBM',\n    'test_preds':test_preds_lgbm,\n    'oof':oof_lgbm,\n    'models':models_lgbm,\n    'score':score,\n    'features_selecionadas': features_selecionadas_lgbm,\n\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:46:47.891943Z","iopub.execute_input":"2024-07-01T19:46:47.892245Z","iopub.status.idle":"2024-07-01T20:10:15.345736Z","shell.execute_reply.started":"2024-07-01T19:46:47.892220Z","shell.execute_reply":"2024-07-01T20:10:15.344617Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_selecionadas_lgbm = selecionar_features_importantes(models_lgbm, all_train_embeds_new,5)\nall_train_embeds_new2 = all_train_embeds_new[:, features_selecionadas_lgbm]\nall_test_embeds_new2 = all_test_embeds_new[:, features_selecionadas_lgbm]\nprint('Feature importance retornou esse all_train_embeds', all_train_embeds_new2.shape )\nmodels_lgbm=[]\noof_lgbm = np.zeros(len(train), dtype='float32')\ntest_preds_lgbm = np.zeros((len(test),FOLDS), dtype='float32')\n\nfor fold in range(FOLDS):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('#'*25)\n\n    train_index = train[\"fold\"] != fold\n    valid_index = train[\"fold\"] == fold\n    \n    print(all_train_embeds_new2.shape)\n    X_train = all_train_embeds_new2[train_index,]\n    y_train = train.loc[train_index,'score'].values\n    X_valid = all_train_embeds_new2[valid_index,]\n    y_valid = train.loc[valid_index,'score'].values\n    X_test = all_test_embeds_new2\n\n\n    model = lgb.LGBMRegressor(\n        objective=qwk_obj,\n        metrics='None',\n        learning_rate=0.05,\n        max_depth=5,\n        num_leaves=10,\n        colsample_bytree=0.5,\n        reg_alpha=0.1,\n        reg_lambda=0.8,\n        n_estimators=1500,\n        random_state=42,\n        extra_trees=True,\n        class_weight='balanced',\n        verbosity=-1,\n        device_type='gpu'\n    )\n\n    model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models_lgbm.append(model)\n\n    preds_lgbm = model.predict(X_valid)\n    test_preds_lgbm[:,fold] = model.predict(X_test)\n    oof_lgbm[valid_index] = preds_lgbm\n\n    score = comp_score(y_valid, preds_lgbm)    \n    print(f\"=> QWK score lgbm m_:{m_}: {score}\")\n    print()   \n    \nRESULT_TRAIN[11] = {\n    'model_name':'LGBM',\n    'test_preds':test_preds_lgbm,\n    'oof':oof_lgbm,\n    'models':models_lgbm,\n    'score':score,\n    'features_selecionadas': features_selecionadas_lgbm,\n\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:10:15.347479Z","iopub.execute_input":"2024-07-01T20:10:15.347867Z","iopub.status.idle":"2024-07-01T20:11:59.109780Z","shell.execute_reply.started":"2024-07-01T20:10:15.347835Z","shell.execute_reply":"2024-07-01T20:11:59.108665Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RESULT_TRAIN[12] = {\n    'model_name':'BERT',\n    'test_preds':predicted_test,\n    'oof':predicted_train,\n\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:14:17.564155Z","iopub.execute_input":"2024-07-01T20:14:17.564567Z","iopub.status.idle":"2024-07-01T20:14:17.569365Z","shell.execute_reply.started":"2024-07-01T20:14:17.564534Z","shell.execute_reply":"2024-07-01T20:14:17.568490Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# modelo publico","metadata":{}},{"cell_type":"code","source":"def text_prep(x):\n    return x\nimport aes2_preproces_cache_vectorize\nTUNE = False\n\nimport gc\ngc.collect()\n\nimport pickle\n\nwith open(\"/kaggle/usr/lib/aes2_added_fb_prize_as_features_preprocessing/train_feats.pickle\", \"rb\") as f:\n    train_feats = pickle.load(f)\nwith open(\"/kaggle/usr/lib/aes2_added_fb_prize_as_features_preprocessing/X.pickle\", \"rb\") as f:\n    X = pickle.load(f)\nwith open(\"/kaggle/usr/lib/aes2_added_fb_prize_as_features_preprocessing/y.pickle\", \"rb\") as f:\n    y = pickle.load(f)\nwith open(\"/kaggle/usr/lib/aes2_added_fb_prize_as_features_preprocessing/y_split.pickle\", \"rb\") as f:\n    y_split = pickle.load(f)\nwith open(\n    \"/kaggle/input/aes2-eval-added-fb-prize-as-features-8168c5/feature_select.pickle\", \"rb\"\n) as f:\n    feature_select = pickle.load(f)\n    \naes2_preproces_cache_vectorize.feature_select = feature_select\n\nimport numpy as np\n\n# X = train_feats[feature_select + [\"max_repeated_word_count\"]].astype(np.float32).values\nX = train_feats[feature_select].astype(np.float32).values\nX.shape\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, cohen_kappa_score\nfrom aes2_preproces_cache_vectorize import *\n\nimport builtins\noriginal_print = builtins.print\ndef patch_print(text, *args, **kwargs):\n    if not isinstance(text, str):\n        return original_print(text, *args, **kwargs)\n    if \"\t\" in text:\n        n = text.split(\"\t\")[0]\n        try:\n            n = int(n.strip(\"[]\"))\n            if n % 100 == 0:\n                original_print(text)\n            return None\n        except:\n            pass\n    original_print(text, *args, **kwargs)\n\nbuiltins.print = patch_print\n\nn_splits = 15\nmodels = []\npredictions = []\nf1_scores = []\nkappa_scores = []\n\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    if isinstance(y_pred, xgb.QuantileDMatrix):\n        # XGB\n        y_true, y_pred = y_pred, y_true\n\n        y_true = (y_true.get_label() + a).round()\n        y_pred = (y_pred + a).clip(1, 6).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk\n\n    else:\n        # For lgb\n        y_true = y_true + a\n        y_pred = (y_pred + a).clip(1, 6).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk, True\n    \ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\n\nclass Predictor:\n    def __init__(self, models: list, n: float = 0.746):\n        self.models = models\n        self.n = n\n#         self.xgb_boost_best_iter = models[1].\n    def predict(self, X):\n        n_models = len(self.models)\n        predicted = None\n        n = self.n\n        for i, model in enumerate(self.models):\n            if i == 0:\n                predicted = n*model.predict(X)\n            else:\n                predicted += (1-n)*model.predict(X)\n        return predicted\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\nxgboost_best_iters = []\nlight_best_iters = []\n\n_test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\nENABLE_DONT_WASTE_YOUR_RUN_TIME = len(_test) < 10\n# ENABLE_DONT_WASTE_YOUR_RUN_TIME = False\noof = np.zeros(len(train), dtype='float32')\nfor i, (train_index, test_index) in enumerate(skf.split(X, y_split), 1):\n    # Split the data into training and testing sets for this fold\n    print('fold',i)\n    X_train_fold, X_test_fold = X[train_index], X[test_index]\n    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75, first_metric_only=True)]\n    light = lgb.LGBMRegressor(\n            objective = qwk_obj,\n            metrics = 'None',\n            learning_rate = 0.05,\n            max_depth = 8,\n            num_leaves = 10,\n            colsample_bytree=0.3,\n            reg_alpha = 0.7,\n            reg_lambda = 0.1,\n            n_estimators=700,\n            random_state=42,\n            extra_trees=True,\n            class_weight='balanced',\n            device='gpu' if CUDA_AVAILABLE else 'cpu',\n            verbosity = - 1\n        )\n\n    # Fit the model on the training data for this fold  \n    light.fit(\n        X_train_fold,\n        y_train_fold,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    light_best_iters.append(light.best_iteration_)\n    print(f\"Light best iter: {light.best_iteration_}\")\n   \n    xgb_regressor = xgb.XGBRegressor(\n        objective = qwk_obj,\n        verbosity = 0,\n        metrics = 'None',\n        learning_rate = 0.1,\n        max_depth = 8,\n        num_leaves = 10,\n        colsample_bytree=0.5,\n        reg_alpha = 0.1,\n        reg_lambda = 1.6, # validation_1-QWK:0.84010\n#         reg_lambda = 0.8, validation_1-QWK:0.83917\n        \n        n_estimators=1024,\n        random_state=42,\n#         gamma=0.05,\n        extra_trees=True,\n        scale_pos_weight=100,\n        class_weight='balanced',\n        tree_method=\"hist\",\n        device=\"gpu\" if CUDA_AVAILABLE else \"cpu\"\n    #             device='gpu',\n    #             verbosity = 1\n    )\n    \n    xgb_callbacks = [\n        xgb.callback.EvaluationMonitor(period=25),\n        xgb.callback.EarlyStopping(75, metric_name=\"QWK\", maximize=True, save_best=True, data_name=\"validation_1\")\n    ]\n    xgb_regressor.fit(\n        X_train_fold,\n        y_train_fold,\n        eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=xgb_callbacks\n    )\n    print(f\"XGBoost best iter: {xgb_regressor.get_booster().best_iteration}\")\n    xgboost_best_iters.append(xgb_regressor.get_booster().best_iteration)\n    predictor = Predictor([light, xgb_regressor], n=0.709)\n\n    models.append(predictor)\n    # Make predictions on the test data for this fold\n    predictions_fold = predictor.predict(X_test_fold)\n    predictions_fold = predictions_fold + a\n    oof[test_index] = predictions_fold\n    predictions_fold = predictions_fold.clip(1, 6).round()\n    predictions.append(predictions_fold)\n    # Calculate and store the F1 score for this fold\n    f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n    f1_scores.append(f1_fold)\n\n    # Calculate and store the Cohen's kappa score for this fold\n    kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n    kappa_scores.append(kappa_fold)\n#         predictor.booster_.save_model(f'fold_{i}.txt')\n    cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n    disp = ConfusionMatrixDisplay(\n        confusion_matrix=cm,\n        display_labels=[x for x in range(1,7)]\n    )\n    disp.plot()\n    plt.show()\n    print(f'F1 score across fold: {f1_fold}')\n    print(f'Cohen kappa score across fold: {kappa_fold}')\n\n    gc.collect()\n    if ENABLE_DONT_WASTE_YOUR_RUN_TIME:\n        break\n\ndef find_thresholds(true, pred, steps=50):\n\n    # SAVE TRIALS FOR PLOTTING\n    xs = [[],[],[],[],[]]\n    ys = [[],[],[],[],[]]\n\n    # COMPUTE BASELINE METRIC\n    threshold = [1.5, 2.5, 3.5, 4.5, 5.5]\n    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n                    labels=[1,2,3,4,5,6]).astype('int32')\n    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n\n    # FIND FIVE OPTIMAL THRESHOLDS\n    for k in range(5):\n        for sign in [1,-1]:\n            v = threshold[k]\n            threshold2 = threshold.copy()\n            stop = 0\n            while stop<steps:\n                # TRY NEW THRESHOLD\n                v += sign * 0.01\n                threshold2[k] = v\n                pred2 = pd.cut(pred, [-np.inf] + threshold2 + [np.inf], \n                                labels=[1,2,3,4,5,6]).astype('int32')\n                metric = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n\n                # SAVE TRIALS FOR PLOTTING\n                xs[k].append(v)\n                ys[k].append(metric)\n\n                # EARLY STOPPING\n                if metric<=best:\n                    stop += 1\n                else:\n                    stop = 0\n                    best = metric\n                    threshold = threshold2.copy()\n\n    # COMPUTE FINAL METRIC\n    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n                    labels=[1,2,3,4,5,6]).astype('int32')\n    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")   \n\n    # RETURN RESULTS\n    threshold = [np.round(t,3) for t in threshold]\n    return best, threshold, xs, ys\n\n# best, thresholds, xs, ys = find_thresholds(y_split, oof, steps=500)\n# print('Best thresholds are:', thresholds )\n# print('=> achieve Overall CV QWK score =', best )\n\nthresholds = [1.51, 2.6, 3.5, 4.59, 5.56]\n\nif TUNE:\n    import optuna\n\n    def objective(trial):\n        xgb_regressor = xgb.XGBRegressor(\n            objective = qwk_obj,\n            verbosity = 0,\n            metrics = 'None',\n            learning_rate = 0.1,\n            max_depth = 8,\n            num_leaves = 10,\n            colsample_bytree=0.5,\n            reg_alpha = 0.1,\n            reg_lambda = trial.suggest_float('reg_lambda', 0, 2, step=0.05),\n    #         reg_lambda = 1.0, # validation_1-QWK:0.84010\n    #         reg_lambda = 0.8, validation_1-QWK:0.83917\n\n            n_estimators=1024,\n            random_state=42,\n    #         gamma=0.05,\n            extra_trees=True,\n            scale_pos_weight=100,\n            class_weight='balanced',\n            tree_method=\"hist\",\n            device=\"gpu\" if CUDA_AVAILABLE else \"cpu\"\n        #             device='gpu',\n        #             verbosity = 1\n        )\n\n        xgb_callbacks = [\n            xgb.callback.EvaluationMonitor(period=25),\n            xgb.callback.EarlyStopping(175, metric_name=\"QWK\", maximize=True, save_best=True, data_name=\"validation_1\")\n        ]\n        predictor = xgb_regressor.fit(\n            X_train_fold,\n            y_train_fold,\n            eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n            eval_metric=quadratic_weighted_kappa,\n            callbacks=xgb_callbacks\n        )\n\n        predictions_fold = predictor.predict(X_test_fold)\n        predictions_fold = predictions_fold + a\n        predictions_fold = predictions_fold.clip(1, 6).round()\n        predictions.append(predictions_fold)\n        # Calculate and store the F1 score for this fold\n\n\n        # Calculate and store the Cohen's kappa score for this fold\n        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n        return kappa_fold\n\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=100)\n\n\n\n    print(f\"{study.best_params=}\")\n\n    with open(\"optuna_study.pickle\", \"wb\") as f:\n        pickle.dump(study, f)\n        \nwith open(\"models.pickle\", \"wb\") as f:\n    pickle.dump(models, f)\n\nmean_f1_score = np.mean(f1_scores)\nmean_kappa_score = np.mean(kappa_scores)\n# Print the mean scores\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')\nprint(f\"XGBoost mean best iters: {sum(xgboost_best_iters)/len(xgboost_best_iters)}\")\nprint(f\"LightBoost mean best iters: {sum(light_best_iters)/len(light_best_iters)}\")\n\nfrom aes2_preproces_cache_vectorize import preprocess_test, infer\n\ntest_feats = preprocess_test()\n\ndef infer(test_feats, models):\n    probabilities = []\n    for model in models:\n        proba = model.predict(test_feats) + a\n        probabilities.append(proba)\n        del model\n        gc.collect()\n\n    # Compute the average probabilities across all models\n    predictions = np.mean(probabilities, axis=0)\n#     predictions = pd.cut(predictions, [-np.inf] + thresholds + [np.inf], \n#                     labels=[1,2,3,4,5,6]).astype('int32')\n\n\n\n#     # Print the predictions\n#     print(predictions)\n\n#     submission = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n#     submission['score'] = predictions\n#     submission['score'] = submission['score'].astype(int)\n#     submission.to_csv(\"submission.csv\", index=None)\n    return predictions\n\n\npredictions = infer(\n    test_feats[feature_select].astype(np.float32),\n    models\n)\ndisplay(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T22:34:01.877852Z","iopub.execute_input":"2024-07-01T22:34:01.878249Z","iopub.status.idle":"2024-07-01T23:24:37.904197Z","shell.execute_reply.started":"2024-07-01T22:34:01.878220Z","shell.execute_reply":"2024-07-01T23:24:37.903119Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(oof)\ndisplay(predictions)\n\nRESULT_TRAIN[13] = {\n    'model_name':'public',\n    'test_preds':predictions,\n    'oof':oof,\n\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-01T23:35:38.174554Z","iopub.execute_input":"2024-07-01T23:35:38.175433Z","iopub.status.idle":"2024-07-01T23:35:38.228046Z","shell.execute_reply.started":"2024-07-01T23:35:38.175399Z","shell.execute_reply":"2024-07-01T23:35:38.226912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ensamble fi","metadata":{}},{"cell_type":"code","source":"# Modelo Final Ensamble\noof = np.zeros(len(train), dtype='float32')\ntest_preds = np.zeros(len(test), dtype='float32') \n\nMODELS_RANGE = [(13,0.85),(0,0.03),(10,0.02),(11,0.02),(12,0.02),(1,0.02),(4,0.02),(2,0.01),(3,0.01)]\n# Adiciona os 'oof' dos modelos especificados\nnum_models = len(MODELS_RANGE)\nfor m_, vs_ in MODELS_RANGE:\n    oof += (RESULT_TRAIN[m_]['oof'] *vs_)\n    # Verifica se 'test_preds' é multidimensional\n    if RESULT_TRAIN[m_]['test_preds'].ndim > 1:\n        # Se sim, calcula a média ao longo do eixo 1\n        test_preds += (np.mean(RESULT_TRAIN[m_]['test_preds'], axis=1) *vs_)\n    else:\n        # Se não, usa o array como está\n        test_preds += (RESULT_TRAIN[m_]['test_preds'] *vs_)\n\ndisplay(oof)\ndisplay(test_preds)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:19:25.146304Z","iopub.execute_input":"2024-07-01T20:19:25.146695Z","iopub.status.idle":"2024-07-01T20:19:25.165111Z","shell.execute_reply.started":"2024-07-01T20:19:25.146666Z","shell.execute_reply":"2024-07-01T20:19:25.164295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Find QWK Thresholds","metadata":{"papermill":{"duration":0.011076,"end_time":"2024-05-13T19:19:07.797436","exception":false,"start_time":"2024-05-13T19:19:07.78636","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def find_thresholds(true, pred, steps=50):\n\n    # SAVE TRIALS FOR PLOTTING\n    xs = [[],[],[],[],[]]\n    ys = [[],[],[],[],[]]\n\n    # COMPUTE BASELINE METRIC\n    threshold = [1.5, 2.5, 3.5, 4.5, 5.5]\n    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n                    labels=[1,2,3,4,5,6]).astype('int32')\n    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n\n    # FIND FIVE OPTIMAL THRESHOLDS\n    for k in range(5):\n        for sign in [1,-1]:\n            v = threshold[k]\n            threshold2 = threshold.copy()\n            stop = 0\n            while stop<steps:\n\n                # TRY NEW THRESHOLD\n                v += sign * 0.001\n                threshold2[k] = v\n                pred2 = pd.cut(pred, [-np.inf] + threshold2 + [np.inf], \n                                labels=[1,2,3,4,5,6]).astype('int32')\n                metric = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n\n                # SAVE TRIALS FOR PLOTTING\n                xs[k].append(v)\n                ys[k].append(metric)\n\n                # EARLY STOPPING\n                if metric<=best:\n                    stop += 1\n                else:\n                    stop = 0\n                    best = metric\n                    threshold = threshold2.copy()\n\n    # COMPUTE FINAL METRIC\n    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n                    labels=[1,2,3,4,5,6]).astype('int32')\n    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")   \n\n    # RETURN RESULTS\n    threshold = [np.round(t,3) for t in threshold]\n    return best, threshold, xs, ys","metadata":{"papermill":{"duration":0.041129,"end_time":"2024-05-13T19:19:07.849209","exception":false,"start_time":"2024-05-13T19:19:07.80808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T20:19:32.972556Z","iopub.execute_input":"2024-07-01T20:19:32.973376Z","iopub.status.idle":"2024-07-01T20:19:32.985545Z","shell.execute_reply.started":"2024-07-01T20:19:32.973344Z","shell.execute_reply":"2024-07-01T20:19:32.984663Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"func_ = True\ntry:\n    best, thresholds, xs, ys = find_thresholds(train.score.values, oof, steps=500)\n    print('Best thresholds are:', thresholds )\n    print('=> achieve Overall CV QWK score =', best )\n    func_ = True\nexcept:\n    thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\n    xs = [[], [], [], [], []]\n    ys = [[], [], [], [], []]\n    func_ = False","metadata":{"papermill":{"duration":110.693874,"end_time":"2024-05-13T19:20:58.553785","exception":false,"start_time":"2024-05-13T19:19:07.859911","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T20:22:52.996303Z","iopub.execute_input":"2024-07-01T20:22:52.996705Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Display Threshold Trials","metadata":{"papermill":{"duration":0.010983,"end_time":"2024-05-13T19:20:58.575958","exception":false,"start_time":"2024-05-13T19:20:58.564975","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nif func_:\n\n    diff = 0.5\n    for k in range(5):\n        plt.figure(figsize=(10,3))\n        plt.scatter(xs[k],ys[k],s=3)\n        m = k+1.5\n        plt.xlim((m-diff,m+diff))\n        i = np.where( (np.array(xs[k])>m-diff)&(np.array(xs[k])<m+diff) )[0]\n        mn = np.min(np.array(ys[k])[i])\n        mx = np.max(np.array(ys[k])[i])\n        plt.ylim((mn,mx))\n\n        plt.plot([thresholds[k],thresholds[k]],[mn,mx],'--',\n                 color='black', label='optimal threshold')\n\n        plt.title(f\"Threshold between {k+1} and {k+2}\",size=16)\n        plt.xlabel('Threshold value',size=10)\n        plt.ylabel('QWK CV score',size=10)\n        plt.legend()\n        plt.show()","metadata":{"papermill":{"duration":1.522249,"end_time":"2024-05-13T19:21:00.108914","exception":false,"start_time":"2024-05-13T19:20:58.586665","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T20:14:06.820166Z","iopub.execute_input":"2024-07-01T20:14:06.820520Z","iopub.status.idle":"2024-07-01T20:14:08.428402Z","shell.execute_reply.started":"2024-07-01T20:14:06.820493Z","shell.execute_reply":"2024-07-01T20:14:08.427338Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Submission CSV","metadata":{"papermill":{"duration":0.013187,"end_time":"2024-05-13T19:21:00.135815","exception":false,"start_time":"2024-05-13T19:21:00.122628","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_preds = test_preds\nprint('Test preds shape:', test_preds.shape )\nprint('First 3 test preds:',test_preds[:3] )","metadata":{"papermill":{"duration":0.021494,"end_time":"2024-05-13T19:21:00.170466","exception":false,"start_time":"2024-05-13T19:21:00.148972","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T20:14:08.429840Z","iopub.execute_input":"2024-07-01T20:14:08.430187Z","iopub.status.idle":"2024-07-01T20:14:08.436547Z","shell.execute_reply.started":"2024-07-01T20:14:08.430153Z","shell.execute_reply":"2024-07-01T20:14:08.435348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds_pp = pd.cut(test_preds, [-np.inf] + thresholds + [np.inf], \n                       labels=[1,2,3,4,5,6]).astype('int32')\nprint('First 3 test preds after PP:',test_preds_pp[:3] )","metadata":{"papermill":{"duration":0.023285,"end_time":"2024-05-13T19:21:00.207086","exception":false,"start_time":"2024-05-13T19:21:00.183801","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T20:14:08.438093Z","iopub.execute_input":"2024-07-01T20:14:08.439326Z","iopub.status.idle":"2024-07-01T20:14:08.449659Z","shell.execute_reply.started":"2024-07-01T20:14:08.439286Z","shell.execute_reply":"2024-07-01T20:14:08.448668Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\nsub[\"score\"] = test_preds_pp\nsub.score = sub.score.astype('int32')\nsub.to_csv(\"submission.csv\",index=False)\nprint(\"Submission shape\", sub.shape )\nsub.head()","metadata":{"papermill":{"duration":0.035365,"end_time":"2024-05-13T19:21:00.257232","exception":false,"start_time":"2024-05-13T19:21:00.221867","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T20:14:08.450757Z","iopub.execute_input":"2024-07-01T20:14:08.451061Z","iopub.status.idle":"2024-07-01T20:14:08.485748Z","shell.execute_reply.started":"2024-07-01T20:14:08.451037Z","shell.execute_reply":"2024-07-01T20:14:08.484755Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null}]}