{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M1 Asking LLMs for Structured Data\n",
        "\n",
        "[Better Data Extraction Using Pydantic and OpenAI Function Calls](https://wandb.ai/jxnlco/function-calls/reports/Better-Data-Extraction-Using-Pydantic-and-OpenAI-Function-Calls--Vmlldzo0ODU4OTA3?utm_source=course&utm_medium=course&utm_campaign=jason)\n",
        "\n",
        "\n",
        "[1-Issues with JSON and dictionaries notebook](https://github.com/wandb/edu/blob/main/llm-structured-extraction/1.introduction.ipynb)\n",
        "  *   Validar os dicionarios para saber o tipo dos dados das keys e valores\n",
        "  *   Solicitar json corretamente no prompt das LLMs (Function calling)\n",
        "      * passar um objeto com o seu schema (PersonBirthday(BaseModel))\n",
        "      * criar schema do objeto usando PersonBirthday.model_json_schema()\n",
        "      * [Building a Virtual Assistant with Google Gemini Function Calling](https://wandb.ai/byyoung3/ml-news/reports/Building-a-Virtual-Assistant-with-Google-Gemini-Function-Calling--Vmlldzo2MzE1NTY1?utm_source=course&utm_medium=course&utm_campaign=jason)\n",
        "      * [Using LLMs to Extract Structured Data: OpenAI Function Calling in Action](https://wandb.ai/darek/llmapps/reports/Using-LLMs-to-Extract-Structured-Data-OpenAI-Function-Calling-in-Action--Vmlldzo0Nzc0MzQ3?utm_source=course&utm_medium=course&utm_campaign=jason)\n",
        "\n",
        "### The core idea around the Instructor:\n",
        "* Using function calling allows us use a llm that is finetuned to use json_schema and output json.\n",
        "* Pydantic can be used to define the object, schema, and validation in one single class, allow us to encapsulate everything neatly.\n",
        "\n",
        "Why Instructor?\n",
        "\n",
        "Libraries like Marvin, Langchain, and Llamaindex all now leverage the Pydantic object in similar ways. The goal is to be as light weight as possible, get you as close as possible to the openai api, and then get out of your way.\n",
        "\n",
        "More importantly, we've also added straight forward validation and reasking to the mix.\n",
        "\n",
        "The goal of instructor is to show you how to think about structured prompting and provide examples and documentation that you can take with you to any framework.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FquZZk0U2HFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exremplo de function calling\n",
        "import datetime\n",
        "\n",
        "\n",
        "class PersonBirthday(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    birthday: datetime.date\n",
        "\n",
        "\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "        \"age\": {\"type\": \"integer\"},\n",
        "        \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},\n",
        "    },\n",
        "    \"required\": [\"name\", \"age\"],\n",
        "    \"type\": \"object\",\n",
        "}\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",\n",
        "        },\n",
        "    ],\n",
        "    functions=[{\"name\": \"Person\", \"parameters\": schema}],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "\n",
        "PersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments)"
      ],
      "metadata": {
        "id": "1aKp7Gwf49IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exemplo usando instructor\n",
        "import instructor\n",
        "import datetime\n",
        "\n",
        "# patch the client to add `response_model` to the `create` method\n",
        "client = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "            Today is {datetime.date.today()}\n",
        "\n",
        "            Extract `Jason Liu is thirty years old his birthday is yesturday`\n",
        "            he lives at 123 Main St, San Francisco, CA\"\"\",\n",
        "        },\n",
        "    ],\n",
        "    response_model=PersonAddress,\n",
        ")\n",
        "resp"
      ],
      "metadata": {
        "id": "TpaqJBJjGAlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting LLMs"
      ],
      "metadata": {
        "id": "RTmhwn4k8VsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## classification\n",
        "For classification, we've found there are generally two methods of modeling:\n",
        "\n",
        "* using Enums\n",
        "* using Literals\n",
        "\n",
        "Use an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.\n",
        "\n",
        "Use literals when you have a small, unchanging set of values that you don't need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values."
      ],
      "metadata": {
        "id": "6sCYRvPb9zzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "from enum import Enum\n",
        "from pydantic import BaseModel, Field\n",
        "from typing_extensions import Literal\n",
        "\n",
        "\n",
        "client = instructor.patch(OpenAI())\n",
        "\n",
        "\n",
        "# Tip: Do not use auto() as they cast to 1,2,3,4\n",
        "class House(Enum):\n",
        "    Gryffindor = \"gryffindor\"\n",
        "    Hufflepuff = \"hufflepuff\"\n",
        "    Ravenclaw = \"ravenclaw\"\n",
        "    Slytherin = \"slytherin\"\n",
        "\n",
        "\n",
        "class Character(BaseModel):\n",
        "    age: int\n",
        "    name: str\n",
        "    house: House\n",
        "\n",
        "    def say_hello(self):\n",
        "        print(\n",
        "            f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"\n",
        "        )\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n",
        "    response_model=Character,\n",
        ")\n",
        "resp.model_dump()"
      ],
      "metadata": {
        "id": "xylMoUDa8Zqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Character(BaseModel):\n",
        "    age: int\n",
        "    name: str\n",
        "    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n",
        "    response_model=Character,\n",
        ")\n",
        "resp.model_dump()"
      ],
      "metadata": {
        "id": "pazT3g169v9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arbitrary properties\n",
        "Often times there are long properties that you might want to extract from data that we can not specify in advance. We can get around this by defining an arbitrary key value store like so:"
      ],
      "metadata": {
        "id": "hS9ZlpGf914_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class Property(BaseModel):\n",
        "    key: str = Field(description=\"Must be snake case\")\n",
        "    value: str\n",
        "\n",
        "\n",
        "class Character(BaseModel):\n",
        "    age: int\n",
        "    name: str\n",
        "    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n",
        "    properties: List[Property]\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n",
        "    response_model=Character,\n",
        ")\n",
        "resp.model_dump()"
      ],
      "metadata": {
        "id": "xhxt1TPO97Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limiting the length of lists\n",
        "In later chapters we'll talk about how to use validators to assert the length of lists but we can also use prompting tricks to enumerate values. Here we'll define an index to count the properties.\n",
        "\n",
        "In the following example instead of extraction we're going to work on generation instead."
      ],
      "metadata": {
        "id": "DgBVkiAb-nx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Property(BaseModel):\n",
        "    index: str = Field(..., description=\"Monotonically increasing ID\")\n",
        "    key: str = Field(description=\"Must be snake case\")\n",
        "    value: str\n",
        "\n",
        "\n",
        "class Character(BaseModel):\n",
        "    age: int\n",
        "    name: str\n",
        "    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n",
        "    properties: List[Property] = Field(\n",
        "        ...,\n",
        "        description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",\n",
        "    )\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n",
        "    response_model=Character,\n",
        ")\n",
        "resp.model_dump()"
      ],
      "metadata": {
        "id": "2qGHaGMx-q0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Multiple Entities\n",
        "Now that we see a single entity with many properties we can continue to nest them into many users! If we add the Iterable type to the User type we can define multiple users in a single prompt, now instead of extracting one user we can extract many users. But only after the completion of the prompt."
      ],
      "metadata": {
        "id": "FfH58z9M-0HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable\n",
        "\n",
        "\n",
        "class Character(BaseModel):\n",
        "    age: int\n",
        "    name: str\n",
        "    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n",
        "    response_model=Iterable[Character],\n",
        ")\n",
        "\n",
        "for character in resp:\n",
        "    print(character)"
      ],
      "metadata": {
        "id": "CK7pETeD_FTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Relationships\n",
        "Now only can we define lists of users, with list of properties one of the more interesting things I've learned about prompting is that we can also easily define lists of references.\n",
        "\n",
        "With the tools we've discussed, we can find numerous real-world applications in production settings. These include extracting action items from transcripts, generating fake data, filling out forms, and creating objects that correspond to generative UI. These simple tricks will be highly useful."
      ],
      "metadata": {
        "id": "j1ja3bKHACZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Character(BaseModel):\n",
        "    id: int\n",
        "    name: str\n",
        "    friends_array: List[int] = Field(description=\"Relationships to their friends using the id\")\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],\n",
        "    stream=True,\n",
        "    response_model=Iterable[Character],\n",
        ")\n",
        "\n",
        "for character in resp:\n",
        "    print(character)"
      ],
      "metadata": {
        "id": "xx0gS91aAGmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Data\n",
        "The Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors.\n",
        "\n",
        "This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations."
      ],
      "metadata": {
        "id": "Y7QDi0SQAYJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "class Character(BaseModel):\n",
        "    age: int\n",
        "    name: str\n",
        "\n",
        "class MaybeCharacter(BaseModel):\n",
        "    result: Optional[Character] = Field(default=None)\n",
        "    error: bool = Field(default=False)\n",
        "    message: Optional[str]\n",
        "\n",
        "@weave.op()\n",
        "def extract(content: str) -> MaybeCharacter:\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        response_model=MaybeCharacter,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "extract(\"Harry Potter\")\n",
        "\n",
        "user = extract(\"404 Error\")\n",
        "\n",
        "if user.error:\n",
        "    raise ValueError(user.message)"
      ],
      "metadata": {
        "id": "y5PZvkW_AeZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Structured Output to RAG applications"
      ],
      "metadata": {
        "id": "ag3M68yLXE_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1) Improving Extractions\n",
        "A common method of using structured output is to extract information from a document and use it to answer a question. Directly, we can be creative in how we extract, summarize and generate potential questions in order for our embeddings to do better.\n",
        "\n",
        "For example, instead of using just a text chunk we could try to:\n",
        "\n",
        "* extract key words and themes\n",
        "* extract hypothetical questions\n",
        "* generate a summary of the text\n",
        "\n",
        "In the example below, we use the instructor library to extract the key words and themes from a text chunk and use them to answer a question."
      ],
      "metadata": {
        "id": "HSLoaZj0ZgC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Extraction(BaseModel):\n",
        "    topic: str\n",
        "    summary: str\n",
        "    hypothetical_questions: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Hypothetical questions that this document could answer\",\n",
        "    )\n",
        "    keywords: List[str] = Field(\n",
        "        default_factory=list, description=\"Keywords that this document is about\"\n",
        "    )\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "text_chunk = \"\"\"\n",
        "## Simple RAG\n",
        "\n",
        "**What is it?**\n",
        "\n",
        "The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n",
        "\n",
        "**What are the limitations?**\n",
        "\n",
        "- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n",
        "    - Query: \"Tell me about climate change effects on marine life.\"\n",
        "    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n",
        "- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n",
        "    - Query: \"Latest research in quantum computing.\"\n",
        "    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n",
        "- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n",
        "    - Query: \"what problems did we fix last week\"\n",
        "    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n",
        "- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n",
        "    - Query: \"Tips for first-time Europe travelers.\"\n",
        "    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n",
        "\"\"\"\n",
        "\n",
        "extractions = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    stream=True,\n",
        "    response_model=Iterable[Extraction],\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": text_chunk},\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "for extraction in extractions:\n",
        "    pprint(extraction.model_dump())"
      ],
      "metadata": {
        "id": "FDC_xYhNaMkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2) Understanding 'recent queries' to add temporal context\n",
        "One common application of using structured outputs for query understanding is to identify the intent of a user's query. In this example we're going to use a simple schema to seperately process the query to add additional temporal context.\n",
        "\n",
        "In this example, DateRange and Query are Pydantic models that structure the user's query with a date range and a list of domains to search within.\n",
        "\n",
        "These models restructure the user's query by including a rewritten query, a range of published dates, and a list of domains to search in.\n",
        "\n",
        "Using the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend."
      ],
      "metadata": {
        "id": "RJzOTxkuasrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "\n",
        "class DateRange(BaseModel):\n",
        "    start: date\n",
        "    end: date\n",
        "\n",
        "\n",
        "class Query(BaseModel):\n",
        "    rewritten_query: str\n",
        "    published_daterange: DateRange\n",
        "\n",
        "def expand_query(q) -> Query:\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        response_model=Query,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "query = expand_query(\"What are some recent developments in AI?\")\n",
        "query"
      ],
      "metadata": {
        "id": "Embc8Eb2auhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DateRange(BaseModel):\n",
        "    chain_of_thought: str = Field(\n",
        "        description=\"Think step by step to plan what is the best time range to search in\"\n",
        "    )\n",
        "    start: date\n",
        "    end: date\n",
        "\n",
        "\n",
        "class Query(BaseModel):\n",
        "    rewritten_query: str = Field(\n",
        "        description=\"Rewrite the query to make it more specific\"\n",
        "    )\n",
        "    published_daterange: DateRange = Field(\n",
        "        description=\"Effective date range to search in\"\n",
        "    )\n",
        "\n",
        "\n",
        "def expand_query(q) -> Query:\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        response_model=Query,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "expand_query(\"What are some recent developments in AI?\")"
      ],
      "metadata": {
        "id": "D-efaJBCbFuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Weights & Biases to track experiments\n",
        "While running a function like this in production is quite simple, a lot of time will be spent on iterating and improving the model. To do this, we can use Weights & Biases to track our experiments.\n",
        "\n",
        "In order to do so we need to manage a few things:\n",
        "\n",
        "* Save input and output pairs for later\n",
        "* Save the JSON schema for the response_model\n",
        "* Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.\n",
        "\n",
        "This is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We can use the wandb library to track our experiments and save the results to a dashboard."
      ],
      "metadata": {
        "id": "RMjwFedUbcgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import instructor\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "from helpers import dicts_to_df\n",
        "from datetime import date\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class DateRange(BaseModel):\n",
        "    chain_of_thought: str = Field(\n",
        "        description=\"Think step by step to plan what is the best time range to search in\"\n",
        "    )\n",
        "    start: date\n",
        "    end: date\n",
        "\n",
        "\n",
        "class Query(BaseModel):\n",
        "    rewritten_query: str = Field(\n",
        "        description=\"Rewrite the query to make it more specific\"\n",
        "    )\n",
        "    published_daterange: DateRange = Field(\n",
        "        description=\"Effective date range to search in\"\n",
        "    )\n",
        "\n",
        "    def report(self):\n",
        "        dct = self.model_dump()\n",
        "        dct[\"usage\"] = self._raw_response.usage.model_dump()\n",
        "        return dct\n",
        "\n",
        "\n",
        "\n",
        "# We'll use a different client for async calls\n",
        "# To highlight the difference and how we can use both\n",
        "aclient = instructor.patch(AsyncOpenAI())\n",
        "\n",
        "\n",
        "async def expand_query(\n",
        "    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n",
        ") -> Query:\n",
        "    return await aclient.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=temp,\n",
        "        response_model=Query,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
        "        ],\n",
        "    )"
      ],
      "metadata": {
        "id": "QuSFhmacbswp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "model = \"gpt-4-1106-preview\"\n",
        "temp = 0\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"llmeng-1-nb3\",\n",
        "    config={\"model\": model, \"temp\": temp},\n",
        ")\n",
        "\n",
        "test_queries = [\n",
        "    \"latest developments in artificial intelligence last 3 weeks\",\n",
        "    \"renewable energy trends past month\",\n",
        "    \"quantum computing advancements last 2 months\",\n",
        "    \"biotechnology updates last 10 days\",\n",
        "]\n",
        "start = time.perf_counter()\n",
        "queries = await asyncio.gather(\n",
        "    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n",
        ")\n",
        "duration = time.perf_counter() - start\n",
        "\n",
        "with open(\"schema.json\", \"w+\") as f:\n",
        "    schema = Query.model_json_schema()\n",
        "    json.dump(schema, f, indent=2)\n",
        "\n",
        "with open(\"results.jsonlines\", \"w+\") as f:\n",
        "    for query in queries:\n",
        "        f.write(query.model_dump_json() + \"\\n\")\n",
        "\n",
        "df = dicts_to_df([q.report() for q in queries])\n",
        "df[\"input\"] = test_queries\n",
        "df.to_csv(\"results.csv\")\n",
        "\n",
        "\n",
        "run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\n",
        "\n",
        "run.log(\n",
        "    {\n",
        "        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n",
        "        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n",
        "        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n",
        "        \"duration (s)\": duration,\n",
        "        \"average duration (s)\": duration / len(queries),\n",
        "        \"n_queries\": len(queries),\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "run.log(\n",
        "    {\n",
        "        \"results\": wandb.Table(dataframe=df),\n",
        "    }\n",
        ")\n",
        "\n",
        "files = wandb.Artifact(\"data\", type=\"dataset\")\n",
        "\n",
        "files.add_file(\"schema.json\")\n",
        "files.add_file(\"results.jsonlines\")\n",
        "files.add_file(\"results.csv\")\n",
        "\n",
        "\n",
        "run.log_artifact(files)\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "pv5doOsAb9jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3) Personal Assistants, parallel processing\n",
        "A personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.\n",
        "\n",
        "For instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.\n",
        "\n",
        "It's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.\n",
        "\n"
      ],
      "metadata": {
        "id": "NsOwfBAScCPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "\n",
        "class SearchClient(BaseModel):\n",
        "    query: str = Field(description=\"The search query that will go into the search bar\")\n",
        "    keywords: List[str]\n",
        "    email: str\n",
        "    source: Literal[\"gmail\", \"calendar\"]\n",
        "    date_range: DateRange\n",
        "\n",
        "\n",
        "class Retrival(BaseModel):\n",
        "    queries: List[SearchClient]"
      ],
      "metadata": {
        "id": "uuJ9sz2Fcf2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can utilize this with a straightforward query such as \"What do I have today?\".\n",
        "\n",
        "The system will attempt to asynchronously dispatch the query to the appropriate backend.\n",
        "\n",
        "However, it's still crucial to remember that effectively prompting the language model is still a key aspect."
      ],
      "metadata": {
        "id": "QupVyzEPcp3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrival = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    response_model=Retrival,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"You are Jason's personal assistant.\n",
        "                He has two emails jason@work.com jason@personal.com\n",
        "                Today is {date.today()}\"\"\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\n",
        "    ],\n",
        ")\n",
        "print(retrival.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "9UnnrQA-cpb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways."
      ],
      "metadata": {
        "id": "2QFeSICgcuRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrival = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    response_model=Retrival,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"You are Jason's personal assistant.\n",
        "                He has two emails jason@work.com jason@personal.com\n",
        "                Today is {date.today()}\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(retrival.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "Rvon_ZsEcxnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 4) Decomposing questions\n",
        "Lastly, a lightly more complex example of a problem that can be solved with structured output is decomposing questions. Where you ultimately want to decompose a question into a series of sub-questions that can be answered by a search backend. For example\n",
        "\n",
        "\"Whats the difference in populations of jason's home country and canada?\"\n",
        "\n",
        "You'd ultimately need to know a few things\n",
        "\n",
        "* Jason's home country\n",
        "* The population of Jason's home country\n",
        "* The population of Canada\n",
        "* The difference between the two\n",
        "\n",
        "This would not be done correctly as a single query, nor would it be done in parallel, however there are some opportunities try to be parallel since not all of the sub-questions are dependent on each other."
      ],
      "metadata": {
        "id": "EoTDHcuHc0VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Question(BaseModel):\n",
        "    id: int = Field(..., description=\"A unique identifier for the question\")\n",
        "    query: str = Field(..., description=\"The question decomposited as much as possible\")\n",
        "    subquestions: List[int] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"The subquestions that this question is composed of\",\n",
        "    )\n",
        "\n",
        "\n",
        "class QueryPlan(BaseModel):\n",
        "    root_question: str = Field(..., description=\"The root question that the user asked\")\n",
        "    plan: List[Question] = Field(\n",
        "        ..., description=\"The plan to answer the root question and its subquestions\"\n",
        "    )\n",
        "\n",
        "\n",
        "retrival = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    response_model=QueryPlan,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the difference between the population of jason's home country and canada?\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(retrival.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "ADlZk7ric-kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Validators and controlling responses"
      ],
      "metadata": {
        "id": "1H3GHCo9b6QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Previously we went over how to use structured Extraction to Query and Plan a search request\n",
        "\n",
        "In this section we'll aim to\n",
        "\n",
        "Expand on how Pydantic's validation features work\n",
        "Apply them to generate better responses by using feedback and validation.\n",
        "Pydantic offers a customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.\n",
        "\n",
        "Validators will enable us to control outputs by defining a function like so:\n",
        "\n",
        "def validation_function(value):\n",
        "    if condition(value):\n",
        "        raise ValueError(\"Value is not valid\")\n",
        "    return mutation(value)\n",
        "Before we get started lets go over the general shape of a validator:"
      ],
      "metadata": {
        "id": "BC5c78fgb_Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Validator Functions\n",
        "from typing_extensions import Annotated\n",
        "from pydantic import BaseModel, AfterValidator, WithJsonSchema\n",
        "\n",
        "\n",
        "def name_must_contain_space(v: str) -> str:\n",
        "    if \" \" not in v:\n",
        "        raise ValueError(\"Name must contain a space.\")\n",
        "    return v\n",
        "\n",
        "def uppercase_name(v: str) -> str:\n",
        "    return v.upper()\n",
        "\n",
        "FullName = Annotated[\n",
        "    str,\n",
        "    AfterValidator(name_must_contain_space),\n",
        "    AfterValidator(uppercase_name),\n",
        "    WithJsonSchema(\n",
        "        {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The user's full name\",\n",
        "        }\n",
        "    )]\n",
        "\n",
        "class UserDetail(BaseModel):\n",
        "    age: int\n",
        "    name: FullName\n",
        "UserDetail(age=30, name=\"Jason Liu\")\n",
        "UserDetail.model_json_schema()\n",
        "try:\n",
        "    person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"})\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\n",
        "# Using Field\n",
        "We can also use the Field class to define validators. This is useful when we want to define a validator for a field that is primitive, like a string or integer which supports a limited number of validators.\n",
        "\n",
        "from pydantic import Field\n",
        "\n",
        "\n",
        "Age = Annotated[int, Field(gt=0)]\n",
        "\n",
        "class UserDetail(BaseModel):\n",
        "    age: Age\n",
        "    name: FullName\n",
        "\n",
        "try:\n",
        "    person = UserDetail(age=-10, name=\"Jason\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "SFjV8qKeb9zy",
        "outputId": "ca7df96d-0488-48d5-fb37-844608156f71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-585039876673>, line 36)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-585039876673>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    Using Field\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Providing Context\n",
        "from pydantic import ValidationInfo\n",
        "\n",
        "def message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -> str:\n",
        "    blacklist = info.context.get(\"blacklist\", [])\n",
        "    for word in blacklist:\n",
        "        assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"\n",
        "    return v\n",
        "\n",
        "ModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n",
        "\n",
        "class Response(BaseModel):\n",
        "    message: ModeratedStr\n",
        "\n",
        "\n",
        "try:\n",
        "    Response.model_validate(\n",
        "        {\"message\": \"I will hurt them.\"},\n",
        "        context={\n",
        "            \"blacklist\": {\n",
        "                \"rob\",\n",
        "                \"steal\",\n",
        "                \"kill\",\n",
        "                \"attack\",\n",
        "            }\n",
        "        },\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "#Using OpenAI Moderation\n",
        "#To enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.\n",
        "\n",
        "#With the instructor library, this is just one function edit away:\n",
        "\n",
        "from typing import Annotated\n",
        "from pydantic import AfterValidator\n",
        "from instructor import openai_moderation\n",
        "\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "client = instructor.patch(OpenAI())\n",
        "\n",
        "# This uses Annotated which is a new feature in Python 3.9\n",
        "# To define custom metadata for a type hint.\n",
        "ModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]\n",
        "\n",
        "\n",
        "class Response(BaseModel):\n",
        "    message: ModeratedStr\n",
        "\n",
        "\n",
        "try:\n",
        "    Response(message=\"I want to make them suffer the consequences\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\n",
        "\n",
        "# General Validator\n",
        "from instructor import llm_validator\n",
        "\n",
        "HealthTopicStr = Annotated[\n",
        "    str,\n",
        "    AfterValidator(\n",
        "        llm_validator(\n",
        "            \"don't talk about any other topic except health best practices and topics\",\n",
        "            client=client,\n",
        "        )\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "class AssistantMessage(BaseModel):\n",
        "    message: HealthTopicStr\n",
        "\n",
        "\n",
        "AssistantMessage(\n",
        "    message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Avoiding hallucination with citations\n",
        "# When incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:\n",
        "\n",
        "from pydantic import ValidationInfo\n",
        "\n",
        "def citation_exists(v: str, info: ValidationInfo):\n",
        "    context = info.context\n",
        "    if context:\n",
        "        context = context.get(\"text_chunk\")\n",
        "        if v not in context:\n",
        "            raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")\n",
        "    return v\n",
        "\n",
        "Citation = Annotated[\n",
        "    str,\n",
        "    AfterValidator(citation_exists),\n",
        "    WithJsonSchema({\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"For every answer provide an exact substring match to the context\"\n",
        "    })\n",
        "]\n",
        "\n",
        "\n",
        "class AnswerWithCitation(BaseModel):\n",
        "    answer: str\n",
        "    citation: Citation\n",
        "\n",
        "try:\n",
        "    AnswerWithCitation.model_validate(\n",
        "        {\n",
        "            \"answer\": \"Jason is cool\",\n",
        "            \"citation\": \"Jason is a cool person\",\n",
        "        },\n",
        "        context={\"text_chunk\": \"Jason is just a normal guy\"},\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\n",
        "\n",
        "# Here we assume that there is a \"text_chunk\" field that contains the text that the model is supposed to use as context. We then use the field_validator decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a ValueError with a message that will be returned to the user.\n",
        "\n",
        "# If we want to pass in the context through the chat.completions.create`` endpoint, we can use the validation_context` parameter\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    response_model=AnswerWithCitation,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": f\"Answer the question `{q}` using the text chunk\\n`{text_chunk}`\"},\n",
        "    ],\n",
        "    validation_context={\"text_chunk\": text_chunk},\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# In practice there are many ways to implement this: we could use a regex to check if the citation is included in the text chunk, or we could use a more sophisticated approach like a semantic similarity check. The important thing is that we have a way to validate that the model is using the provided context accurately.\n",
        "\n",
        "# Reasking with validators\n",
        "# For most of these examples all we've done we've mostly only defined the validation logic. Which can be seperate from generation, however when we are given validation errors, we shouldn't end there! Instead instructor allows us to collect all the validation errors and reask the llm to rewrite their answer.\n",
        "\n",
        "# Lets try to use an extreme example to illustrate this point:\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "question = \"What is the meaning of life?\"\n",
        "context = (\n",
        "    \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n",
        ")\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    response_model=QuestionAnswer,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(resp.model_dump_json(indent=2))\n",
        "from instructor import llm_validator\n",
        "\n",
        "\n",
        "NotEvilAnswer = Annotated[\n",
        "    str,\n",
        "    AfterValidator(\n",
        "        llm_validator(\"don't say objectionable things\", client=client)\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    question: str\n",
        "    answer: NotEvilAnswer\n",
        "\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    response_model=QuestionAnswer,\n",
        "    max_retries=2,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(resp.model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "qxO1sUWnd_vY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}