{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXemx6J5HQQ3"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/llm-apps-course/notebooks/02.%20Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{llmapps-generation} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VIdxoITHQQ7"
      },
      "source": [
        "# Generation\n",
        "<!--- @wandbcode{llmapps-generation} -->\n",
        "\n",
        "In this notebook we will dive deeper on prompting the model by passing a better context by using available data from users questions and using the documentation files to generate better answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rea0eHhqHQQ8"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkqtg4KGHQQ9",
        "outputId": "2eecd345-5c6f-47d2-aee4-694ed27951a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m1.1/1.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -Uqqq rich openai==0.27.2 tiktoken wandb tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N-X1cpbmHQQ-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from getpass import getpass\n",
        "\n",
        "from rich.markdown import Markdown\n",
        "import pandas as pd\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential, # for exponential backoff\n",
        ")\n",
        "import wandb\n",
        "from wandb.integration.openai import autolog"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Recuperar a chave API do segredo salvo\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Definir a chave API como uma variável de ambiente\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "openai.api_key = openai_api_key"
      ],
      "metadata": {
        "id": "VLPrBIuFHdoa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIyXldRBHQQ-",
        "outputId": "5be35787-7ead-43c2-873a-62118289ff16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-21 00:28:49--  https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/examples.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40595 (40K) [text/plain]\n",
            "Saving to: ‘examples.txt’\n",
            "\n",
            "examples.txt        100%[===================>]  39.64K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-11-21 00:28:49 (5.16 MB/s) - ‘examples.txt’ saved [40595/40595]\n",
            "\n",
            "--2024-11-21 00:28:49--  https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/prompt_template.txt\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1054 (1.0K) [text/plain]\n",
            "Saving to: ‘prompt_template.txt’\n",
            "\n",
            "prompt_template.txt 100%[===================>]   1.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-21 00:28:49 (79.4 MB/s) - ‘prompt_template.txt’ saved [1054/1054]\n",
            "\n",
            "--2024-11-21 00:28:49--  https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/system_template.txt\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 469 [text/plain]\n",
            "Saving to: ‘system_template.txt’\n",
            "\n",
            "system_template.txt 100%[===================>]     469  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-21 00:28:49 (28.4 MB/s) - ‘system_template.txt’ saved [469/469]\n",
            "\n",
            "FINISHED --2024-11-21 00:28:49--\n",
            "Total wall clock time: 0.6s\n",
            "Downloaded: 3 files, 41K in 0.008s (5.33 MB/s)\n"
          ]
        }
      ],
      "source": [
        "# Download files on colab\n",
        "if not Path(\"examples.txt\").exists():\n",
        "    !wget https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/{examples,prompt_template,system_template}.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kGm485bHQQ-"
      },
      "source": [
        "You will need an OpenAI API key to run this notebook. You can get one [here](https://platform.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRXvibXJHQQ_",
        "outputId": "f940b344-8c61-4022-f43d-6edfdf5e4c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key configured\n"
          ]
        }
      ],
      "source": [
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
        "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIDrJthVHQQ_"
      },
      "source": [
        "Let's enable W&B autologging to track our experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "KuVfHnSUHQQ_",
        "outputId": "460fe9c1-2eeb-41d6-e62f-4453a079b88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241121_002930-7cnlm8cb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rafaelsudbrackzimmermann/llmapps/runs/7cnlm8cb' target=\"_blank\">jumping-plant-3</a></strong> to <a href='https://wandb.ai/rafaelsudbrackzimmermann/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rafaelsudbrackzimmermann/llmapps' target=\"_blank\">https://wandb.ai/rafaelsudbrackzimmermann/llmapps</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rafaelsudbrackzimmermann/llmapps/runs/7cnlm8cb' target=\"_blank\">https://wandb.ai/rafaelsudbrackzimmermann/llmapps/runs/7cnlm8cb</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# start logging to W&B\n",
        "autolog({\"project\":\"llmapps\", \"job_type\": \"generation\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8amdYTnLHQRA"
      },
      "source": [
        "# Generating synthetic support questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrc2Y0P5HQRA"
      },
      "source": [
        "We will add a retry behavior in case we hit the API rate limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dnuu8H2tHQRA"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def completion_with_backoff(**kwargs):\n",
        "    return openai.ChatCompletion.create(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "42eBNoNyHQRA"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "# MODEL_NAME = \"gpt-4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "ypNLThb4HQRA",
        "outputId": "30e9650d-e4fe-4e06-f1fb-b7698520f9d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "How do I track individual expenses for different projects within my organization using the W&B software?           \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How do I track individual expenses for different projects within my organization using the W&amp;B software?           \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sure! Here is a support question that a user could ask:                                                            \n",
              "\n",
              "\"Hello, I am having trouble adding a new employee to my team in the W&B system. Can you please guide me on how to  \n",
              "do this correctly?\"                                                                                                \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sure! Here is a support question that a user could ask:                                                            \n",
              "\n",
              "\"Hello, I am having trouble adding a new employee to my team in the W&amp;B system. Can you please guide me on how to  \n",
              "do this correctly?\"                                                                                                \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sure, here's a support question a user might ask: \"I am having trouble logging into my account despite entering the\n",
              "correct username and password. Can you help me troubleshoot this issue?\"                                           \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sure, here's a support question a user might ask: \"I am having trouble logging into my account despite entering the\n",
              "correct username and password. Can you help me troubleshoot this issue?\"                                           \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Can you provide guidance on how to create custom reports in the W&B platform?\"                                    \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"Can you provide guidance on how to create custom reports in the W&amp;B platform?\"                                    \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Can you provide guidance on how to properly track and manage our company's expenses using the W&B platform?\"      \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"Can you provide guidance on how to properly track and manage our company's expenses using the W&amp;B platform?\"      \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful assistant.\"\n",
        "user_prompt = \"Generate a support question from a W&B user\"\n",
        "\n",
        "def generate_and_print(system_prompt, user_prompt, n=5):\n",
        "    messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    responses = completion_with_backoff(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        n = n,\n",
        "        )\n",
        "    for response in responses.choices:\n",
        "        generation = response.message.content\n",
        "        display(Markdown(generation))\n",
        "\n",
        "generate_and_print(system_prompt, user_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBXRBg-THQRA"
      },
      "source": [
        "# Few Shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_4y5Us7HQRA"
      },
      "source": [
        "Let's read some user submitted queries from the file `examples.txt`. This file contains multiline questions separated by tabs (`\\t`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Kh4zY31eHQRB"
      },
      "outputs": [],
      "source": [
        "# Test if examples.txt is present, download if not\n",
        "if not Path(\"examples.txt\").exists():\n",
        "    !wget https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/examples.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "fv0xg7XgHQRB",
        "outputId": "93933507-11de-48c1-d116-cb4213268947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'We have 228 real queries:'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sample one: \"my logging doesn't seem to include errors when the training crashes, how do I change the logging level\n",
              "for wandb logging?\"                                                                                                \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample one: \"my logging doesn't seem to include errors when the training crashes, how do I change the logging level\n",
              "for wandb logging?\"                                                                                                \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "delimiter = \"\\t\" # tab separated queries\n",
        "with open(\"examples.txt\", \"r\") as file:\n",
        "    data = file.read()\n",
        "    real_queries = data.split(delimiter)\n",
        "\n",
        "pprint(f\"We have {len(real_queries)} real queries:\")\n",
        "Markdown(f\"Sample one: \\n\\\"{random.choice(real_queries)}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGgGAiZ3HQRB"
      },
      "source": [
        "We can now use those real user questions to guide our model to produce synthetic questions like those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "xlE_4zHJHQRB",
        "outputId": "bc641fa7-c835-4c28-8ce4-30269c68d09d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generate a support question from a W&B user Below you will find a few examples of real user queries: Why do I get  \n",
              "this error anytime I run wandb.init()? \u001b[1;36;40mThe wandb service process exited with 1. Ensure that `sys.executable` is a \u001b[0m \n",
              "\u001b[1;36;40mvalid python interpreter. You can override it with the `_executable` setting or with the `WANDB__EXECUTABLE` \u001b[0m      \n",
              "\u001b[1;36;40menvironment variable.\u001b[0m I have an config file apart from sweep_config file so when I try to run sweep agent, I am    \n",
              "getting UnboundLocalError(\"local variable 'config' referenced before assignment\"). How can I log and visualize time\n",
              "series data in WandB? Let's start!                                                                                 \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user Below you will find a few examples of real user queries: Why do I get  \n",
              "this error anytime I run wandb.init()? <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">The wandb service process exited with 1. Ensure that `sys.executable` is a </span> \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">valid python interpreter. You can override it with the `_executable` setting or with the `WANDB__EXECUTABLE` </span>      \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">environment variable.</span> I have an config file apart from sweep_config file so when I try to run sweep agent, I am    \n",
              "getting UnboundLocalError(\"local variable 'config' referenced before assignment\"). How can I log and visualize time\n",
              "series data in WandB? Let's start!                                                                                 \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def generate_few_shot_prompt(queries, n=3):\n",
        "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
        "        \"Below you will find a few examples of real user queries:\\n\"\n",
        "    for _ in range(n):\n",
        "        prompt += random.choice(queries) + \"\\n\"\n",
        "    prompt += \"Let's start!\"\n",
        "    return prompt\n",
        "\n",
        "generation_prompt = generate_few_shot_prompt(real_queries)\n",
        "Markdown(generation_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3QeXpO8HQRB"
      },
      "source": [
        "OpenAI `Chat` models are really good at following instructions with a few examples. Let's see how it does here. This is going to use some context from the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "1YpUPJZCHQRB",
        "outputId": "0c9fa229-e2ec-4514-8ce6-9fe04f9f758e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "How can I debug a \"TypeError: unhashable type: 'slice'\" error that occurs when trying to use WandB for tracking and\n",
              "visualization?                                                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I debug a \"TypeError: unhashable type: 'slice'\" error that occurs when trying to use WandB for tracking and\n",
              "visualization?                                                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Why am I seeing incomplete or missing metrics data in my WandB dashboard even though my code is logging the metrics\n",
              "properly during training?                                                                                          \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Why am I seeing incomplete or missing metrics data in my WandB dashboard even though my code is logging the metrics\n",
              "properly during training?                                                                                          \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "How can I use the WandB API to upload custom metrics and visualize them in my project dashboard?                   \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I use the WandB API to upload custom metrics and visualize them in my project dashboard?                   \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "How can I view the detailed logs of my previous runs in WandB?                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I view the detailed logs of my previous runs in WandB?                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "How can I resolve the error message \"The wandb service process exited with 1\" when trying to initialize wandb?     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I resolve the error message \"The wandb service process exited with 1\" when trying to initialize wandb?     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_and_print(system_prompt, user_prompt=generation_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSdTGESHQRB"
      },
      "source": [
        "# Add Context & Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_dgthKSHQRB"
      },
      "source": [
        "Let's create a function to find all the markdown files in a directory and return it's content and path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY-Rv4iIHQRC",
        "outputId": "d9f7e3d2-acda-47d5-ca07-5959fc8d6901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'edu'...\n",
            "remote: Enumerating objects: 4766, done.\u001b[K\n",
            "remote: Counting objects: 100% (1649/1649), done.\u001b[K\n",
            "remote: Compressing objects: 100% (725/725), done.\u001b[K\n",
            "remote: Total 4766 (delta 1185), reused 1195 (delta 912), pack-reused 3117 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4766/4766), 41.95 MiB | 26.56 MiB/s, done.\n",
            "Resolving deltas: 100% (2599/2599), done.\n"
          ]
        }
      ],
      "source": [
        "# check if directory exists, if not, create it and download the files, e.g if running in colab\n",
        "if not os.path.exists(\"../docs_sample/\"):\n",
        "  !git clone https://github.com/wandb/edu.git\n",
        "  !cp -r edu/llm-apps-course/docs_sample ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASmPzJhGHQRC",
        "outputId": "375387bc-4a8b-4858-b3c4-45efc6c7b2ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def find_md_files(directory):\n",
        "    \"Find all markdown files in a directory and return their content and path\"\n",
        "    md_files = []\n",
        "    for file in Path(directory).rglob(\"*.md\"):\n",
        "        with open(file, 'r', encoding='utf-8') as md_file:\n",
        "            content = md_file.read()\n",
        "        md_files.append((file.relative_to(directory), content))\n",
        "    return md_files\n",
        "\n",
        "documents = find_md_files('../docs_sample/')\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-DFBYTwHQRC"
      },
      "source": [
        "Let's check if the documents are not too long for our context window. We need to compute the number of tokens in each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8nK7jCNHQRC",
        "outputId": "e718b0f2-7f46-44dc-f776-811b73bedea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[956, 803, 2529, 537, 1206, 4179, 2940, 2093, 365, 1644, 2596]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
        "tokens_per_document = [len(tokenizer.encode(document)) for _, document in documents]\n",
        "pprint(tokens_per_document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sAvcteeHQRC"
      },
      "source": [
        "Some of them are too long - instead of using entire documents, we'll extract a random chunk from them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dDuGAGKwHQRC"
      },
      "outputs": [],
      "source": [
        "# extract a random chunk from a document\n",
        "def extract_random_chunk(document, max_tokens=512):\n",
        "    tokens = tokenizer.encode(document)\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return document\n",
        "    start = random.randint(0, len(tokens) - max_tokens)\n",
        "    end = start + max_tokens\n",
        "    return tokenizer.decode(tokens[start:end])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WCY5sW9HQRC"
      },
      "source": [
        "Now, we will use that extracted chunk to create a question that can be answered by the document. This way we can generate questions that our current documentation is capable of answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZrRuEFydHQRC"
      },
      "outputs": [],
      "source": [
        "def generate_context_prompt(chunk):\n",
        "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
        "        \"The question should be answerable by provided fragment of W&B documentation.\\n\" +\\\n",
        "        \"Below you will find a fragment of W&B documentation:\\n\" +\\\n",
        "        chunk + \"\\n\" +\\\n",
        "        \"Let's start!\"\n",
        "    return prompt\n",
        "\n",
        "chunk = extract_random_chunk(documents[0][1])\n",
        "generation_prompt = generate_context_prompt(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "2fM4wBQwHQRC",
        "outputId": "984f3ad5-4b5f-4989-b42f-be5911dd893c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generate a support question from a W&B user The question should be answerable by provided fragment of W&B          \n",
              "documentation. Below you will find a fragment of W&B documentation: \", project=\"capsules\", tags=[\"debug\"])         \n",
              "\n",
              "...                                                                                                                \n",
              "\n",
              "if current_loss < threshold: run.tags = run.tags + (\"release_candidate\",)                                          \n",
              "\n",
              "\u001b[48;2;39;40;34m                                                                                                                   \n",
              "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThere are also several ways to add tags after runs have been logged to Weights & Biases.\u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m<Tabs\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  defaultValue=\"publicapi\"\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  values={[\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    {label: 'Using the Public API', value: 'publicapi'},\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    {label: 'Project Page', value: 'projectpage'},\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    {label: 'Run Page', value: 'runpage'},\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  ]}>\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  <TabItem value=\"publicapi\">\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAfter a run is created, you can update tags using [our public API](../../../guides/track/public-api-guide.md) lik\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mso:\u001b[0m\u001b[48;2;39;40;34m                                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m```python\u001b[0m\u001b[48;2;39;40;34m                                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun = wandb.Api().run(\"{entity}/{project}/{run-id}\"})\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun.tags.append(\"tag1\")  # you can choose tags based on run data here\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun.update()\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m                                                                                                                   \n",
              "\u001b[0m\n",
              "You can read more about how to use the Public API in the \u001b]8;id=991480;../../../ref/README.md\u001b\\\u001b[4;34mreference documentation\u001b[0m\u001b]8;;\u001b\\ or \u001b]8;id=992654;../../../guides/track/public-api-guide.md\u001b\\\u001b[4;34mguide\u001b[0m\u001b]8;;\u001b\\.                         \n",
              "\n",
              "\n",
              "This method is best suited to tagging large numbers of runs with the same tag or tags.                             \n",
              "\n",
              "In the \u001b]8;id=109167;../pages/project-page.md#search-for-runs\u001b\\\u001b[4;34mruns sidebar\u001b[0m\u001b]8;;\u001b\\ of the \u001b]8;id=502451;../pages/project-page.md\u001b\\\u001b[4;34mProject Page\u001b[0m\u001b]8;;\u001b\\,  click the table icon in the upper-right.  This will expand the sidebar   \n",
              "into the full \u001b]8;id=374198;runs-table.md\u001b\\\u001b[4;34mruns table\u001b[0m\u001b]8;;\u001b\\.                                                                                          \n",
              "\n",
              "Hover over a run in the table to see a checkbox on the left or look in the header row for a checkbox that will     \n",
              "allow you to select all runs.                                                                                      \n",
              "\n",
              "Click the checkbox to enable bulk actions. Select the runs to which you'd like to apply your tag(s).               \n",
              "\n",
              "Click the Tag button above the rows of runs.                                                                       \n",
              "\n",
              "Type a tag you'd like to add and click \"Add\" below the text box to add a new tag.                                  \n",
              "\n",
              "\n",
              "This method is best suited to applying a tag or tags to a single run by hand.                                      \n",
              "\n",
              "In the left sidebar of the \u001b]8;id=995469;../pages/run-page.md\u001b\\\u001b[4;34mRun Page\u001b[0m\u001b]8;;\u001b\\, click the top \u001b]8;id=804700;../pages/run-page.md#overview-tab\u001b\\\u001b[4;34mOverview tab\u001b[0m\u001b]8;;\u001b\\.                                                   \n",
              "\n",
              "Next to \"Tags\" is a gray ➕ button. Click on that plus to add a tag.                                               \n",
              "\n",
              "Type a tag you'd like to add and click \"Add\" below the text box to add a new tag.                                  \n",
              "\n",
              "Let's start!                                                                                                       \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user The question should be answerable by provided fragment of W&amp;B          \n",
              "documentation. Below you will find a fragment of W&amp;B documentation: \", project=\"capsules\", tags=[\"debug\"])         \n",
              "\n",
              "...                                                                                                                \n",
              "\n",
              "if current_loss &lt; threshold: run.tags = run.tags + (\"release_candidate\",)                                          \n",
              "\n",
              "<span style=\"background-color: #272822\">                                                                                                                   \n",
              "                                                                                                                   </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">There are also several ways to add tags after runs have been logged to Weights &amp; Biases.</span><span style=\"background-color: #272822\">                          </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">&lt;Tabs</span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  defaultValue=\"publicapi\"</span><span style=\"background-color: #272822\">                                                                                        </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  values={[</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {label: 'Using the Public API', value: 'publicapi'},</span><span style=\"background-color: #272822\">                                                          </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {label: 'Project Page', value: 'projectpage'},</span><span style=\"background-color: #272822\">                                                                </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {label: 'Run Page', value: 'runpage'},</span><span style=\"background-color: #272822\">                                                                        </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  ]}&gt;</span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  &lt;TabItem value=\"publicapi\"&gt;</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">After a run is created, you can update tags using [our public API](../../../guides/track/public-api-guide.md) lik</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">so:</span><span style=\"background-color: #272822\">                                                                                                               </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">```python</span><span style=\"background-color: #272822\">                                                                                                         </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">run = wandb.Api().run(\"{entity}/{project}/{run-id}\"})</span><span style=\"background-color: #272822\">                                                             </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">run.tags.append(\"tag1\")  # you can choose tags based on run data here</span><span style=\"background-color: #272822\">                                             </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">run.update()</span><span style=\"background-color: #272822\">                                                                                                      </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                                   \n",
              "</span>\n",
              "You can read more about how to use the Public API in the <a href=\"../../../ref/README.md\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">reference documentation</span></a> or <a href=\"../../../guides/track/public-api-guide.md\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">guide</span></a>.                         \n",
              "\n",
              "\n",
              "This method is best suited to tagging large numbers of runs with the same tag or tags.                             \n",
              "\n",
              "In the <a href=\"../pages/project-page.md#search-for-runs\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">runs sidebar</span></a> of the <a href=\"../pages/project-page.md\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Project Page</span></a>,  click the table icon in the upper-right.  This will expand the sidebar   \n",
              "into the full <a href=\"runs-table.md\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">runs table</span></a>.                                                                                          \n",
              "\n",
              "Hover over a run in the table to see a checkbox on the left or look in the header row for a checkbox that will     \n",
              "allow you to select all runs.                                                                                      \n",
              "\n",
              "Click the checkbox to enable bulk actions. Select the runs to which you'd like to apply your tag(s).               \n",
              "\n",
              "Click the Tag button above the rows of runs.                                                                       \n",
              "\n",
              "Type a tag you'd like to add and click \"Add\" below the text box to add a new tag.                                  \n",
              "\n",
              "\n",
              "This method is best suited to applying a tag or tags to a single run by hand.                                      \n",
              "\n",
              "In the left sidebar of the <a href=\"../pages/run-page.md\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Run Page</span></a>, click the top <a href=\"../pages/run-page.md#overview-tab\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Overview tab</span></a>.                                                   \n",
              "\n",
              "Next to \"Tags\" is a gray ➕ button. Click on that plus to add a tag.                                               \n",
              "\n",
              "Type a tag you'd like to add and click \"Add\" below the text box to add a new tag.                                  \n",
              "\n",
              "Let's start!                                                                                                       \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "Markdown(generation_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_mLj3IRHQRD"
      },
      "source": [
        "Let's generate 3 possible questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sBLYL1zHQRD"
      },
      "outputs": [],
      "source": [
        "generate_and_print(system_prompt, generation_prompt, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP8auvf7HQRD"
      },
      "source": [
        "> As you can see, sometimes the generation contains an intro phrase like: \"Sure, here's a support question based on the documentation:\", we may want to put some instructions to avoid this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i7jr8FjHQRD"
      },
      "source": [
        "### Level 5 prompt\n",
        "\n",
        "Complex directive that includes the following:\n",
        "- Description of high-level goal\n",
        "- A detailed bulleted list of sub-tasks\n",
        "- An explicit statement asking LLM to explain its own output\n",
        "- A guideline on how LLM output will be evaluated\n",
        "- Few-shot examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4FPsN-HHQRD"
      },
      "outputs": [],
      "source": [
        "# we will use GPT4 from here, as it gives better answers and abides to instructions better\n",
        "MODEL_NAME = \"gpt-4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgur8w3fHQRH"
      },
      "outputs": [],
      "source": [
        "# read system_template.txt file into an f-string\n",
        "with open(\"system_template.txt\", \"r\") as file:\n",
        "    system_prompt = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYlplHsuHQRH"
      },
      "outputs": [],
      "source": [
        "Markdown(system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhKOc2tiHQRH"
      },
      "outputs": [],
      "source": [
        "# read prompt_template.txt file into an f-string\n",
        "with open(\"prompt_template.txt\", \"r\") as file:\n",
        "    prompt_template = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU4MfnkJHQRH"
      },
      "outputs": [],
      "source": [
        "Markdown(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyEIoyQaHQRI"
      },
      "outputs": [],
      "source": [
        "def generate_context_prompt(chunk, n_questions=3):\n",
        "    questions = '\\n'.join(random.sample(real_queries, n_questions))\n",
        "    user_prompt = prompt_template.format(QUESTIONS=questions, CHUNK=chunk)\n",
        "    return user_prompt\n",
        "\n",
        "user_prompt = generate_context_prompt(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7dXyc65HQRI"
      },
      "outputs": [],
      "source": [
        "Markdown(user_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WCWF9WMHQRI"
      },
      "outputs": [],
      "source": [
        "def generate_questions(documents, n_questions=3, n_generations=5):\n",
        "    questions = []\n",
        "    for _, document in documents:\n",
        "        chunk = extract_random_chunk(document)\n",
        "        user_prompt = generate_context_prompt(chunk, n_questions)\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "        response = completion_with_backoff(\n",
        "            model=MODEL_NAME,\n",
        "            messages=messages,\n",
        "            n = n_generations,\n",
        "            )\n",
        "        questions.extend([response.choices[i].message.content for i in range(n_generations)])\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_dNtOpWHQRI"
      },
      "source": [
        "> A Note about the `system` role: For GPT4 based pipelines you probably want to move some part of the context prompt to the `system` context. As we are using `gpt3.5-turbo` here, you can put the instruction on the user prompt, you can read more about this on [OpenAI docs here](https://platform.openai.com/docs/guides/chat/instructing-chat-models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNuVXia1HQRI"
      },
      "outputs": [],
      "source": [
        "# function to parse model generation and extract CONTEXT, QUESTION and ANSWER\n",
        "def parse_generation(generation):\n",
        "    lines = generation.split(\"\\n\")\n",
        "    context = []\n",
        "    question = []\n",
        "    answer = []\n",
        "    flag = None\n",
        "\n",
        "    for line in lines:\n",
        "        if \"CONTEXT:\" in line:\n",
        "            flag = \"context\"\n",
        "            line = line.replace(\"CONTEXT:\", \"\").strip()\n",
        "        elif \"QUESTION:\" in line:\n",
        "            flag = \"question\"\n",
        "            line = line.replace(\"QUESTION:\", \"\").strip()\n",
        "        elif \"ANSWER:\" in line:\n",
        "            flag = \"answer\"\n",
        "            line = line.replace(\"ANSWER:\", \"\").strip()\n",
        "\n",
        "        if flag == \"context\":\n",
        "            context.append(line)\n",
        "        elif flag == \"question\":\n",
        "            question.append(line)\n",
        "        elif flag == \"answer\":\n",
        "            answer.append(line)\n",
        "\n",
        "    context = \"\\n\".join(context)\n",
        "    question = \"\\n\".join(question)\n",
        "    answer = \"\\n\".join(answer)\n",
        "    return context, question, answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jwj1URjHQRI"
      },
      "outputs": [],
      "source": [
        "generations = generate_questions([documents[0]], n_questions=3, n_generations=5)\n",
        "parse_generation(generations[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCF20J4ZHQRI"
      },
      "outputs": [],
      "source": [
        "parsed_generations = []\n",
        "generations = generate_questions(documents, n_questions=3, n_generations=5)\n",
        "for generation in generations:\n",
        "    context, question, answer = parse_generation(generation)\n",
        "    parsed_generations.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
        "\n",
        "# let's convert parsed_generations to a pandas dataframe and save it locally\n",
        "df = pd.DataFrame(parsed_generations)\n",
        "df.to_csv('generated_examples.csv', index=False)\n",
        "\n",
        "# log df as a table to W&B for interactive exploration\n",
        "wandb.log({\"generated_examples\": wandb.Table(dataframe=df)})\n",
        "\n",
        "# log csv file as an artifact to W&B for later use\n",
        "artifact = wandb.Artifact(\"generated_examples\", type=\"dataset\")\n",
        "artifact.add_file(\"generated_examples.csv\")\n",
        "wandb.log_artifact(artifact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2jTGEIxHQRJ"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rELACtV2HQRJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}